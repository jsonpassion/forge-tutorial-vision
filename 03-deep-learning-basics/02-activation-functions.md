# í™œì„±í™” í•¨ìˆ˜

> ReLU, Sigmoid, Tanh, GELU ë¹„êµ

## ê°œìš”

ì• ì„¹ì…˜ì—ì„œ ë‰´ëŸ°ì´ "ê°€ì¤‘í•© â†’ í™œì„±í™” í•¨ìˆ˜"ë¥¼ ê±°ì¹œë‹¤ê³  ë°°ì› ìŠµë‹ˆë‹¤. ê·¸ëŸ°ë° ì™œ í™œì„±í™” í•¨ìˆ˜ê°€ í•„ìš”í• ê¹Œìš”? í™œì„±í™” í•¨ìˆ˜ê°€ ì—†ìœ¼ë©´ ì•„ë¬´ë¦¬ ë ˆì´ì–´ë¥¼ ìŒ“ì•„ë„ ê²°êµ­ **í•˜ë‚˜ì˜ ì„ í˜• ë³€í™˜**ê³¼ ê°™ì•„ì ¸ì„œ, ë³µì¡í•œ íŒ¨í„´ì„ ì ˆëŒ€ í•™ìŠµí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í™œì„±í™” í•¨ìˆ˜ëŠ” ì‹ ê²½ë§ì— **ë¹„ì„ í˜•ì„±**ì„ ë¶€ì—¬í•˜ëŠ” í•µì‹¬ ë¶€í’ˆì…ë‹ˆë‹¤.

**ì„ ìˆ˜ ì§€ì‹**: [ì‹ ê²½ë§ì˜ êµ¬ì¡°](./01-neural-network.md) â€” ë‰´ëŸ°, ê°€ì¤‘í•©, ë ˆì´ì–´
**í•™ìŠµ ëª©í‘œ**:
- í™œì„±í™” í•¨ìˆ˜ê°€ ì™œ í•„ìš”í•œì§€ ì§ê´€ì ìœ¼ë¡œ ì´í•´í•œë‹¤
- Sigmoid, Tanh, ReLU, GELUì˜ íŠ¹ì„±ê³¼ ì°¨ì´ë¥¼ êµ¬ë¶„í•œë‹¤
- ìƒí™©ì— ë§ëŠ” í™œì„±í™” í•¨ìˆ˜ë¥¼ ì„ íƒí•  ìˆ˜ ìˆë‹¤

## ì™œ ì•Œì•„ì•¼ í• ê¹Œ?

í™œì„±í™” í•¨ìˆ˜ ì„ íƒì€ ëª¨ë¸ì˜ í•™ìŠµ ì†ë„ì™€ ì„±ëŠ¥ì— ì§ì ‘ ì˜í–¥ì„ ì¤ë‹ˆë‹¤. CNNì—ì„œëŠ” ReLUê°€, Transformerì—ì„œëŠ” GELUê°€ í‘œì¤€ìœ¼ë¡œ ì“°ì…ë‹ˆë‹¤. ì˜ëª»ëœ í™œì„±í™” í•¨ìˆ˜ë¥¼ ì“°ë©´ **ê¸°ìš¸ê¸° ì†Œì‹¤(Vanishing Gradient)** ë¬¸ì œë¡œ ëª¨ë¸ì´ ì „í˜€ í•™ìŠµí•˜ì§€ ëª»í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.

## í•µì‹¬ ê°œë…

### 1. ì™œ ë¹„ì„ í˜•ì´ í•„ìš”í•œê°€?

> ğŸ’¡ **ë¹„ìœ **: í™œì„±í™” í•¨ìˆ˜ ì—†ëŠ” ì‹ ê²½ë§ì€ **ì§ì„  ìë§Œ ê°€ì§„ ì œë„ì‚¬**ì™€ ê°™ìŠµë‹ˆë‹¤. ì§ì„ ë§Œìœ¼ë¡œëŠ” ê³¡ì„ ì´ë‚˜ ë³µì¡í•œ ë„í˜•ì„ ê·¸ë¦´ ìˆ˜ ì—†ì£ . í™œì„±í™” í•¨ìˆ˜ëŠ” **ê³¡ì„  ì(ê³¡ì)**ë¥¼ ì¶”ê°€í•´ ì£¼ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ê²ƒì´ ìˆì–´ì•¼ ë¹„ë¡œì†Œ ë³µì¡í•œ íŒ¨í„´ì„ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

í™œì„±í™” í•¨ìˆ˜ê°€ ì—†ë‹¤ë©´:

> ë ˆì´ì–´1: y = Wâ‚x + bâ‚
> ë ˆì´ì–´2: z = Wâ‚‚y + bâ‚‚ = Wâ‚‚(Wâ‚x + bâ‚) + bâ‚‚ = (Wâ‚‚Wâ‚)x + (Wâ‚‚bâ‚ + bâ‚‚)

ê²°êµ­ í•˜ë‚˜ì˜ ì„ í˜• ë³€í™˜ `z = Ax + c`ì™€ ë™ì¼í•©ë‹ˆë‹¤. 100ê°œ ë ˆì´ì–´ë¥¼ ìŒ“ì•„ë„ 1ê°œì™€ ê°™ì€ íš¨ê³¼ì…ë‹ˆë‹¤.

### 2. Sigmoid â€” 0ê³¼ 1 ì‚¬ì´ë¡œ ì••ì¶•

> ğŸ’¡ **ë¹„ìœ **: ì‹œí—˜ ì ìˆ˜ë¥¼ **í•©ê²©ë¥ (0~100%)**ë¡œ ë°”ê¾¸ëŠ” ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤. ì ìˆ˜ê°€ ì•„ë¬´ë¦¬ ë†’ì•„ë„ 100%ë¥¼ ë„˜ì§€ ì•Šê³ , ì•„ë¬´ë¦¬ ë‚®ì•„ë„ 0% ì•„ë˜ë¡œ ë‚´ë ¤ê°€ì§€ ì•ŠìŠµë‹ˆë‹¤.

**ìˆ˜ì‹**: Ïƒ(x) = 1 / (1 + eâ»Ë£)

| íŠ¹ì„± | ê°’ |
|------|---|
| ì¶œë ¥ ë²”ìœ„ | (0, 1) |
| x = 0ì¼ ë•Œ | 0.5 |
| í° ì–‘ìˆ˜ | â†’ 1ì— ìˆ˜ë ´ |
| í° ìŒìˆ˜ | â†’ 0ì— ìˆ˜ë ´ |

```python
import torch
import torch.nn as nn

sigmoid = nn.Sigmoid()

x = torch.tensor([-5.0, -1.0, 0.0, 1.0, 5.0])
y = sigmoid(x)
print(f"ì…ë ¥: {x.tolist()}")
print(f"ì¶œë ¥: {[f'{v:.4f}' for v in y.tolist()]}")
# ì¶œë ¥: [0.0067, 0.2689, 0.5000, 0.7311, 0.9933]
```

**ì¥ì **: ì¶œë ¥ì´ í™•ë¥ ì²˜ëŸ¼ í•´ì„ ê°€ëŠ¥
**ë‹¨ì **: í° ê°’/ì‘ì€ ê°’ì—ì„œ ê¸°ìš¸ê¸°ê°€ ê±°ì˜ 0 â†’ **ê¸°ìš¸ê¸° ì†Œì‹¤ ë¬¸ì œ**

> âš ï¸ ê¸°ìš¸ê¸° ì†Œì‹¤(Vanishing Gradient): ì—­ì „íŒŒ ì‹œ ê¸°ìš¸ê¸°ê°€ 0ì— ê°€ê¹Œì›Œì ¸ ì•ìª½ ë ˆì´ì–´ê°€ ê±°ì˜ í•™ìŠµë˜ì§€ ì•ŠëŠ” í˜„ìƒ. ê¹Šì€ ë„¤íŠ¸ì›Œí¬ì—ì„œ ì¹˜ëª…ì ì…ë‹ˆë‹¤.

### 3. Tanh â€” -1ê³¼ 1 ì‚¬ì´ë¡œ ì••ì¶•

> ğŸ’¡ **ë¹„ìœ **: Sigmoidì™€ ë¹„ìŠ·í•˜ì§€ë§Œ, ê²°ê³¼ë¥¼ **-100ì  ~ +100ì ** ë²”ìœ„ë¡œ ë§¤ê¸°ëŠ” ê²ƒì…ë‹ˆë‹¤. **0ì„ ê¸°ì¤€ìœ¼ë¡œ ëŒ€ì¹­**ì´ë¼ ì–‘ìˆ˜/ìŒìˆ˜ ëª¨ë‘ í‘œí˜„í•©ë‹ˆë‹¤.

**ìˆ˜ì‹**: tanh(x) = (eË£ - eâ»Ë£) / (eË£ + eâ»Ë£)

```python
import torch
import torch.nn as nn

tanh = nn.Tanh()

x = torch.tensor([-5.0, -1.0, 0.0, 1.0, 5.0])
y = tanh(x)
print(f"ì…ë ¥: {x.tolist()}")
print(f"ì¶œë ¥: {[f'{v:.4f}' for v in y.tolist()]}")
# ì¶œë ¥: [-0.9999, -0.7616, 0.0000, 0.7616, 0.9999]
```

**Sigmoid ëŒ€ë¹„ ì¥ì **: ì¶œë ¥ì´ 0 ì¤‘ì‹¬ â†’ ë‹¤ìŒ ë ˆì´ì–´ í•™ìŠµì— ìœ ë¦¬
**ë‹¨ì **: ì—¬ì „íˆ í° ê°’ì—ì„œ ê¸°ìš¸ê¸° ì†Œì‹¤ ë°œìƒ

### 4. ReLU â€” ë‹¨ìˆœí•˜ì§€ë§Œ ê°•ë ¥í•œ

> ğŸ’¡ **ë¹„ìœ **: **ìŒìˆ˜ëŠ” 0ìœ¼ë¡œ, ì–‘ìˆ˜ëŠ” ê·¸ëŒ€ë¡œ** í†µê³¼ì‹œí‚¤ëŠ” ë¬¸ì§€ê¸°ì…ë‹ˆë‹¤. "ë¶€ì •ì ì¸ ê±´ ì°¨ë‹¨í•˜ê³ , ê¸ì •ì ì¸ ê±´ ê·¸ëŒ€ë¡œ ë³´ë‚´ë¼!" ê·œì¹™ì´ ë§¤ìš° ë‹¨ìˆœí•´ì„œ ë¹ ë¥´ê³ , ë†€ëë„ë¡ ì˜ ì‘ë™í•©ë‹ˆë‹¤.

**ìˆ˜ì‹**: ReLU(x) = max(0, x)

```python
import torch
import torch.nn as nn

relu = nn.ReLU()

x = torch.tensor([-3.0, -1.0, 0.0, 1.0, 3.0])
y = relu(x)
print(f"ì…ë ¥: {x.tolist()}")
print(f"ì¶œë ¥: {y.tolist()}")
# ì¶œë ¥: [0.0, 0.0, 0.0, 1.0, 3.0]
```

**ì™œ ReLUê°€ ëŒ€ì„¸ê°€ ë˜ì—ˆë‚˜:**
- ê³„ì‚°ì´ ë§¤ìš° ë¹ ë¦„ (`max` ì—°ì‚° í•˜ë‚˜)
- ì–‘ìˆ˜ ì˜ì—­ì—ì„œ ê¸°ìš¸ê¸°ê°€ í•­ìƒ 1 â†’ **ê¸°ìš¸ê¸° ì†Œì‹¤ ì—†ìŒ**
- Sigmoid/Tanhë³´ë‹¤ í•™ìŠµì´ **6ë°° ë¹ ë¥´ë‹¤**ëŠ” ì—°êµ¬ ê²°ê³¼ (AlexNet ë…¼ë¬¸)

**ReLUì˜ ë‹¨ì  â€” Dead Neuron ë¬¸ì œ:**

> í•™ìŠµ ì¤‘ ê°€ì¤‘ì¹˜ê°€ ì—…ë°ì´íŠ¸ë˜ì–´ ë‰´ëŸ°ì˜ ì…ë ¥ì´ í•­ìƒ ìŒìˆ˜ê°€ ë˜ë©´, ì¶œë ¥ì´ ì˜ì›íˆ 0ì´ ë©ë‹ˆë‹¤. ì´ëŸ° ë‰´ëŸ°ì€ "ì£½ì€ ë‰´ëŸ°"ì´ë¼ í•˜ë©°, ë” ì´ìƒ í•™ìŠµì— ì°¸ì—¬í•˜ì§€ ëª»í•©ë‹ˆë‹¤.

**í•´ê²°ì±… â€” Leaky ReLU:**

```python
import torch
import torch.nn as nn

# Leaky ReLU: ìŒìˆ˜ë„ ì•„ì£¼ ì‘ì€ ê¸°ìš¸ê¸°(0.01)ë¥¼ ì¤Œ
leaky_relu = nn.LeakyReLU(0.01)

x = torch.tensor([-3.0, -1.0, 0.0, 1.0, 3.0])
y = leaky_relu(x)
print(f"ì…ë ¥:      {x.tolist()}")
print(f"LeakyReLU: {y.tolist()}")
# ì¶œë ¥: [-0.03, -0.01, 0.0, 1.0, 3.0]  â† ìŒìˆ˜ë„ ì™„ì „ 0ì´ ì•„ë‹˜
```

### 5. GELU â€” Transformer ì‹œëŒ€ì˜ í‘œì¤€

> ğŸ’¡ **ë¹„ìœ **: ReLUê°€ "í†µê³¼/ì°¨ë‹¨"ì„ **ë”± ì˜ë¼ì„œ** ê²°ì •í•˜ëŠ” ê²½ë¹„ì›ì´ë¼ë©´, GELUëŠ” **í™•ë¥ ì ìœ¼ë¡œ** ê²°ì •í•˜ëŠ” ë¶€ë“œëŸ¬ìš´ ê²½ë¹„ì›ì…ë‹ˆë‹¤. "ì´ ì •ë„ë©´ 70% í™•ë¥ ë¡œ í†µê³¼ì‹œí‚¤ê² ìŠµë‹ˆë‹¤" ì‹ìœ¼ë¡œ ë¶€ë“œëŸ¬ìš´ ê²½ê³„ë¥¼ ë§Œë“­ë‹ˆë‹¤.

**ìˆ˜ì‹**: GELU(x) = x Â· Î¦(x) (Î¦ëŠ” í‘œì¤€ ì •ê·œë¶„í¬ì˜ ëˆ„ì ë¶„í¬í•¨ìˆ˜)

```python
import torch
import torch.nn as nn

gelu = nn.GELU()

x = torch.tensor([-3.0, -1.0, 0.0, 1.0, 3.0])
y = gelu(x)
print(f"ì…ë ¥: {x.tolist()}")
print(f"GELU: {[f'{v:.4f}' for v in y.tolist()]}")
# ì¶œë ¥: [-0.0036, -0.1587, 0.0000, 0.8413, 2.9964]
```

**GELUë¥¼ ì“°ëŠ” ëª¨ë¸ë“¤**: BERT, GPT, ViT, Swin Transformer ë“± Transformer ê¸°ë°˜ ëª¨ë¸ ëŒ€ë¶€ë¶„

### 6. í•œëˆˆì— ë¹„êµ

| í™œì„±í™” í•¨ìˆ˜ | ì¶œë ¥ ë²”ìœ„ | ê¸°ìš¸ê¸° ì†Œì‹¤ | ì†ë„ | ì£¼ìš” ìš©ë„ |
|------------|----------|-----------|------|----------|
| **Sigmoid** | (0, 1) | ì‹¬í•¨ | ë³´í†µ | ì´ì§„ ë¶„ë¥˜ ì¶œë ¥ì¸µ |
| **Tanh** | (-1, 1) | ìˆìŒ | ë³´í†µ | RNN ì€ë‹‰ ìƒíƒœ |
| **ReLU** | [0, âˆ) | ì—†ìŒ (ì–‘ìˆ˜) | ë§¤ìš° ë¹ ë¦„ | **CNN ê¸°ë³¸ê°’** |
| **Leaky ReLU** | (-âˆ, âˆ) | ì—†ìŒ | ë¹ ë¦„ | Dead Neuron ë°©ì§€ |
| **GELU** | â‰ˆ(-0.17, âˆ) | ì—†ìŒ | ë³´í†µ | **Transformer ê¸°ë³¸ê°’** |

> **ì‹¤ë¬´ ê°€ì´ë“œ**: CNN â†’ **ReLU**, Transformer â†’ **GELU**, ì¶œë ¥ì¸µ í™•ë¥  â†’ **Sigmoid**(ì´ì§„) ë˜ëŠ” **Softmax**(ë‹¤ì¤‘)

## ì‹¤ìŠµ: ì§ì ‘ í•´ë³´ê¸°

### í™œì„±í™” í•¨ìˆ˜ë³„ ì¶œë ¥ ë¹„êµ

```python
import torch
import torch.nn as nn

# í™œì„±í™” í•¨ìˆ˜ ëª¨ìŒ
activations = {
    "Sigmoid": nn.Sigmoid(),
    "Tanh": nn.Tanh(),
    "ReLU": nn.ReLU(),
    "LeakyReLU": nn.LeakyReLU(0.01),
    "GELU": nn.GELU(),
}

x = torch.linspace(-5, 5, 11)  # -5ë¶€í„° 5ê¹Œì§€ 11ê°œ ì 

print(f"{'ì…ë ¥':>8s}", end="")
for name in activations:
    print(f" | {name:>10s}", end="")
print()
print("-" * 75)

for val in x:
    print(f"{val.item():>8.1f}", end="")
    for name, fn in activations.items():
        out = fn(val.unsqueeze(0)).item()
        print(f" | {out:>10.4f}", end="")
    print()
```

## í•µì‹¬ ì •ë¦¬

| ê°œë… | ì„¤ëª… |
|------|------|
| **ë¹„ì„ í˜•ì„±** | ë³µì¡í•œ íŒ¨í„´ì„ í•™ìŠµí•˜ê¸° ìœ„í•´ í•„ìˆ˜. ì—†ìœ¼ë©´ ê¹Šì€ ë§ = ì–•ì€ ë§ |
| **ê¸°ìš¸ê¸° ì†Œì‹¤** | Sigmoid/Tanhì˜ ë¬¸ì œ. ê¹Šì€ ë„¤íŠ¸ì›Œí¬ì—ì„œ ì•ìª½ ë ˆì´ì–´ í•™ìŠµ ë¶ˆëŠ¥ |
| **ReLU** | max(0, x). CNNì˜ ê¸°ë³¸ í™œì„±í™” í•¨ìˆ˜. ë¹ ë¥´ê³  ê¸°ìš¸ê¸° ì†Œì‹¤ ì—†ìŒ |
| **GELU** | ë¶€ë“œëŸ¬ìš´ ReLU. Transformer ê³„ì—´ì˜ í‘œì¤€ |
| **Dead Neuron** | ReLUì—ì„œ í•­ìƒ 0 ì¶œë ¥í•˜ëŠ” ë‰´ëŸ°. Leaky ReLUë¡œ í•´ê²° |

## ë‹¤ìŒ ì„¹ì…˜ ë¯¸ë¦¬ë³´ê¸°

ë‰´ëŸ°ì´ ìˆœì „íŒŒë¡œ ì¶œë ¥ì„ ë§Œë“œëŠ” ë²•ì„ ë°°ì› ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ "í•™ìŠµ"ì€ ì–´ë–»ê²Œ ì¼ì–´ë‚ ê¹Œìš”? ë‹¤ìŒ ì„¹ì…˜ **[ì—­ì „íŒŒ ì•Œê³ ë¦¬ì¦˜](./03-backpropagation.md)**ì—ì„œëŠ” ì‹ ê²½ë§ì´ **ì‹¤ìˆ˜ë¥¼ êµì •í•˜ë©° ìŠ¤ìŠ¤ë¡œ ê°œì„ **í•˜ëŠ” í•µì‹¬ ë©”ì»¤ë‹ˆì¦˜ì„ ë°°ì›ë‹ˆë‹¤.

## ì°¸ê³  ìë£Œ

- [Interactive Guide to Activation Functions](https://mbrenndoerfer.com/writing/activation-functions-neural-networks-complete-guide) - Sigmoidë¶€í„° GELUê¹Œì§€ ì¸í„°ë™í‹°ë¸Œ ì‹œê°í™”
- [IABAC - ReLU Activation Function Complete Guide 2025](https://iabac.org/blog/relu-activation-function) - ReLUì˜ ì—­ì‚¬ì™€ ë³€í˜• ì¢…í•© ê°€ì´ë“œ
- [GeeksforGeeks - Activation Functions in Neural Networks](https://www.geeksforgeeks.org/machine-learning/activation-functions-neural-networks/) - ëª¨ë“  í™œì„±í™” í•¨ìˆ˜ ë¹„êµ ì •ë¦¬
- [Prodia Blog - GELU vs ReLU](https://blog.prodia.com/post/compare-4-key-differences-gelu-vs-re-lu-in-neural-networks) - GELUì™€ ReLUì˜ 4ê°€ì§€ í•µì‹¬ ì°¨ì´
