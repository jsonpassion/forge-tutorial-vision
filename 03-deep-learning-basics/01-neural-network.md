# μ‹ κ²½λ§μ κµ¬μ΅°

> λ‰΄λ°, λ μ΄μ–΄, κ°€μ¤‘μΉμ μ΄ν•΄

## κ°μ”

λ“λ””μ–΄ **λ”¥λ¬λ‹**μ μ„Έκ³„μ— λ“¤μ–΄μ™”μµλ‹λ‹¤. μ•μ„ λ°°μ΄ ν•„ν„°μ™€ νΉμ§•μ  κ²€μ¶μ€ μ‚¬λμ΄ κ·μΉ™μ„ μ„¤κ³„ν–μ§€λ§, λ”¥λ¬λ‹μ€ **μ»΄ν“¨ν„°κ°€ μ¤μ¤λ΅ κ·μΉ™μ„ ν•™μµν•©λ‹λ‹¤.** κ·Έ ν•µμ‹¬μ΄ λ°”λ΅ μ‹ κ²½λ§(Neural Network)μ…λ‹λ‹¤. μ΄ μ„Ήμ…μ—μ„λ” μ‹ κ²½λ§μ„ μ΄λ£¨λ” κ°€μ¥ κΈ°λ³Έ λ‹¨μ„μΈ λ‰΄λ°, λ μ΄μ–΄, κ°€μ¤‘μΉλ¥Ό ν•λ‚μ”© μ‚΄ν΄λ΄…λ‹λ‹¤.

**μ„ μ μ§€μ‹**: [ν•νƒν•™μ  μ—°μ‚°](../02-classical-cv/05-morphology.md)κΉμ§€μ κΈ°μ΄ CV μ§€μ‹, Python κΈ°λ³Έ λ¬Έλ²•
**ν•™μµ λ©ν‘**:
- μΈκ³µ λ‰΄λ°μ΄ λ¬΄μ—‡μ„ κ³„μ‚°ν•λ”μ§€ μ„¤λ…ν•  μ μλ‹¤
- λ μ΄μ–΄μ μΆ…λ¥(μ…λ ¥, μ€λ‹‰, μ¶λ ¥)λ¥Ό κµ¬λ¶„ν•  μ μλ‹¤
- κ°€μ¤‘μΉμ™€ νΈν–¥μ μ—­ν• μ„ μ΄ν•΄ν•λ‹¤
- PyTorchλ΅ κ°„λ‹¨ν• μ‹ κ²½λ§μ„ κµ¬μ„±ν•  μ μλ‹¤

## μ™ μ•μ•„μ•Ό ν• κΉ?

CNN, Transformer, Diffusion λ¨λΈ λ“± μ΄ν›„ λ°°μΈ λ¨λ“  λ”¥λ¬λ‹ λ¨λΈμ€ **μ‹ κ²½λ§μ΄λΌλ” κ³µν†µ κ³¨κ²©** μ„μ— μ„Έμ›μ Έ μμµλ‹λ‹¤. μ‹ κ²½λ§μ κΈ°λ³Έμ„ ν™•μ‹¤ν μ΄ν•΄ν•λ©΄, μ΄ν›„ μ•„λ¬΄λ¦¬ λ³µμ΅ν• λ¨λΈμ„ λ§λ‚λ„ "κ²°κµ­ λ‰΄λ°κ³Ό λ μ΄μ–΄μ μ΅°ν•©"μ΄λΌλ” μ‚¬μ‹¤μ„ κΈ°μ–µν•  μ μμµλ‹λ‹¤.

## ν•µμ‹¬ κ°λ…

### 1. μΈκ³µ λ‰΄λ° β€” μ‹ κ²½λ§μ μµμ† λ‹¨μ„

> π’΅ **λΉ„μ **: μΈκ³µ λ‰΄λ°μ€ **ν¬ν‘ μ§‘κ³„μ†**μ™€ κ°™μµλ‹λ‹¤. μ—¬λ¬ μ‚¬λ(μ…λ ¥)μ΄ κ°κ° μκ²¬(κ°’)μ„ λ‚΄λ†“μΌλ©΄, μ§‘κ³„μ†λ” κ° μκ²¬μ— **μ¤‘μ”λ„(κ°€μ¤‘μΉ)**λ¥Ό κ³±ν•΄ ν•©μ‚°ν• λ’¤, κΈ°μ¤€μ„ λ„μΌλ©΄ "ν†µκ³Ό"(ν™μ„±ν™”), λ» λ„μΌλ©΄ "νƒλ½"μ„ μ„ μ–Έν•©λ‹λ‹¤.

ν•λ‚μ λ‰΄λ°μ΄ ν•λ” μΌμ„ μμ‹μΌλ΅ μ“°λ©΄:

> **μ¶λ ¥ = ν™μ„±ν™” ν•¨μ( wβ‚xβ‚ + wβ‚‚xβ‚‚ + ... + wβ‚™xβ‚™ + b )**

κ° κΈ°νΈμ μλ―Έ:

| κΈ°νΈ | μ΄λ¦„ | μλ―Έ |
|------|------|------|
| xβ‚, xβ‚‚, ... | **μ…λ ¥** | λ‰΄λ°μ— λ“¤μ–΄μ¤λ” λ°μ΄ν„° |
| wβ‚, wβ‚‚, ... | **κ°€μ¤‘μΉ (Weight)** | κ° μ…λ ¥μ μ¤‘μ”λ„ |
| b | **νΈν–¥ (Bias)** | κΈ°μ¤€μ μ„ μ΅°μ ν•λ” κ°’ |
| Ξ£ | **κ°€μ¤‘ν•©** | μ…λ ¥ Γ— κ°€μ¤‘μΉμ μ΄ν•© |
| f() | **ν™μ„±ν™” ν•¨μ** | κ²°κ³Όλ¥Ό λΉ„μ„ ν•μΌλ΅ λ³€ν™ |

```python
import numpy as np

# ν•λ‚μ λ‰΄λ° μ§μ ‘ κµ¬ν„
def neuron(inputs, weights, bias):
    """μ…λ ¥μ— κ°€μ¤‘μΉλ¥Ό κ³±ν•κ³  νΈν–¥μ„ λ”ν• λ’¤ ν™μ„±ν™”(ReLU)"""
    weighted_sum = np.dot(inputs, weights) + bias
    output = max(0, weighted_sum)  # ReLU ν™μ„±ν™”
    return output

# μμ‹: 3κ°μ μ…λ ¥μ„ λ°›λ” λ‰΄λ°
inputs = np.array([1.0, 2.0, 3.0])
weights = np.array([0.4, 0.5, -0.2])
bias = 0.1

result = neuron(inputs, weights, bias)
print(f"μ…λ ¥: {inputs}")
print(f"κ°€μ¤‘μΉ: {weights}")
print(f"κ°€μ¤‘ν•©: {np.dot(inputs, weights) + bias:.1f}")
print(f"μ¶λ ¥ (ReLU): {result:.1f}")
```

### 2. κ°€μ¤‘μΉ(Weight)μ™€ νΈν–¥(Bias)

> π’΅ **λΉ„μ **: κ°€μ¤‘μΉλ” **λ³Όλ¥¨ μ΅°μ  λ…ΈλΈ**μ…λ‹λ‹¤. μ…λ ¥ μ‹ νΈκ°€ λ“¤μ–΄μ¬ λ• μ–΄λ–¤ μ…λ ¥μ„ ν¬κ² ν‚¤μ°κ³ (μ¤‘μ”), μ–΄λ–¤ μ…λ ¥μ„ μ¤„μ΄κ±°λ‚ λ’¤μ§‘μ„μ§€(μ¤‘μ”ν•μ§€ μ•μ) κ²°μ •ν•©λ‹λ‹¤. νΈν–¥μ€ **μ „μ²΄ μλ‰μ κΈ°λ³Έκ°’**μΌλ΅, λ³Όλ¥¨μ„ λ¨λ‘ 0μΌλ΅ λ†”λ„ μµμ†ν• μ΄λ§νΌμ€ μ†λ¦¬κ°€ λ‚κ²(λλ” μ• λ‚κ²) λ§λ“­λ‹λ‹¤.

| κ°λ… | μ—­ν•  | ν•™μµ κ³Όμ • |
|------|------|----------|
| **κ°€μ¤‘μΉ** | κ° μ…λ ¥μ μ¤‘μ”λ„ μ΅°μ  | ν›λ ¨ μ¤‘ μλ™ μ—…λ°μ΄νΈ |
| **νΈν–¥** | ν™μ„±ν™” κΈ°μ¤€μ  μ΅°μ  | ν›λ ¨ μ¤‘ μλ™ μ—…λ°μ΄νΈ |

> **ν•µμ‹¬**: λ”¥λ¬λ‹μ "ν•™μµ"μ΄λ€, μ΄ κ°€μ¤‘μΉμ™€ νΈν–¥μ„ λ°μ΄ν„°λ¥Ό λ³΄λ©° **μµμ μ κ°’μΌλ΅ μ΅°μ •ν•λ” κ³Όμ •**μ…λ‹λ‹¤.

### 3. λ μ΄μ–΄ β€” λ‰΄λ°μ κ·Έλ£Ή

> π’΅ **λΉ„μ **: κ³µμ¥μ **μ΅°λ¦½ λΌμΈ**μ„ μƒκ°ν•μ„Έμ”. μ›μμ¬(μ…λ ¥)κ°€ λ“¤μ–΄μ¤λ©΄ 1λ² λΌμΈ(μ…λ ¥ λ μ΄μ–΄)μ—μ„ κΈ°λ³Έ μ²λ¦¬λ¥Ό ν•κ³ , 2λ²Β·3λ² λΌμΈ(μ€λ‹‰ λ μ΄μ–΄)μ—μ„ μ μ  μ •κµν•κ² κ°€κ³µν•κ³ , λ§μ§€λ§‰ λΌμΈ(μ¶λ ¥ λ μ΄μ–΄)μ—μ„ μ™„μ„±ν’(κ²°κ³Ό)μ΄ λ‚μµλ‹λ‹¤.

μ‹ κ²½λ§μ€ λ‰΄λ°μ„ **μΈµ(Layer)**μΌλ΅ λ¬¶μ–΄ μμ„λ€λ΅ μ—°κ²°ν•©λ‹λ‹¤.

| λ μ΄μ–΄ | μ„μΉ | μ—­ν•  | μμ‹ |
|--------|------|------|------|
| **μ…λ ¥ λ μ΄μ–΄** | λ§¨ μ• | μ›λ³Έ λ°μ΄ν„°λ¥Ό λ°›μ•„λ“¤μ„ | 28Γ—28 μ΄λ―Έμ§€ = 784κ° λ‰΄λ° |
| **μ€λ‹‰ λ μ΄μ–΄** | μ¤‘κ°„ | λ°μ΄ν„°μ—μ„ ν¨ν„΄μ„ μ¶”μ¶ | 128κ°, 64κ° λ‰΄λ° λ“± |
| **μ¶λ ¥ λ μ΄μ–΄** | λ§¨ λ’¤ | μµμΆ… κ²°κ³Ό μƒμ„± | 10κ° λ‰΄λ° (μ«μ 0~9 λ¶„λ¥) |

> "**λ”¥**λ¬λ‹"μ "λ”¥"μ€ **μ€λ‹‰ λ μ΄μ–΄κ°€ μ—¬λ¬ κ°**λΌλ” λ»μ…λ‹λ‹¤. λ μ΄μ–΄κ°€ κΉμ„μλ΅ λ” λ³µμ΅ν• ν¨ν„΄μ„ ν•™μµν•  μ μμµλ‹λ‹¤.

### 4. μμ „ν(Forward Pass) β€” λ°μ΄ν„°κ°€ νλ¥΄λ” λ°©ν–¥

λ°μ΄ν„°κ°€ μ…λ ¥ λ μ΄μ–΄μ—μ„ μ¶λ ¥ λ μ΄μ–΄κΉμ§€ **μ•μΌλ΅λ§ νλ¥΄λ”** κ³Όμ •μ„ μμ „νλΌκ³  ν•©λ‹λ‹¤.

> μ…λ ¥ λ°μ΄ν„° β†’ λ μ΄μ–΄ 1 (κ°€μ¤‘ν•© + ν™μ„±ν™”) β†’ λ μ΄μ–΄ 2 (κ°€μ¤‘ν•© + ν™μ„±ν™”) β†’ ... β†’ μ¶λ ¥

```python
import numpy as np

def simple_network(x):
    """2κ°μ μ€λ‹‰ λ μ΄μ–΄λ¥Ό κ°€μ§„ κ°„λ‹¨ν• μ‹ κ²½λ§"""
    # λ μ΄μ–΄ 1: μ…λ ¥ 3κ° β†’ λ‰΄λ° 4κ°
    W1 = np.random.randn(3, 4) * 0.5
    b1 = np.zeros(4)
    h1 = np.maximum(0, x @ W1 + b1)  # ReLU ν™μ„±ν™”

    # λ μ΄μ–΄ 2: λ‰΄λ° 4κ° β†’ λ‰΄λ° 2κ°
    W2 = np.random.randn(4, 2) * 0.5
    b2 = np.zeros(2)
    h2 = np.maximum(0, h1 @ W2 + b2)  # ReLU ν™μ„±ν™”

    # μ¶λ ¥ λ μ΄μ–΄: λ‰΄λ° 2κ° β†’ μ¶λ ¥ 1κ°
    W3 = np.random.randn(2, 1) * 0.5
    b3 = np.zeros(1)
    output = h1 @ W2 @ W3  # λ§μ§€λ§‰μ€ ν™μ„±ν™” μ—†μ΄

    return output

x = np.array([1.0, 2.0, 3.0])
result = simple_network(x)
print(f"μ…λ ¥: {x}")
print(f"μ¶λ ¥: {result}")
```

### 5. PyTorchλ΅ μ‹ κ²½λ§ λ§λ“¤κΈ°

PyTorchμ—μ„λ” `nn.Module`μ„ μƒμ†λ°›μ•„ μ‹ κ²½λ§μ„ μ •μν•©λ‹λ‹¤.

```python
import torch
import torch.nn as nn

class SimpleNetwork(nn.Module):
    def __init__(self):
        super().__init__()
        # λ μ΄μ–΄ μ •μ
        self.layer1 = nn.Linear(784, 128)  # μ…λ ¥ 784 β†’ μ€λ‹‰ 128
        self.layer2 = nn.Linear(128, 64)   # μ€λ‹‰ 128 β†’ μ€λ‹‰ 64
        self.layer3 = nn.Linear(64, 10)    # μ€λ‹‰ 64 β†’ μ¶λ ¥ 10
        self.relu = nn.ReLU()              # ν™μ„±ν™” ν•¨μ

    def forward(self, x):
        """μμ „ν: λ°μ΄ν„°κ°€ λ μ΄μ–΄λ¥Ό ν†µκ³Όν•λ” κ³Όμ •"""
        x = self.relu(self.layer1(x))  # λ μ΄μ–΄1 + ν™μ„±ν™”
        x = self.relu(self.layer2(x))  # λ μ΄μ–΄2 + ν™μ„±ν™”
        x = self.layer3(x)             # μ¶λ ¥ (ν™μ„±ν™” μ—†μ)
        return x

# λ¨λΈ μƒμ„±
model = SimpleNetwork()
print(model)

# νλΌλ―Έν„°(κ°€μ¤‘μΉ+νΈν–¥) μ ν™•μΈ
total_params = sum(p.numel() for p in model.parameters())
print(f"\nμ΄ νλΌλ―Έν„° μ: {total_params:,}")
```

```python
import torch

# λ¨λΈμ— λ°μ΄ν„° ν†µκ³Όμ‹ν‚¤κΈ°
model = SimpleNetwork()

# κ°€μ§ μ…λ ¥: λ°°μΉ ν¬κΈ° 1, νΉμ„± 784κ°
x = torch.randn(1, 784)

# μμ „ν
output = model(x)
print(f"μ…λ ¥ shape: {x.shape}")    # [1, 784]
print(f"μ¶λ ¥ shape: {output.shape}")  # [1, 10]
print(f"μ¶λ ¥ κ°’: {output.data}")

# κ° λ μ΄μ–΄μ κ°€μ¤‘μΉ shape ν™•μΈ
for name, param in model.named_parameters():
    print(f"{name:20s} | shape: {str(param.shape):15s} | νλΌλ―Έν„° μ: {param.numel():,}")
```

### 6. νλΌλ―Έν„° μ κ³„μ‚°ν•κΈ°

`nn.Linear(μ…λ ¥, μ¶λ ¥)`μ νλΌλ―Έν„° μ = (μ…λ ¥ Γ— μ¶λ ¥) + μ¶λ ¥(νΈν–¥)

| λ μ΄μ–΄ | κµ¬μ΅° | κ°€μ¤‘μΉ | νΈν–¥ | ν•©κ³„ |
|--------|------|--------|------|------|
| layer1 | 784 β†’ 128 | 784Γ—128 = 100,352 | 128 | 100,480 |
| layer2 | 128 β†’ 64 | 128Γ—64 = 8,192 | 64 | 8,256 |
| layer3 | 64 β†’ 10 | 64Γ—10 = 640 | 10 | 650 |
| **ν•©κ³„** | | | | **109,386** |

> μ΄ κ°„λ‹¨ν• 3κ° λ μ΄μ–΄ λ„¤νΈμ›ν¬μ—λ„ **μ•½ 11λ§ κ°μ νλΌλ―Έν„°**κ°€ μμµλ‹λ‹¤. ν„λ€ λ”¥λ¬λ‹ λ¨λΈμ€ μμ‹­μ–µ κ°μ νλΌλ―Έν„°λ¥Ό κ°€μ§‘λ‹λ‹¤.

## μ‹¤μµ: μ§μ ‘ ν•΄λ³΄κΈ°

### nn.Linearμ λ‚΄λ¶€ λ“¤μ—¬λ‹¤λ³΄κΈ°

```python
import torch
import torch.nn as nn

# ν•λ‚μ Linear λ μ΄μ–΄ μƒμ„±
linear = nn.Linear(in_features=3, out_features=2)

# κ°€μ¤‘μΉμ™€ νΈν–¥ ν™•μΈ
print(f"κ°€μ¤‘μΉ shape: {linear.weight.shape}")  # [2, 3]
print(f"κ°€μ¤‘μΉ κ°’:\n{linear.weight.data}")
print(f"\nνΈν–¥ shape: {linear.bias.shape}")      # [2]
print(f"νΈν–¥ κ°’: {linear.bias.data}")

# μ§μ ‘ κ³„μ‚°ν•΄λ³΄κΈ°
x = torch.tensor([1.0, 2.0, 3.0])
manual_output = x @ linear.weight.T + linear.bias
auto_output = linear(x)

print(f"\nμλ™ κ³„μ‚°: {manual_output.data}")
print(f"Linear κ²°κ³Ό: {auto_output.data}")
print(f"λ™μΌν•κ°€? {torch.allclose(manual_output, auto_output)}")
```

## ν•µμ‹¬ μ •λ¦¬

| κ°λ… | μ„¤λ… |
|------|------|
| **μΈκ³µ λ‰΄λ°** | μ…λ ¥μ— κ°€μ¤‘μΉλ¥Ό κ³±ν•κ³ , νΈν–¥μ„ λ”ν•κ³ , ν™μ„±ν™” ν•¨μλ¥Ό ν†µκ³Ό |
| **κ°€μ¤‘μΉ (Weight)** | κ° μ…λ ¥μ μ¤‘μ”λ„λ¥Ό κ²°μ •ν•λ” ν•™μµ κ°€λ¥ν• νλΌλ―Έν„° |
| **νΈν–¥ (Bias)** | ν™μ„±ν™” κΈ°μ¤€μ μ„ μ΅°μ ν•λ” ν•™μµ κ°€λ¥ν• νλΌλ―Έν„° |
| **λ μ΄μ–΄** | λ‰΄λ°μ λ¬¶μ. μ…λ ¥β†’μ€λ‹‰β†’μ¶λ ¥ μμ„λ΅ μ—°κ²° |
| **μμ „ν** | λ°μ΄ν„°κ°€ μ…λ ¥μ—μ„ μ¶λ ¥κΉμ§€ λ μ΄μ–΄λ¥Ό ν†µκ³Όν•λ” κ³Όμ • |
| **nn.Linear** | PyTorchμ—μ„ μ™„μ „μ—°κ²° λ μ΄μ–΄λ¥Ό λ§λ“λ” ν΄λμ¤ |

## λ‹¤μ μ„Ήμ… λ―Έλ¦¬λ³΄κΈ°

λ‰΄λ°μ μ¶λ ¥μ—μ„ "ν™μ„±ν™” ν•¨μ"λ¥Ό μ–ΈκΈ‰ν–λ”λ°, μ•„μ§ μμ„Έν λ‹¤λ£¨μ§€ μ•μ•μµλ‹λ‹¤. λ‹¤μ μ„Ήμ… **[ν™μ„±ν™” ν•¨μ](./02-activation-functions.md)**μ—μ„λ” ReLU, Sigmoid, GELU λ“± μ™ ν™μ„±ν™” ν•¨μκ°€ ν•„μ”ν•μ§€, κ°κ° μ–΄λ–¤ νΉμ„±μ„ κ°€μ§€λ”μ§€ κΉμ΄ νλ΄…λ‹λ‹¤.

## μ°Έκ³  μλ£

- [Google ML Crash Course - Neural Network Nodes and Layers](https://developers.google.com/machine-learning/crash-course/neural-networks/nodes-hidden-layers) - λ‰΄λ°κ³Ό λ μ΄μ–΄μ μ§κ΄€μ  μ„¤λ…
- [PyTorch κ³µμ‹ νν† λ¦¬μ–Ό - Neural Networks](https://docs.pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html) - PyTorchλ΅ μ‹ κ²½λ§ κµ¬μ¶•ν•κΈ°
- [Victor Zhou - Introduction to Neural Networks](https://victorzhou.com/blog/intro-to-neural-networks/) - μν•™λ¶€ν„° μ½”λ“κΉμ§€ λ‹¨κ³„λ³„ μ„¤λ…
- [GeeksforGeeks - Weights and Bias in Neural Networks](https://www.geeksforgeeks.org/deep-learning/the-role-of-weights-and-bias-in-neural-networks/) - κ°€μ¤‘μΉμ™€ νΈν–¥μ μ—­ν•  μƒμ„Έ μ„¤λ…
