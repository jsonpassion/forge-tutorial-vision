# ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜

> Self-Attentionê³¼ Multi-Head Attention

## ê°œìš”

ì´ë¯¸ì§€ë¥¼ ì´í•´í•  ë•Œ, ìš°ë¦¬ ëˆˆì€ ëª¨ë“  í”½ì…€ì„ ê· ë“±í•˜ê²Œ ë³´ì§€ ì•ŠìŠµë‹ˆë‹¤. ì¤‘ìš”í•œ ë¶€ë¶„ì— ì‹œì„ ì´ ì§‘ì¤‘ë˜ì£ . ì´ ì„¹ì…˜ì—ì„œëŠ” ë”¥ëŸ¬ë‹ ëª¨ë¸ì´ "ì–´ë””ì— ì§‘ì¤‘í• ì§€"ë¥¼ ìŠ¤ìŠ¤ë¡œ í•™ìŠµí•˜ëŠ” **ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜(Attention Mechanism)**ì„ ë°°ì›ë‹ˆë‹¤. íŠ¹íˆ Transformerì˜ í•µì‹¬ì¸ **Self-Attention**ê³¼ **Multi-Head Attention**ì„ ë¹„ìœ ë¶€í„° ìˆ˜ì‹, ì½”ë“œê¹Œì§€ ì™„ì „íˆ ì´í•´í•´ ë´…ì‹œë‹¤.

**ì„ ìˆ˜ ì§€ì‹**: [CNN í•µì‹¬ ê°œë…](../04-cnn-fundamentals/01-convolution.md)ì—ì„œ ë°°ìš´ í•©ì„±ê³± ì—°ì‚°, [ë°°ì¹˜ ì •ê·œí™”](../04-cnn-fundamentals/03-batch-normalization.md)ì—ì„œ ë°°ìš´ LayerNorm ê°œë…
**í•™ìŠµ ëª©í‘œ**:
- ì–´í…ì…˜ì´ ë¬´ì—‡ì´ê³  ì™œ í•„ìš”í•œì§€ ì§ê´€ì ìœ¼ë¡œ ì´í•´í•˜ê¸°
- Query, Key, Valueì˜ ì—­í• ì„ ë¹„ìœ ë¡œ ì™„ë²½íˆ íŒŒì•…í•˜ê¸°
- Scaled Dot-Product Attentionì˜ ìˆ˜ì‹ê³¼ ì½”ë“œ êµ¬í˜„
- Multi-Head Attentionì´ ì™œ "ì—¬ëŸ¬ ê°œì˜ ëˆˆ"ì¸ì§€ ì´í•´í•˜ê¸°

## ì™œ ì•Œì•„ì•¼ í• ê¹Œ?

[CNN ì•„í‚¤í…ì²˜](../05-cnn-architectures/01-lenet-alexnet.md)ì—ì„œ ë°°ì› ë“¯ì´, CNNì€ ì´ë¯¸ì§€ ì²˜ë¦¬ì˜ ì™•ì¢Œë¥¼ ì˜¤ë«ë™ì•ˆ ì§€ì¼œì™”ìŠµë‹ˆë‹¤. ê·¸ëŸ°ë° CNNì—ëŠ” ê·¼ë³¸ì ì¸ í•œê³„ê°€ ìˆì—ˆì–´ìš”. **ë¡œì»¬ ìˆ˜ìš© ì˜ì—­(local receptive field)**ì´ë¼ëŠ” íŠ¹ì„± ë•Œë¬¸ì—, ì´ë¯¸ì§€ì˜ ë¨¼ ê³³ì— ìˆëŠ” ì •ë³´ë¥¼ í•œ ë²ˆì— ì—°ê²°í•˜ê¸° ì–´ë ¤ì› ê±°ë“ ìš”.

ì˜ˆë¥¼ ë“¤ì–´ë³¼ê¹Œìš”? ì‚¬ì§„ ì†ì—ì„œ í•˜ëŠ˜ì— ìˆëŠ” ë¹„í–‰ê¸°ì™€ ë•… ìœ„ì˜ ê·¸ë¦¼ìë¥¼ ë™ì‹œì— íŒŒì•…í•˜ë ¤ë©´, CNNì€ ì—¬ëŸ¬ ë ˆì´ì–´ë¥¼ ê±°ì³ì•¼ í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì€ **ë‹¨ í•œ ë²ˆì˜ ì—°ì‚°ìœ¼ë¡œ** ì´ë¯¸ì§€ì˜ ì–´ëŠ ìœ„ì¹˜ë“  ì—°ê²°í•  ìˆ˜ ìˆì£ . ì´ê²ƒì´ ë°”ë¡œ 2020ë…„ëŒ€ì— Vision Transformerê°€ CNNì„ ëŒ€ì²´í•˜ê¸° ì‹œì‘í•œ í•µì‹¬ ì´ìœ ì…ë‹ˆë‹¤.

ì˜¤ëŠ˜ë‚  ChatGPT, DALL-E, Stable Diffusion, Sora ë“± ìš°ë¦¬ê°€ ì•„ëŠ” ê±°ì˜ ëª¨ë“  ìµœì‹  AIì˜ ì‹¬ì¥ì—ëŠ” ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì´ ë›°ê³  ìˆìŠµë‹ˆë‹¤.

## í•µì‹¬ ê°œë…

### ê°œë… 1: ì–´í…ì…˜ì´ë€? â€” "ì‹œí—˜ ë¬¸ì œì˜ í˜•ê´‘íœ"

> ğŸ’¡ **ë¹„ìœ **: ì‹œí—˜ ê³µë¶€ë¥¼ í•  ë•Œë¥¼ ë– ì˜¬ë ¤ ë³´ì„¸ìš”. êµê³¼ì„œì˜ ëª¨ë“  ë¬¸ì¥ì„ ë˜‘ê°™ì€ ë¹„ì¤‘ìœ¼ë¡œ ì½ì§€ ì•Šì£ ? ì¤‘ìš”í•œ ë¶€ë¶„ì— **í˜•ê´‘íœ**ì„ ì¹˜ê³ , ê·¸ ë¶€ë¶„ì— ì§‘ì¤‘í•©ë‹ˆë‹¤. ì–´í…ì…˜ë„ ë˜‘ê°™ìŠµë‹ˆë‹¤. ì…ë ¥ ë°ì´í„°ì˜ ëª¨ë“  ë¶€ë¶„ ì¤‘ì—ì„œ **ì§€ê¸ˆ ì¤‘ìš”í•œ ë¶€ë¶„ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜**ë¥¼ ë¶€ì—¬í•˜ëŠ” ë©”ì»¤ë‹ˆì¦˜ì´ì—ìš”.

ì–´í…ì…˜(Attention)ì˜ í•µì‹¬ ì•„ì´ë””ì–´ëŠ” ë‹¨ìˆœí•©ë‹ˆë‹¤:

- **ì…ë ¥ì˜ ëª¨ë“  ìœ„ì¹˜ë¥¼ ì‚´í´ë³´ë˜**
- **í˜„ì¬ ì‘ì—…ì— ê´€ë ¨ëœ ë¶€ë¶„ì— ë” ë§ì€ "ì£¼ì˜"ë¥¼ ê¸°ìš¸ì´ê³ **
- **ê´€ë ¨ ì—†ëŠ” ë¶€ë¶„ì€ ë¬´ì‹œí•œë‹¤**

ì‚¬ì‹¤ ì–´í…ì…˜ ê°œë…ì€ Transformerë³´ë‹¤ ë¨¼ì € ë“±ì¥í–ˆìŠµë‹ˆë‹¤. 2014ë…„ Bahdanauê°€ ê¸°ê³„ ë²ˆì—­ì—ì„œ ì²˜ìŒ ì œì•ˆí–ˆëŠ”ë°ìš”, ê¸´ ë¬¸ì¥ì„ ë²ˆì—­í•  ë•Œ ëª¨ë“  ë‹¨ì–´ë¥¼ í•˜ë‚˜ì˜ ë²¡í„°ë¡œ ì••ì¶•í•˜ë©´ ì •ë³´ê°€ ì†ì‹¤ë˜ë‹ˆ, ì›ë¬¸ì˜ ê° ë‹¨ì–´ì— "ì£¼ì˜"ë¥¼ ê¸°ìš¸ì´ìëŠ” ì•„ì´ë””ì–´ì˜€ì£ .

### ê°œë… 2: Query, Key, Value â€” "ë„ì„œê´€ ê²€ìƒ‰ ì‹œìŠ¤í…œ"

> ğŸ’¡ **ë¹„ìœ **: ë„ì„œê´€ì—ì„œ ì±…ì„ ì°¾ëŠ” ê³¼ì •ì„ ìƒìƒí•´ ë³´ì„¸ìš”.
> - **Query(ì§ˆë¬¸)**: ë‹¹ì‹ ì´ ì‚¬ì„œì—ê²Œ í•˜ëŠ” ì§ˆë¬¸ â€” "ìš°ì£¼ì— ê´€í•œ ì±… ì°¾ì•„ì£¼ì„¸ìš”"
> - **Key(ì¹´íƒˆë¡œê·¸)**: ë„ì„œê´€ì˜ ì¹´íƒˆë¡œê·¸ì— ì íŒ ê° ì±…ì˜ í‚¤ì›Œë“œ â€” "ë¬¼ë¦¬í•™", "ìš°ì£¼ë¡ ", "ìš”ë¦¬"...
> - **Value(ì‹¤ì œ ì±…)**: ì¹´íƒˆë¡œê·¸ì— ë§¤ì¹­ëœ ì‹¤ì œ ì±…ì˜ ë‚´ìš©
>
> ë‹¹ì‹ ì˜ ì§ˆë¬¸(Query)ê³¼ ì¹´íƒˆë¡œê·¸(Key)ë¥¼ ë¹„êµí•´ì„œ, ê°€ì¥ ê´€ë ¨ ìˆëŠ” ì±…(Value)ì„ ê°€ì ¸ì˜¤ëŠ” ê²ë‹ˆë‹¤!

ì´ê²ƒì„ ìˆ˜í•™ì ìœ¼ë¡œ í‘œí˜„í•˜ë©´:

1. **ìœ ì‚¬ë„ ê³„ì‚°**: Queryì™€ ëª¨ë“  Key ì‚¬ì´ì˜ ìœ ì‚¬ë„(ë‚´ì )ë¥¼ êµ¬í•©ë‹ˆë‹¤
2. **ê°€ì¤‘ì¹˜ ì •ê·œí™”**: Softmaxë¡œ ìœ ì‚¬ë„ë¥¼ í™•ë¥ ë¡œ ë³€í™˜í•©ë‹ˆë‹¤
3. **ê°€ì¤‘ í•©ì‚°**: í™•ë¥ ì„ ê°€ì¤‘ì¹˜ë¡œ ì‚¼ì•„ Valueë“¤ì˜ ê°€ì¤‘ í•©ì„ êµ¬í•©ë‹ˆë‹¤

ì—¬ê¸°ì„œ ì¤‘ìš”í•œ ì ì€, Q, K, V ëª¨ë‘ **ê°™ì€ ì…ë ¥ì—ì„œ íŒŒìƒ**ëœë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ì…ë ¥ ë²¡í„° $x$ì— ì„œë¡œ ë‹¤ë¥¸ ê°€ì¤‘ì¹˜ í–‰ë ¬ $W_Q$, $W_K$, $W_V$ë¥¼ ê³±í•´ì„œ ë§Œë“¤ì£ .

$$Q = xW_Q, \quad K = xW_K, \quad V = xW_V$$

- $x$: ì…ë ¥ ì‹œí€€ìŠ¤ (ì´ë¯¸ì§€ íŒ¨ì¹˜ë‚˜ ë‹¨ì–´ ì„ë² ë”©)
- $W_Q, W_K, W_V$: í•™ìŠµ ê°€ëŠ¥í•œ ê°€ì¤‘ì¹˜ í–‰ë ¬
- $Q, K, V$: ê°ê° ì§ˆë¬¸, ì¹´íƒˆë¡œê·¸, ì‹¤ì œ ì •ë³´ ì—­í• 

> âš ï¸ **í”í•œ ì˜¤í•´**: "Q, K, Vê°€ ì„œë¡œ ë‹¤ë¥¸ ë°ì´í„°ì—ì„œ ë‚˜ì˜¨ë‹¤"ê³  ìƒê°í•˜ê¸° ì‰¬ìš´ë°ìš”, **Self-Attention**ì—ì„œëŠ” Q, K, V ëª¨ë‘ **ê°™ì€ ì…ë ¥**ì—ì„œ ë§Œë“¤ì–´ì§‘ë‹ˆë‹¤. "Self"ë¼ëŠ” ì´ë¦„ì´ ë¶™ì€ ì´ìœ ê°€ ë°”ë¡œ ì´ê²ƒì´ì—ìš”. ìê¸° ìì‹ ì˜ ë‹¤ë¥¸ ìœ„ì¹˜ë“¤ê³¼ì˜ ê´€ê³„ë¥¼ íŒŒì•…í•˜ëŠ” ê±°ì£ .

### ê°œë… 3: Scaled Dot-Product Attention â€” "ë³¼ë¥¨ ì¡°ì ˆì´ í•„ìš”í•œ ì´ìœ "

Self-Attentionì˜ í•µì‹¬ ìˆ˜ì‹ì€ ë†€ë¼ìš¸ ì •ë„ë¡œ ê°„ê²°í•©ë‹ˆë‹¤:

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

- $QK^T$: Queryì™€ Keyì˜ ë‚´ì  â†’ ìœ ì‚¬ë„ ì ìˆ˜
- $\sqrt{d_k}$: Key ë²¡í„°ì˜ ì°¨ì› ìˆ˜ì˜ ì œê³±ê·¼ â†’ **ìŠ¤ì¼€ì¼ë§ íŒ©í„°**
- $\text{softmax}$: ìœ ì‚¬ë„ë¥¼ 0~1 í™•ë¥ ë¡œ ì •ê·œí™”
- $V$: ìµœì¢… ê°€ì¤‘ í•©ì‚°ì— ì‚¬ìš©í•  Value

> ğŸ’¡ **ë¹„ìœ **: $\sqrt{d_k}$ë¡œ ë‚˜ëˆ„ëŠ” ì´ìœ ê°€ ë­˜ê¹Œìš”? ë§ˆì´í¬ ë³¼ë¥¨ì— ë¹„ìœ í•˜ë©´ ì´í•´ê°€ ì‰½ìŠµë‹ˆë‹¤. ì°¨ì› ìˆ˜ê°€ ì»¤ì§€ë©´ ë‚´ì  ê°’ë„ ì»¤ì§€ëŠ”ë°, ì´ ê°’ì´ ë„ˆë¬´ ì»¤ì§€ë©´ Softmaxê°€ ê·¹ë‹¨ì ì¸ ê°’(ê±°ì˜ 0 ë˜ëŠ” 1)ì„ ì¶œë ¥í•´ ë²„ë¦½ë‹ˆë‹¤. ë§ˆì´í¬ ë³¼ë¥¨ì´ ë„ˆë¬´ í¬ë©´ ì†Œë¦¬ê°€ ì°¢ì–´ì§€ëŠ” ê²ƒì²˜ëŸ¼ìš”. $\sqrt{d_k}$ë¡œ ë‚˜ëˆ ì„œ **ì ì ˆí•œ ë³¼ë¥¨**ì„ ìœ ì§€í•˜ëŠ” ê²ë‹ˆë‹¤.

ë‹¨ê³„ë³„ë¡œ ë”°ë¼ê°€ ë´…ì‹œë‹¤:

| ë‹¨ê³„ | ì—°ì‚° | ì˜ë¯¸ |
|------|------|------|
| 1ë‹¨ê³„ | $QK^T$ | ëª¨ë“  Query-Key ìŒì˜ ìœ ì‚¬ë„ ì ìˆ˜ ê³„ì‚° |
| 2ë‹¨ê³„ | $\div \sqrt{d_k}$ | ê°’ì´ ë„ˆë¬´ ì»¤ì§€ì§€ ì•Šê²Œ ìŠ¤ì¼€ì¼ë§ |
| 3ë‹¨ê³„ | Softmax | ì ìˆ˜ë¥¼ í™•ë¥ (ê°€ì¤‘ì¹˜)ë¡œ ë³€í™˜ |
| 4ë‹¨ê³„ | $\times V$ | ê°€ì¤‘ì¹˜ë¡œ Valueë¥¼ í•©ì‚°í•˜ì—¬ ì¶œë ¥ |

### ê°œë… 4: Multi-Head Attention â€” "ì—¬ëŸ¬ ê°œì˜ ëˆˆìœ¼ë¡œ ë™ì‹œì— ë³´ê¸°"

> ğŸ’¡ **ë¹„ìœ **: ë¯¸ìˆ ê´€ì—ì„œ ê·¸ë¦¼ í•œ ì ì„ ê°ìƒí•œë‹¤ê³  í•´ë³´ì„¸ìš”. ë¯¸ìˆ  í‰ë¡ ê°€ í•œ ëª…ì€ **ìƒ‰ì±„**ì— ì£¼ëª©í•˜ê³ , ë‹¤ë¥¸ í•œ ëª…ì€ **êµ¬ë„**ë¥¼, ë˜ ë‹¤ë¥¸ í•œ ëª…ì€ **ë¶“í„°ì¹˜**ë¥¼ ë´…ë‹ˆë‹¤. ê°ì ë‹¤ë¥¸ ê´€ì ì—ì„œ ê°™ì€ ê·¸ë¦¼ì„ ë³´ê³ , ì´ ì˜ê²¬ë“¤ì„ ì¢…í•©í•˜ë©´ ë” í’ë¶€í•œ í•´ì„ì´ ë‚˜ì˜¤ê² ì£ ? Multi-Head Attentionì´ ë°”ë¡œ ì´ê²ƒì…ë‹ˆë‹¤!

í•˜ë‚˜ì˜ ì–´í…ì…˜ í—¤ë“œëŠ” í•˜ë‚˜ì˜ ê´€ì ë§Œ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì‹¤ì œ ë°ì´í„°ì—ëŠ” ë‹¤ì–‘í•œ ì¢…ë¥˜ì˜ ê´€ê³„ê°€ ê³µì¡´í•˜ì£ :

- í•œ í—¤ë“œëŠ” **ìƒ‰ìƒ ìœ ì‚¬ì„±**ì— ì£¼ëª©
- ë‹¤ë¥¸ í—¤ë“œëŠ” **ê³µê°„ì  ì¸ì ‘ì„±**ì— ì£¼ëª©
- ë˜ ë‹¤ë¥¸ í—¤ë“œëŠ” **ì˜ë¯¸ì  ê´€ë ¨ì„±**ì— ì£¼ëª©

Multi-Head Attentionì€ ì´ë ‡ê²Œ ë™ì‘í•©ë‹ˆë‹¤:

1. **ë¶„í• **: Q, K, Vë¥¼ $h$ê°œì˜ í—¤ë“œë¡œ ë‚˜ëˆ•ë‹ˆë‹¤ (ì°¨ì›ì„ $h$ë“±ë¶„)
2. **ë³‘ë ¬ ì–´í…ì…˜**: ê° í—¤ë“œê°€ ë…ë¦½ì ìœ¼ë¡œ Scaled Dot-Product Attention ìˆ˜í–‰
3. **ë³‘í•©**: ëª¨ë“  í—¤ë“œì˜ ì¶œë ¥ì„ ì´ì–´ ë¶™ì…ë‹ˆë‹¤(Concatenate)
4. **ì„ í˜• ë³€í™˜**: ìµœì¢… ê°€ì¤‘ì¹˜ í–‰ë ¬ë¡œ ì›ë˜ ì°¨ì›ìœ¼ë¡œ ë³€í™˜

$$\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h)W^O$$

$$\text{where} \quad \text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$

- $h$: í—¤ë“œ ìˆ˜ (ë³´í†µ 8 ë˜ëŠ” 12)
- $W_i^Q, W_i^K, W_i^V$: ê° í—¤ë“œë³„ í”„ë¡œì ì…˜ ê°€ì¤‘ì¹˜
- $W^O$: ì¶œë ¥ í”„ë¡œì ì…˜ ê°€ì¤‘ì¹˜

í•µì‹¬ì€ **ì´ ê³„ì‚°ëŸ‰ì´ ë‹¨ì¼ í—¤ë“œì™€ ê±°ì˜ ê°™ë‹¤**ëŠ” ì ì…ë‹ˆë‹¤! ì°¨ì›ì„ $h$ë“±ë¶„í•´ì„œ ê° í—¤ë“œì— ë°°ë¶„í•˜ë‹ˆê¹Œìš”. ì˜ˆë¥¼ ë“¤ì–´ 512ì°¨ì›ì„ 8ê°œ í—¤ë“œë¡œ ë‚˜ëˆ„ë©´, ê° í—¤ë“œëŠ” 64ì°¨ì›ë§Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.

## ì‹¤ìŠµ: ì§ì ‘ í•´ë³´ê¸°

### Scaled Dot-Product Attention êµ¬í˜„

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import math

def scaled_dot_product_attention(query, key, value, mask=None):
    """
    Scaled Dot-Product Attention êµ¬í˜„

    Args:
        query: (batch, seq_len, d_k) - ì§ˆë¬¸ ë²¡í„°
        key:   (batch, seq_len, d_k) - ì¹´íƒˆë¡œê·¸ ë²¡í„°
        value: (batch, seq_len, d_v) - ì‹¤ì œ ì •ë³´ ë²¡í„°
        mask:  ì–´í…ì…˜ ë§ˆìŠ¤í¬ (ì„ íƒì‚¬í•­)
    """
    d_k = query.size(-1)  # Key ë²¡í„°ì˜ ì°¨ì›

    # 1ë‹¨ê³„: Queryì™€ Keyì˜ ë‚´ì ìœ¼ë¡œ ìœ ì‚¬ë„ ê³„ì‚°
    scores = torch.matmul(query, key.transpose(-2, -1))  # (batch, seq_len, seq_len)

    # 2ë‹¨ê³„: âˆšd_kë¡œ ìŠ¤ì¼€ì¼ë§ (ë³¼ë¥¨ ì¡°ì ˆ!)
    scores = scores / math.sqrt(d_k)

    # ë§ˆìŠ¤í¬ ì ìš© (í•„ìš” ì‹œ)
    if mask is not None:
        scores = scores.masked_fill(mask == 0, float('-inf'))

    # 3ë‹¨ê³„: Softmaxë¡œ ê°€ì¤‘ì¹˜ ë³€í™˜
    attention_weights = F.softmax(scores, dim=-1)

    # 4ë‹¨ê³„: ê°€ì¤‘ì¹˜ë¡œ Value í•©ì‚°
    output = torch.matmul(attention_weights, value)

    return output, attention_weights

# í…ŒìŠ¤íŠ¸: 4ê°œ í† í°(ë˜ëŠ” ì´ë¯¸ì§€ íŒ¨ì¹˜), 64ì°¨ì›
batch_size = 1
seq_len = 4  # ì˜ˆ: ì´ë¯¸ì§€ íŒ¨ì¹˜ 4ê°œ
d_k = 64     # ì„ë² ë”© ì°¨ì›

# ì„ì˜ì˜ Q, K, V ìƒì„±
Q = torch.randn(batch_size, seq_len, d_k)
K = torch.randn(batch_size, seq_len, d_k)
V = torch.randn(batch_size, seq_len, d_k)

output, weights = scaled_dot_product_attention(Q, K, V)

print(f"ì…ë ¥ Q í¬ê¸°: {Q.shape}")           # [1, 4, 64]
print(f"ì–´í…ì…˜ ê°€ì¤‘ì¹˜ í¬ê¸°: {weights.shape}")  # [1, 4, 4] â€” 4x4 ê´€ê³„ ë§µ!
print(f"ì¶œë ¥ í¬ê¸°: {output.shape}")          # [1, 4, 64]
print(f"\nì–´í…ì…˜ ê°€ì¤‘ì¹˜ (ê° í–‰ì˜ í•© = 1):")
print(weights[0])
```

### Multi-Head Attention êµ¬í˜„

```python
class MultiHeadAttention(nn.Module):
    """
    Multi-Head Attention êµ¬í˜„
    ì—¬ëŸ¬ ê´€ì ì—ì„œ ë™ì‹œì— ì–´í…ì…˜ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤
    """
    def __init__(self, d_model=512, num_heads=8):
        super().__init__()
        assert d_model % num_heads == 0, "d_modelì€ num_headsë¡œ ë‚˜ëˆ ì ¸ì•¼ í•©ë‹ˆë‹¤"

        self.d_model = d_model       # ì „ì²´ ëª¨ë¸ ì°¨ì› (ì˜ˆ: 512)
        self.num_heads = num_heads   # í—¤ë“œ ìˆ˜ (ì˜ˆ: 8)
        self.d_k = d_model // num_heads  # í—¤ë“œë‹¹ ì°¨ì› (ì˜ˆ: 64)

        # Q, K, V í”„ë¡œì ì…˜ ë ˆì´ì–´ (í•œ ë²ˆì— ëª¨ë“  í—¤ë“œìš©)
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)

        # ì¶œë ¥ í”„ë¡œì ì…˜
        self.W_o = nn.Linear(d_model, d_model)

    def forward(self, x, mask=None):
        batch_size, seq_len, _ = x.shape

        # 1) Q, K, V ìƒì„± í›„ í—¤ë“œë³„ë¡œ ë¶„í• 
        Q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
        K = self.W_k(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
        V = self.W_v(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
        # ê²°ê³¼ í¬ê¸°: (batch, num_heads, seq_len, d_k)

        # 2) ê° í—¤ë“œì—ì„œ Scaled Dot-Product Attention ìˆ˜í–‰
        attn_output, attn_weights = scaled_dot_product_attention(Q, K, V, mask)
        # attn_output: (batch, num_heads, seq_len, d_k)

        # 3) í—¤ë“œ ì¶œë ¥ ë³‘í•© (Concatenate)
        attn_output = attn_output.transpose(1, 2).contiguous().view(
            batch_size, seq_len, self.d_model
        )

        # 4) ìµœì¢… ì„ í˜• ë³€í™˜
        output = self.W_o(attn_output)

        return output, attn_weights

# í…ŒìŠ¤íŠ¸
d_model = 512
num_heads = 8
seq_len = 16    # ì˜ˆ: 4x4 ì´ë¯¸ì§€ íŒ¨ì¹˜
batch_size = 2

mha = MultiHeadAttention(d_model=d_model, num_heads=num_heads)
x = torch.randn(batch_size, seq_len, d_model)

output, weights = mha(x)
print(f"ì…ë ¥ í¬ê¸°: {x.shape}")            # [2, 16, 512]
print(f"ì¶œë ¥ í¬ê¸°: {output.shape}")        # [2, 16, 512] â€” ì…ë ¥ê³¼ ë™ì¼!
print(f"ì–´í…ì…˜ ê°€ì¤‘ì¹˜: {weights.shape}")    # [2, 8, 16, 16] â€” 8ê°œ í—¤ë“œì˜ ê´€ê³„ ë§µ
print(f"í•™ìŠµ íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in mha.parameters()):,}")
```

### ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì‹œê°í™”

```python
import matplotlib.pyplot as plt

# ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì‹œê°í™” (4ê°œ íŒ¨ì¹˜ ê°„ì˜ ê´€ê³„)
Q = torch.randn(1, 4, 64)
K = torch.randn(1, 4, 64)
V = torch.randn(1, 4, 64)

_, weights = scaled_dot_product_attention(Q, K, V)

# ì–´í…ì…˜ ë§µ ì‹œê°í™”
fig, ax = plt.subplots(figsize=(6, 5))
im = ax.imshow(weights[0].detach().numpy(), cmap='Blues', vmin=0, vmax=1)

# íŒ¨ì¹˜ ë¼ë²¨ ì„¤ì •
labels = ['íŒ¨ì¹˜1', 'íŒ¨ì¹˜2', 'íŒ¨ì¹˜3', 'íŒ¨ì¹˜4']
ax.set_xticks(range(4))
ax.set_yticks(range(4))
ax.set_xticklabels(labels)
ax.set_yticklabels(labels)
ax.set_xlabel('Key (ì°¸ì¡° ëŒ€ìƒ)')
ax.set_ylabel('Query (ì§ˆë¬¸ì)')
ax.set_title('Self-Attention ê°€ì¤‘ì¹˜ ë§µ')

# ê° ì…€ì— ê°’ í‘œì‹œ
for i in range(4):
    for j in range(4):
        ax.text(j, i, f'{weights[0][i][j]:.2f}', ha='center', va='center')

plt.colorbar(im)
plt.tight_layout()
plt.show()
```

## ë” ê¹Šì´ ì•Œì•„ë³´ê¸°

### "Attention Is All You Need" â€” AI ì—­ì‚¬ë¥¼ ë°”ê¾¼ í•œ í¸ì˜ ë…¼ë¬¸

2017ë…„, Google Brainì˜ ì—°êµ¬ì› 8ëª…ì´ ë°œí‘œí•œ ë…¼ë¬¸ *"Attention Is All You Need"*ëŠ” AI ì—­ì‚¬ì˜ ì „í™˜ì ì´ ë©ë‹ˆë‹¤. ì´ ë…¼ë¬¸ì€ ë‹¹ì‹œ ì§€ë°°ì ì´ì—ˆë˜ RNN(ìˆœí™˜ ì‹ ê²½ë§)ê³¼ CNN ì—†ì´, **ì˜¤ì§ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ë§Œìœ¼ë¡œ** ì‹œí€€ìŠ¤ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆë‹¤ëŠ” íŒŒê²©ì ì¸ ì£¼ì¥ì„ í–ˆì£ .

ë†€ë¼ìš´ ê²ƒì€ ì´ ë…¼ë¬¸ì˜ ì˜í–¥ë ¥ì…ë‹ˆë‹¤. 2025ë…„ ê¸°ì¤€ **17ë§Œ íšŒ ì´ìƒ ì¸ìš©**ë˜ì–´, 21ì„¸ê¸° ê°€ì¥ ë§ì´ ì¸ìš©ëœ ë…¼ë¬¸ ì¤‘ í•˜ë‚˜ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤. 2024ë…„ NVIDIA GTC ì»¨í¼ëŸ°ìŠ¤ì—ì„œ ì  ìŠ¨ í™© CEOëŠ” 8ëª…ì˜ ê³µë™ ì €ìë¥¼ ë¬´ëŒ€ì— ì´ˆëŒ€í•˜ë©° **"ì—¬ëŸ¬ë¶„ì´ ì„¸ê³„ë¥¼ ë³€í˜í–ˆë‹¤(You Transformed the World)"**ë¼ê³  ë§í–ˆëŠ”ë°, ì´ ë§ì—ëŠ” "Transformer"ë¼ëŠ” ëª¨ë¸ ì´ë¦„ê³¼ "ë³€í˜(transform)"ì´ë¼ëŠ” ì´ì¤‘ ì˜ë¯¸ê°€ ë‹´ê²¨ ìˆì—ˆìŠµë‹ˆë‹¤.

í¥ë¯¸ë¡œìš´ ì ì€ 8ëª…ì˜ ì €ì ëŒ€ë¶€ë¶„ì´ ì´í›„ êµ¬ê¸€ì„ ë– ë‚˜ ê°ì AI ìŠ¤íƒ€íŠ¸ì—…ì„ ì°½ì—…í–ˆë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. Aidan GomezëŠ” Cohereë¥¼, Noam ShazeerëŠ” Character.AIë¥¼ ë§Œë“¤ì—ˆì£ .

### Self-Attentionì˜ ì‹œê°„ ë³µì¡ë„

Self-Attentionì˜ ì‹œê°„ ë³µì¡ë„ëŠ” $O(n^2 \cdot d)$ì…ë‹ˆë‹¤. ì—¬ê¸°ì„œ $n$ì€ ì‹œí€€ìŠ¤ ê¸¸ì´, $d$ëŠ” ì°¨ì› ìˆ˜ì…ë‹ˆë‹¤. ì´ê²ƒì€ ëª¨ë“  ìœ„ì¹˜ ìŒì˜ ê´€ê³„ë¥¼ ê³„ì‚°í•˜ê¸° ë•Œë¬¸ì¸ë°ìš”, ì‹œí€€ìŠ¤ê°€ ê¸¸ì–´ì§€ë©´ **ê³„ì‚°ëŸ‰ì´ ì œê³±ìœ¼ë¡œ ëŠ˜ì–´ë‚˜ëŠ”** ê²ƒì´ ì£¼ìš” ë‹¨ì ì…ë‹ˆë‹¤.

| ë°©ì‹ | ì‹œê°„ ë³µì¡ë„ | ì¥ê±°ë¦¬ ì˜ì¡´ì„± | ë³‘ë ¬í™” |
|------|-----------|-------------|--------|
| RNN | $O(n \cdot d^2)$ | ì–´ë ¤ì›€ | ë¶ˆê°€ëŠ¥ |
| CNN | $O(k \cdot n \cdot d^2)$ | ë ˆì´ì–´ ìˆ˜ì— ë¹„ë¡€ | ê°€ëŠ¥ |
| Self-Attention | $O(n^2 \cdot d)$ | **ë‹¨ì¼ ë ˆì´ì–´** | **ê°€ëŠ¥** |

ì´ $O(n^2)$ ë¬¸ì œë¥¼ í•´ê²°í•˜ë ¤ëŠ” ë…¸ë ¥ì´ ì´í›„ [Swin Transformer](./04-swin-transformer.md)ì˜ ìœˆë„ìš° ì–´í…ì…˜ ë“±ìœ¼ë¡œ ì´ì–´ì§‘ë‹ˆë‹¤.

## í”í•œ ì˜¤í•´ì™€ íŒ

> âš ï¸ **í”í•œ ì˜¤í•´**: "ì–´í…ì…˜ ê°€ì¤‘ì¹˜ê°€ ë†’ìœ¼ë©´ ê·¸ ë¶€ë¶„ì´ ì¤‘ìš”í•˜ë‹¤"ê³  í•´ì„í•˜ê¸° ì‰½ì§€ë§Œ, ì‹¤ì œë¡œ ì–´í…ì…˜ ê°€ì¤‘ì¹˜ëŠ” **ìƒê´€ê´€ê³„(correlation)**ë¥¼ ë‚˜íƒ€ë‚´ì§€ **ì¸ê³¼ê´€ê³„(causation)**ë¥¼ ì˜ë¯¸í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì–´í…ì…˜ ê°€ì¤‘ì¹˜ë§Œìœ¼ë¡œ ëª¨ë¸ì˜ "ì´ìœ "ë¥¼ ì„¤ëª…í•˜ëŠ” ê²ƒì€ ì£¼ì˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.

> ğŸ’¡ **ì•Œê³  ê³„ì…¨ë‚˜ìš”?**: "Attention Is All You Need"ë¼ëŠ” ë…¼ë¬¸ ì œëª©ì€ Beatlesì˜ "All You Need Is Love"ë¥¼ íŒ¨ëŸ¬ë””í•œ ê²ƒì´ë¼ëŠ” ì„¤ì´ ìˆìŠµë‹ˆë‹¤. ì €ìë“¤ì˜ ìœ ë¨¸ ê°ê°ì´ ì—­ì‚¬ì  ë…¼ë¬¸ ì œëª©ì— ë…¹ì•„ë“  ì…ˆì´ì£ .

> ğŸ”¥ **ì‹¤ë¬´ íŒ**: PyTorch 2.0+ì—ì„œëŠ” `torch.nn.functional.scaled_dot_product_attention()`ì´ ë‚´ì¥ë˜ì–´ ìˆì–´, Flash Attention ë“± ìµœì í™”ëœ êµ¬í˜„ì„ ìë™ìœ¼ë¡œ í™œìš©í•©ë‹ˆë‹¤. ì§ì ‘ êµ¬í˜„ë³´ë‹¤ ì´ê²ƒì„ ì“°ëŠ” ê²Œ ì‹¤ë¬´ì—ì„œëŠ” í›¨ì”¬ ë¹ ë¦…ë‹ˆë‹¤.

> âš ï¸ **í”í•œ ì˜¤í•´**: "Multi-Head Attentionì€ ê³„ì‚°ëŸ‰ì´ í—¤ë“œ ìˆ˜ë§Œí¼ ëŠ˜ì–´ë‚œë‹¤"ê³  ìƒê°í•˜ê¸° ì‰½ì§€ë§Œ, ì°¨ì›ì„ í—¤ë“œ ìˆ˜ë¡œ ë‚˜ëˆ„ê¸° ë•Œë¬¸ì— **ì´ ê³„ì‚°ëŸ‰ì€ ë‹¨ì¼ í—¤ë“œì™€ ê±°ì˜ ë™ì¼**í•©ë‹ˆë‹¤. 8ê°œ í—¤ë“œ = 8ë°° ë¹„ìš©ì´ ì•„ë‹ˆì—ìš”!

## í•µì‹¬ ì •ë¦¬

| ê°œë… | ì„¤ëª… |
|------|------|
| Attention | ì…ë ¥ì˜ ì¤‘ìš”í•œ ë¶€ë¶„ì— ì„ íƒì ìœ¼ë¡œ ì§‘ì¤‘í•˜ëŠ” ë©”ì»¤ë‹ˆì¦˜ |
| Query (Q) | "ë¬´ì—‡ì„ ì°¾ê³  ìˆëŠ”ì§€" â€” ì§ˆë¬¸ ë²¡í„° |
| Key (K) | "ë¬´ì—‡ì´ ìˆëŠ”ì§€" â€” ë§¤ì¹­ìš© ì¹´íƒˆë¡œê·¸ ë²¡í„° |
| Value (V) | "ì‹¤ì œ ì •ë³´" â€” ê°€ì¤‘ í•©ì‚°ë  ê°’ ë²¡í„° |
| Scaled Dot-Product | $\text{softmax}(QK^T / \sqrt{d_k})V$ â€” í•µì‹¬ ì–´í…ì…˜ ì—°ì‚° |
| $\sqrt{d_k}$ ìŠ¤ì¼€ì¼ë§ | ë‚´ì  ê°’ì´ ë„ˆë¬´ ì»¤ì ¸ Softmaxê°€ ê·¹ë‹¨í™”ë˜ëŠ” ê²ƒì„ ë°©ì§€ |
| Self-Attention | Q, K, Vê°€ ëª¨ë‘ ê°™ì€ ì…ë ¥ì—ì„œ íŒŒìƒ â€” ìê¸° ì°¸ì¡° |
| Multi-Head Attention | ì—¬ëŸ¬ ê´€ì ì—ì„œ ë™ì‹œì— ì–´í…ì…˜ ìˆ˜í–‰, ì´ ë¹„ìš©ì€ ê±°ì˜ ë™ì¼ |
| ì‹œê°„ ë³µì¡ë„ | $O(n^2 \cdot d)$ â€” ì‹œí€€ìŠ¤ ê¸¸ì´ì— ì œê³± ë¹„ë¡€ |

## ë‹¤ìŒ ì„¹ì…˜ ë¯¸ë¦¬ë³´ê¸°

ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì˜ ì›ë¦¬ë¥¼ ì´í•´í–ˆë‹¤ë©´, ì´ì œ ì´ê²ƒì´ ì–´ë–»ê²Œ í•˜ë‚˜ì˜ ì™„ì „í•œ ì•„í‚¤í…ì²˜ë¡œ ì¡°ë¦½ë˜ëŠ”ì§€ ë³¼ ì°¨ë¡€ì…ë‹ˆë‹¤. ë‹¤ìŒ [Transformer ì•„í‚¤í…ì²˜](./02-transformer-basics.md)ì—ì„œëŠ” Multi-Head Attentionì„ ê¸°ë°˜ìœ¼ë¡œ Encoder-Decoder êµ¬ì¡°ê°€ ì–´ë–»ê²Œ ë§Œë“¤ì–´ì§€ê³ , Positional Encodingì€ ì™œ í•„ìš”í•œì§€ ë°°ì›Œë´…ë‹ˆë‹¤.

## ì°¸ê³  ìë£Œ

- [Attention Is All You Need (Vaswani et al., 2017)](https://arxiv.org/abs/1706.03762) - Transformerì˜ ì‹œì‘ì , ëª¨ë“  ê²ƒì˜ ì›ì „
- [Understanding and Coding Self-Attention (Sebastian Raschka, 2023)](https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html) - Self-Attentionì„ ì²˜ìŒë¶€í„° ì½”ë”©í•˜ëŠ” í›Œë¥­í•œ íŠœí† ë¦¬ì–¼
- [The Illustrated Transformer (Jay Alammar)](https://jalammar.github.io/illustrated-transformer/) - ì‹œê°ì ìœ¼ë¡œ ê°€ì¥ ì˜ ì„¤ëª…ëœ Transformer í•´ì„¤
- [Multi-Head Attention - Dive into Deep Learning](https://d2l.ai/chapter_attention-mechanisms-and-transformers/multihead-attention.html) - ìˆ˜ì‹ê³¼ ì½”ë“œì˜ ê· í˜•ì´ ì¢‹ì€ êµì¬
- [An Intuition for Attention (Jay Mody)](https://jaykmody.com/blog/attention-intuition/) - Q, K, Vì˜ ì§ê´€ì„ ê¹Šì´ ìˆê²Œ ì„¤ëª…
