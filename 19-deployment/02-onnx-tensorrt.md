# ONNXì™€ TensorRT

> ì¶”ë¡  ê°€ì†í™”

## ê°œìš”

[ì´ì „ ì„¹ì…˜](./01-model-optimization.md)ì—ì„œ ëª¨ë¸ì„ ì••ì¶•í•˜ëŠ” ë°©ë²•ì„ ë°°ì› ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ PyTorchë¡œ í•™ìŠµí•œ ëª¨ë¸ì€ **PyTorch ëŸ°íƒ€ì„**ì—ì„œë§Œ ì‹¤í–‰ë©ë‹ˆë‹¤. ì‹¤ì œ ë°°í¬ í™˜ê²½ì—ì„œëŠ” ë” ë¹ ë¥´ê³  íš¨ìœ¨ì ì¸ **ì¶”ë¡  ì „ìš© ì—”ì§„**ì´ í•„ìš”í•©ë‹ˆë‹¤. ì´ ì„¹ì…˜ì—ì„œëŠ” **ONNX**ë¡œ í”„ë ˆì„ì›Œí¬ ê°„ í˜¸í™˜ì„±ì„ í™•ë³´í•˜ê³ , **TensorRT**ë¡œ NVIDIA GPUì—ì„œ ìµœëŒ€ ì„±ëŠ¥ì„ ëŒì–´ë‚´ëŠ” ë°©ë²•ì„ ë°°ì›ë‹ˆë‹¤.

**ì„ ìˆ˜ ì§€ì‹**:
- [ëª¨ë¸ ìµœì í™”](./01-model-optimization.md)
- PyTorch ê¸°ë³¸ ì‚¬ìš©ë²•

**í•™ìŠµ ëª©í‘œ**:
- ONNX í¬ë§·ì˜ ì—­í• ê³¼ ëª¨ë¸ ë³€í™˜ ë°©ë²• ì´í•´í•˜ê¸°
- TensorRT ìµœì í™” ì›ë¦¬ì™€ ì ìš© ë°©ë²• ìµíˆê¸°
- ONNX Runtimeê³¼ TensorRT ì¶”ë¡  êµ¬í˜„í•˜ê¸°

## ì™œ ì•Œì•„ì•¼ í• ê¹Œ?

> ğŸ’¡ **ë¹„ìœ **: ì˜í™”ë¥¼ ë§Œë“¤ ë•Œ **ì´¬ì˜(í•™ìŠµ)**ê³¼ **ìƒì˜(ì¶”ë¡ )**ì€ ë‹¤ë¥¸ ì¥ë¹„ê°€ í•„ìš”í•©ë‹ˆë‹¤. ì´¬ì˜ì—ëŠ” ëŒ€í˜• ì¹´ë©”ë¼ì™€ ì¡°ëª… ì¥ë¹„ê°€, ìƒì˜ì—ëŠ” í”„ë¡œì í„°ì™€ ìŠ¤í¬ë¦°ì´ í•„ìš”í•˜ì£ . PyTorchëŠ” ì´¬ì˜ ì¥ë¹„(í•™ìŠµ), TensorRTëŠ” ìƒì˜ ì¥ë¹„(ì¶”ë¡ )ì…ë‹ˆë‹¤. ìƒì˜ê´€ì—ì„œ ì´¬ì˜ ì¥ë¹„ë¥¼ ì“¸ í•„ìš”ê°€ ì—†ë“¯, ì¶”ë¡ ì—ëŠ” **ì¶”ë¡  ì „ìš© ì—”ì§„**ì´ í›¨ì”¬ íš¨ìœ¨ì ì…ë‹ˆë‹¤.

**PyTorch vs ì¶”ë¡  ì—”ì§„ ë¹„êµ:**

| í•­ëª© | PyTorch | ONNX Runtime | TensorRT |
|------|---------|--------------|----------|
| **ì£¼ ìš©ë„** | í•™ìŠµ + ì—°êµ¬ | ë²”ìš© ì¶”ë¡  | GPU ìµœì í™” ì¶”ë¡  |
| **ì†ë„** | ê¸°ì¤€ (1x) | 1.5-2x | 2-5x |
| **ë©”ëª¨ë¦¬** | ë†’ìŒ | ì¤‘ê°„ | ë‚®ìŒ |
| **í”Œë«í¼** | Python ì¤‘ì‹¬ | ë‹¤ì–‘ (C++, C#, Java) | NVIDIA GPU ì „ìš© |
| **ë™ì  ê·¸ë˜í”„** | ì§€ì› | ì œí•œì  | ë¯¸ì§€ì› |

ì‹¤ì œ ì‚¬ë¡€: ResNet-50 ì¶”ë¡  ì„±ëŠ¥ (NVIDIA V100 GPU)
- PyTorch: 5.2 ms/ì´ë¯¸ì§€
- ONNX Runtime: 3.1 ms (1.7ë°° ë¹ ë¦„)
- TensorRT FP16: 1.4 ms (3.7ë°° ë¹ ë¦„)
- TensorRT INT8: 0.8 ms (6.5ë°° ë¹ ë¦„)

## í•µì‹¬ ê°œë…

### ê°œë… 1: ONNXë€?

> ğŸ’¡ **ë¹„ìœ **: ONNXëŠ” **ì„¸ê³„ ê³µí†µì–´(ì˜ì–´)**ì™€ ê°™ìŠµë‹ˆë‹¤. í•œêµ­ì–´ë¡œ ì“´ ì†Œì„¤ì„ ì¼ë³¸ì—ì„œ ì½ìœ¼ë ¤ë©´ ë²ˆì—­ì´ í•„ìš”í•˜ì£ . ë§ˆì°¬ê°€ì§€ë¡œ PyTorch ëª¨ë¸ì„ TensorFlowë‚˜ ë‹¤ë¥¸ í™˜ê²½ì—ì„œ ì“°ë ¤ë©´ ê³µí†µ í˜•ì‹ì´ í•„ìš”í•©ë‹ˆë‹¤. ONNXê°€ ë°”ë¡œ ê·¸ **ì‹ ê²½ë§ì˜ ê³µí†µì–´**ì…ë‹ˆë‹¤.

**ONNX (Open Neural Network Exchange)**ëŠ” Facebook(Meta)ê³¼ Microsoftê°€ 2017ë…„ì— ë°œí‘œí•œ ì˜¤í”ˆ í¬ë§·ì…ë‹ˆë‹¤. ë‹¤ì–‘í•œ ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ ê°„ì˜ ëª¨ë¸ í˜¸í™˜ì„±ì„ ì œê³µí•©ë‹ˆë‹¤.

**ONNX ìƒíƒœê³„:**

| êµ¬ì„±ìš”ì†Œ | ì„¤ëª… |
|----------|------|
| **ONNX í¬ë§·** | ëª¨ë¸ êµ¬ì¡°ì™€ ê°€ì¤‘ì¹˜ë¥¼ ì €ì¥í•˜ëŠ” í‘œì¤€ í˜•ì‹ (.onnx) |
| **ONNX Runtime** | Microsoftì˜ í¬ë¡œìŠ¤ í”Œë«í¼ ì¶”ë¡  ì—”ì§„ |
| **ONNX ì—°ì‚°ì** | 150+ í‘œì¤€ ì—°ì‚°ì ì •ì˜ (Conv, MatMul, ReLU ë“±) |
| **ONNX ë³€í™˜ê¸°** | ê° í”„ë ˆì„ì›Œí¬ â†’ ONNX ë³€í™˜ ë„êµ¬ |

```python
import torch
import torch.onnx
from torchvision import models

# 1. PyTorch ëª¨ë¸ ì¤€ë¹„
model = models.resnet18(pretrained=True)
model.eval()

# ë”ë¯¸ ì…ë ¥ (ONNX ë³€í™˜ì— í•„ìš”)
dummy_input = torch.randn(1, 3, 224, 224)

# 2. ONNXë¡œ ë‚´ë³´ë‚´ê¸°
torch.onnx.export(
    model,                           # ëª¨ë¸
    dummy_input,                     # ì˜ˆì‹œ ì…ë ¥
    "resnet18.onnx",                 # ì €ì¥ ê²½ë¡œ
    export_params=True,              # ê°€ì¤‘ì¹˜ í¬í•¨
    opset_version=17,                # ONNX ë²„ì „ (ìµœì‹  ê¶Œì¥)
    do_constant_folding=True,        # ìƒìˆ˜ í´ë”© ìµœì í™”
    input_names=['input'],           # ì…ë ¥ ì´ë¦„
    output_names=['output'],         # ì¶œë ¥ ì´ë¦„
    dynamic_axes={                   # ë™ì  ë°°ì¹˜ í¬ê¸° ì§€ì›
        'input': {0: 'batch_size'},
        'output': {0: 'batch_size'}
    }
)

print("ONNX ëª¨ë¸ ì €ì¥ ì™„ë£Œ: resnet18.onnx")
```

```python
# 3. ONNX ëª¨ë¸ ê²€ì¦
import onnx

# ëª¨ë¸ ë¡œë“œ
onnx_model = onnx.load("resnet18.onnx")

# ëª¨ë¸ êµ¬ì¡° ê²€ì¦
onnx.checker.check_model(onnx_model)
print("ONNX ëª¨ë¸ ê²€ì¦ í†µê³¼!")

# ëª¨ë¸ ì •ë³´ í™•ì¸
print(f"ONNX opset ë²„ì „: {onnx_model.opset_import[0].version}")
print(f"ì…ë ¥: {onnx_model.graph.input[0].name}")
print(f"ì¶œë ¥: {onnx_model.graph.output[0].name}")
```

> ğŸ’¡ **ì•Œê³  ê³„ì…¨ë‚˜ìš”?**: ONNXì˜ íƒ„ìƒ ë°°ê²½ì€ í¥ë¯¸ë¡­ìŠµë‹ˆë‹¤. 2017ë…„ ë‹¹ì‹œ PyTorch, TensorFlow, Caffe2 ë“± í”„ë ˆì„ì›Œí¬ ê°„ ëª¨ë¸ ê³µìœ ê°€ ë¶ˆê°€ëŠ¥í•´ ì—°êµ¬ìë“¤ì´ ë¶ˆí¸ì„ ê²ªì—ˆìŠµë‹ˆë‹¤. Facebookê³¼ Microsoftê°€ ì˜ê¸°íˆ¬í•©í•´ ë§Œë“  ONNXëŠ” ì´ì œ ì‚¬ì‹¤ìƒ ì—…ê³„ í‘œì¤€ì´ ë˜ì—ˆê³ , AWS, Google, NVIDIA ë“± ëª¨ë“  ì£¼ìš” ê¸°ì—…ì´ ì§€ì›í•©ë‹ˆë‹¤.

### ê°œë… 2: ONNX Runtimeìœ¼ë¡œ ì¶”ë¡ í•˜ê¸°

ONNX Runtimeì€ Microsoftê°€ ê°œë°œí•œ **ê³ ì„±ëŠ¥ ì¶”ë¡  ì—”ì§„**ì…ë‹ˆë‹¤. CPU, GPU, ì—£ì§€ ë””ë°”ì´ìŠ¤ ë“± ë‹¤ì–‘í•œ í™˜ê²½ì—ì„œ ONNX ëª¨ë¸ì„ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
# pip install onnxruntime-gpu  # GPUìš©
# pip install onnxruntime      # CPUìš©

import onnxruntime as ort
import numpy as np
from PIL import Image
import torchvision.transforms as transforms

# 1. ONNX Runtime ì„¸ì…˜ ìƒì„±
# GPU ì‚¬ìš© ì‹œ CUDAExecutionProvider, CPUëŠ” CPUExecutionProvider
providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']
session = ort.InferenceSession("resnet18.onnx", providers=providers)

# í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ í”„ë¡œë°”ì´ë” í™•ì¸
print(f"ì‚¬ìš© ì¤‘ì¸ í”„ë¡œë°”ì´ë”: {session.get_providers()}")

# 2. ì…ë ¥/ì¶œë ¥ ì •ë³´ í™•ì¸
input_info = session.get_inputs()[0]
output_info = session.get_outputs()[0]
print(f"ì…ë ¥: {input_info.name}, í˜•íƒœ: {input_info.shape}, íƒ€ì…: {input_info.type}")
print(f"ì¶œë ¥: {output_info.name}, í˜•íƒœ: {output_info.shape}")
```

```python
# 3. ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ë° ì¶”ë¡ 
def preprocess_image(image_path):
    """ImageNet ì „ì²˜ë¦¬"""
    transform = transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize(
            mean=[0.485, 0.456, 0.406],
            std=[0.229, 0.224, 0.225]
        )
    ])
    image = Image.open(image_path).convert('RGB')
    return transform(image).unsqueeze(0).numpy()  # numpyë¡œ ë³€í™˜

# ì¶”ë¡  ì‹¤í–‰
input_data = preprocess_image("test_image.jpg")
# ë˜ëŠ” ë”ë¯¸ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸
input_data = np.random.randn(1, 3, 224, 224).astype(np.float32)

# ONNX Runtime ì¶”ë¡ 
outputs = session.run(
    None,  # ëª¨ë“  ì¶œë ¥ ë°˜í™˜
    {input_info.name: input_data}
)

# ê²°ê³¼ í™•ì¸
predictions = outputs[0]
predicted_class = np.argmax(predictions[0])
confidence = np.max(outputs[0][0])
print(f"ì˜ˆì¸¡ í´ë˜ìŠ¤: {predicted_class}, ì‹ ë¢°ë„: {confidence:.4f}")
```

```python
# 4. ë°°ì¹˜ ì¶”ë¡  ë° ì„±ëŠ¥ ì¸¡ì •
import time

def benchmark_onnx(session, input_shape, num_iterations=100):
    """ONNX Runtime ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
    input_data = np.random.randn(*input_shape).astype(np.float32)
    input_name = session.get_inputs()[0].name

    # ì›Œë°ì—…
    for _ in range(10):
        session.run(None, {input_name: input_data})

    # ë²¤ì¹˜ë§ˆí¬
    start = time.time()
    for _ in range(num_iterations):
        session.run(None, {input_name: input_data})
    elapsed = time.time() - start

    avg_time = (elapsed / num_iterations) * 1000  # ms
    throughput = num_iterations / elapsed
    print(f"í‰ê·  ì¶”ë¡  ì‹œê°„: {avg_time:.2f} ms")
    print(f"ì²˜ë¦¬ëŸ‰: {throughput:.1f} images/sec")
    return avg_time

# ë°°ì¹˜ í¬ê¸°ë³„ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
for batch_size in [1, 4, 8, 16]:
    print(f"\në°°ì¹˜ í¬ê¸°: {batch_size}")
    benchmark_onnx(session, (batch_size, 3, 224, 224))
```

### ê°œë… 3: TensorRT ìµœì í™”

> ğŸ’¡ **ë¹„ìœ **: TensorRTëŠ” **F1 ë ˆì´ì‹±ì¹´ íŠœë‹**ê³¼ ê°™ìŠµë‹ˆë‹¤. ì¼ë°˜ ìë™ì°¨ë„ ë‹¬ë¦´ ìˆ˜ ìˆì§€ë§Œ, F1 íŒ€ì€ ì—”ì§„, ê³µê¸°ì—­í•™, íƒ€ì´ì–´ ëª¨ë“  ë¶€ë¶„ì„ **ê·¹í•œê¹Œì§€ ìµœì í™”**í•©ë‹ˆë‹¤. TensorRTë„ NVIDIA GPUì˜ ëª¨ë“  ê¸°ëŠ¥ì„ í™œìš©í•´ ì¶”ë¡  ì„±ëŠ¥ì„ ê·¹ëŒ€í™”í•©ë‹ˆë‹¤.

**TensorRT ìµœì í™” ê¸°ë²•:**

| ê¸°ë²• | ì„¤ëª… | íš¨ê³¼ |
|------|------|------|
| **ë ˆì´ì–´ í“¨ì „** | Conv+BN+ReLUë¥¼ í•˜ë‚˜ë¡œ í•©ì¹¨ | ë©”ëª¨ë¦¬ ëŒ€ì—­í­ â†“ |
| **ì •ë°€ë„ ë³€í™˜** | FP32 â†’ FP16/INT8 | 2-4ë°° ì†ë„ â†‘ |
| **ì»¤ë„ ìë™ íŠœë‹** | GPUì— ë§ëŠ” ìµœì  ì»¤ë„ ì„ íƒ | ìµœì  ì„±ëŠ¥ |
| **ë™ì  í…ì„œ ë©”ëª¨ë¦¬** | ë©”ëª¨ë¦¬ ì¬ì‚¬ìš© | ë©”ëª¨ë¦¬ â†“ |
| **ë‹¤ì¤‘ ìŠ¤íŠ¸ë¦¼** | ë³‘ë ¬ ì‹¤í–‰ | ì²˜ë¦¬ëŸ‰ â†‘ |

```python
# TensorRT ì„¤ì¹˜: pip install tensorrt
# ë˜ëŠ” NVIDIAì—ì„œ ì§ì ‘ ë‹¤ìš´ë¡œë“œ

import tensorrt as trt
import numpy as np

# TensorRT ë¡œê±° ì„¤ì •
TRT_LOGGER = trt.Logger(trt.Logger.WARNING)

def build_engine_from_onnx(onnx_path, engine_path, fp16=True, int8=False):
    """ONNX â†’ TensorRT ì—”ì§„ ë³€í™˜"""

    # ë¹Œë” ìƒì„±
    builder = trt.Builder(TRT_LOGGER)
    network_flags = 1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)
    network = builder.create_network(network_flags)

    # ONNX íŒŒì„œë¡œ ëª¨ë¸ ë¡œë“œ
    parser = trt.OnnxParser(network, TRT_LOGGER)
    with open(onnx_path, 'rb') as f:
        if not parser.parse(f.read()):
            for error in range(parser.num_errors):
                print(parser.get_error(error))
            return None

    # ë¹Œë” ì„¤ì •
    config = builder.create_builder_config()
    config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 1 << 30)  # 1GB

    # FP16 ëª¨ë“œ í™œì„±í™” (2ë°° ì†ë„ í–¥ìƒ)
    if fp16:
        config.set_flag(trt.BuilderFlag.FP16)
        print("FP16 ëª¨ë“œ í™œì„±í™”")

    # INT8 ëª¨ë“œ (ìº˜ë¦¬ë¸Œë ˆì´ì…˜ í•„ìš”)
    if int8:
        config.set_flag(trt.BuilderFlag.INT8)
        # config.int8_calibrator = MyCalibrator(...)  # ìº˜ë¦¬ë¸Œë ˆì´í„° í•„ìš”
        print("INT8 ëª¨ë“œ í™œì„±í™”")

    # ë™ì  ì…ë ¥ í¬ê¸° ì„¤ì •
    profile = builder.create_optimization_profile()
    profile.set_shape(
        "input",
        min=(1, 3, 224, 224),   # ìµœì†Œ ë°°ì¹˜
        opt=(8, 3, 224, 224),   # ìµœì  ë°°ì¹˜
        max=(32, 3, 224, 224)   # ìµœëŒ€ ë°°ì¹˜
    )
    config.add_optimization_profile(profile)

    # ì—”ì§„ ë¹Œë“œ (ì‹œê°„ì´ ê±¸ë¦¼)
    print("TensorRT ì—”ì§„ ë¹Œë“œ ì¤‘... (ëª‡ ë¶„ ì†Œìš”)")
    serialized_engine = builder.build_serialized_network(network, config)

    # ì—”ì§„ ì €ì¥
    with open(engine_path, 'wb') as f:
        f.write(serialized_engine)
    print(f"ì—”ì§„ ì €ì¥ ì™„ë£Œ: {engine_path}")

    return serialized_engine

# ì‚¬ìš© ì˜ˆì‹œ
# build_engine_from_onnx("resnet18.onnx", "resnet18.engine", fp16=True)
```

```python
# TensorRT ì—”ì§„ìœ¼ë¡œ ì¶”ë¡ 
import pycuda.driver as cuda
import pycuda.autoinit

class TensorRTInference:
    def __init__(self, engine_path):
        """TensorRT ì—”ì§„ ë¡œë“œ ë° ì¶”ë¡  ì¤€ë¹„"""
        # ì—”ì§„ ë¡œë“œ
        with open(engine_path, 'rb') as f:
            engine_data = f.read()

        runtime = trt.Runtime(TRT_LOGGER)
        self.engine = runtime.deserialize_cuda_engine(engine_data)
        self.context = self.engine.create_execution_context()

        # ì…ì¶œë ¥ ë²„í¼ ì¤€ë¹„
        self.inputs = []
        self.outputs = []
        self.bindings = []

        for i in range(self.engine.num_io_tensors):
            name = self.engine.get_tensor_name(i)
            dtype = trt.nptype(self.engine.get_tensor_dtype(name))
            shape = self.context.get_tensor_shape(name)

            size = trt.volume(shape)
            host_mem = cuda.pagelocked_empty(size, dtype)
            device_mem = cuda.mem_alloc(host_mem.nbytes)

            self.bindings.append(int(device_mem))

            if self.engine.get_tensor_mode(name) == trt.TensorIOMode.INPUT:
                self.inputs.append({'host': host_mem, 'device': device_mem, 'shape': shape})
            else:
                self.outputs.append({'host': host_mem, 'device': device_mem, 'shape': shape})

        self.stream = cuda.Stream()

    def infer(self, input_data):
        """ì¶”ë¡  ì‹¤í–‰"""
        # ì…ë ¥ ë°ì´í„° ë³µì‚¬
        np.copyto(self.inputs[0]['host'], input_data.ravel())
        cuda.memcpy_htod_async(
            self.inputs[0]['device'],
            self.inputs[0]['host'],
            self.stream
        )

        # ì¶”ë¡  ì‹¤í–‰
        self.context.execute_async_v2(
            bindings=self.bindings,
            stream_handle=self.stream.handle
        )

        # ì¶œë ¥ ë³µì‚¬
        cuda.memcpy_dtoh_async(
            self.outputs[0]['host'],
            self.outputs[0]['device'],
            self.stream
        )
        self.stream.synchronize()

        return self.outputs[0]['host'].reshape(self.outputs[0]['shape'])

# ì‚¬ìš© ì˜ˆì‹œ
# trt_infer = TensorRTInference("resnet18.engine")
# output = trt_infer.infer(input_data)
```

> âš ï¸ **í”í•œ ì˜¤í•´**: "TensorRTëŠ” ì–´ë µê³  ë³µì¡í•˜ë‹¤" â€” ì‹¤ì œë¡œ ONNXë¥¼ í†µí•œ ë³€í™˜ì€ 10ì¤„ ì´ë‚´ ì½”ë“œë¡œ ê°€ëŠ¥í•©ë‹ˆë‹¤. ë˜í•œ **trtexec** ì»¤ë§¨ë“œë¼ì¸ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ë©´ ì½”ë“œ ì—†ì´ë„ ë³€í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ê°œë… 4: ONNX Runtime + TensorRT í†µí•©

ê°€ì¥ ì‹¤ìš©ì ì¸ ë°©ë²•ì€ **ONNX Runtimeì˜ TensorRT Execution Provider**ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ONNX Runtimeì˜ ê°„í¸í•œ APIì™€ TensorRTì˜ ì„±ëŠ¥ì„ ë™ì‹œì— ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
import onnxruntime as ort
import numpy as np

def create_tensorrt_session(onnx_path, fp16=True):
    """ONNX Runtime + TensorRT ì„¸ì…˜ ìƒì„±"""

    # TensorRT í”„ë¡œë°”ì´ë” ì˜µì…˜
    trt_options = {
        'device_id': 0,  # GPU ë²ˆí˜¸
        'trt_max_workspace_size': 2 << 30,  # 2GB
        'trt_fp16_enable': fp16,
        'trt_engine_cache_enable': True,  # ì—”ì§„ ìºì‹œ (ì¬ì‚¬ìš©)
        'trt_engine_cache_path': './trt_cache',
    }

    # í”„ë¡œë°”ì´ë” ìˆœì„œ: TensorRT â†’ CUDA â†’ CPU
    providers = [
        ('TensorrtExecutionProvider', trt_options),
        'CUDAExecutionProvider',
        'CPUExecutionProvider'
    ]

    session = ort.InferenceSession(onnx_path, providers=providers)
    print(f"ì‚¬ìš© í”„ë¡œë°”ì´ë”: {session.get_providers()}")
    return session

# ì„¸ì…˜ ìƒì„±
session = create_tensorrt_session("resnet18.onnx", fp16=True)

# ì¶”ë¡  (ONNX Runtime API ë™ì¼)
input_data = np.random.randn(1, 3, 224, 224).astype(np.float32)
outputs = session.run(None, {'input': input_data})
print(f"ì¶œë ¥ í˜•íƒœ: {outputs[0].shape}")
```

```python
# ì„±ëŠ¥ ë¹„êµ: CPU vs CUDA vs TensorRT
import time

def compare_providers(onnx_path, input_shape, iterations=100):
    """ê° í”„ë¡œë°”ì´ë”ë³„ ì„±ëŠ¥ ë¹„êµ"""
    results = {}

    providers_list = [
        ('CPU', ['CPUExecutionProvider']),
        ('CUDA', ['CUDAExecutionProvider']),
        ('TensorRT', [
            ('TensorrtExecutionProvider', {
                'trt_fp16_enable': True,
                'trt_engine_cache_enable': True,
                'trt_engine_cache_path': './trt_cache'
            }),
            'CUDAExecutionProvider'
        ])
    ]

    input_data = np.random.randn(*input_shape).astype(np.float32)

    for name, providers in providers_list:
        try:
            session = ort.InferenceSession(onnx_path, providers=providers)
            input_name = session.get_inputs()[0].name

            # ì›Œë°ì—…
            for _ in range(10):
                session.run(None, {input_name: input_data})

            # ë²¤ì¹˜ë§ˆí¬
            start = time.time()
            for _ in range(iterations):
                session.run(None, {input_name: input_data})
            elapsed = time.time() - start

            avg_ms = (elapsed / iterations) * 1000
            results[name] = avg_ms
            print(f"{name}: {avg_ms:.2f} ms")
        except Exception as e:
            print(f"{name}: ì‚¬ìš© ë¶ˆê°€ - {e}")

    return results

# ë¹„êµ ì‹¤í–‰
# results = compare_providers("resnet18.onnx", (1, 3, 224, 224))
```

> ğŸ”¥ **ì‹¤ë¬´ íŒ**: ì²« ì‹¤í–‰ ì‹œ TensorRT ì—”ì§„ ë¹Œë“œì— ì‹œê°„ì´ ê±¸ë¦½ë‹ˆë‹¤. `trt_engine_cache_enable=True`ë¡œ ìºì‹œë¥¼ í™œì„±í™”í•˜ë©´ ë‘ ë²ˆì§¸ ì‹¤í–‰ë¶€í„°ëŠ” ì¦‰ì‹œ ì‹œì‘ë©ë‹ˆë‹¤. ë°°í¬ ì‹œì—ëŠ” ë¯¸ë¦¬ ë¹Œë“œí•œ ì—”ì§„ íŒŒì¼ì„ í•¨ê»˜ ë°°í¬í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.

### ê°œë… 5: trtexec ì»¤ë§¨ë“œë¼ì¸ ë„êµ¬

ì½”ë“œ ì—†ì´ ë¹ ë¥´ê²Œ ONNX â†’ TensorRT ë³€í™˜ê³¼ ë²¤ì¹˜ë§ˆí¬ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.

```bash
# ONNX â†’ TensorRT ì—”ì§„ ë³€í™˜
trtexec --onnx=resnet18.onnx \
        --saveEngine=resnet18.engine \
        --fp16 \
        --workspace=1024

# ë™ì  ë°°ì¹˜ í¬ê¸° ì„¤ì •
trtexec --onnx=resnet18.onnx \
        --saveEngine=resnet18_dynamic.engine \
        --minShapes=input:1x3x224x224 \
        --optShapes=input:8x3x224x224 \
        --maxShapes=input:32x3x224x224 \
        --fp16

# ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
trtexec --loadEngine=resnet18.engine \
        --iterations=1000 \
        --avgRuns=10

# INT8 ì–‘ìí™” (ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ë°ì´í„° í•„ìš”)
trtexec --onnx=resnet18.onnx \
        --saveEngine=resnet18_int8.engine \
        --int8 \
        --calib=calibration_cache.txt
```

## ë” ê¹Šì´ ì•Œì•„ë³´ê¸°: ìµœì‹  ë™í–¥

**2025ë…„ TensorRT 10.x ì£¼ìš” ì—…ë°ì´íŠ¸:**

1. **TensorRT-LLM í†µí•©**: LLM ì¶”ë¡  ìµœì í™” í†µí•©
2. **Transformer ìµœì í™”**: Flash Attention, KV ìºì‹œ ìë™ ìµœì í™”
3. **NVIDIA Model Optimizer ì—°ë™**: ì–‘ìí™”-TensorRT ì›í™œí•œ íŒŒì´í”„ë¼ì¸
4. **Hopper/Blackwell ì§€ì›**: FP8 ì—°ì‚°, Transformer Engine

**ë¹„ì „ ëª¨ë¸ë³„ ì†ë„ í–¥ìƒ (A100 GPU):**

| ëª¨ë¸ | PyTorch | TensorRT FP16 | ì†ë„ í–¥ìƒ |
|------|---------|---------------|----------|
| ResNet-50 | 4.2 ms | 1.1 ms | 3.8x |
| EfficientNet-B0 | 2.8 ms | 0.9 ms | 3.1x |
| ViT-B/16 | 6.5 ms | 2.1 ms | 3.1x |
| YOLO v8-n | 3.2 ms | 0.8 ms | 4.0x |
| SAM (encoder) | 45 ms | 12 ms | 3.8x |

## í•µì‹¬ ì •ë¦¬

| ê°œë… | ì„¤ëª… |
|------|------|
| **ONNX** | ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ í‘œì¤€ êµí™˜ í¬ë§·, í”„ë ˆì„ì›Œí¬ ê°„ í˜¸í™˜ì„± ì œê³µ |
| **ONNX Runtime** | Microsoftì˜ ë²”ìš© ì¶”ë¡  ì—”ì§„, ë‹¤ì–‘í•œ í”Œë«í¼ ì§€ì› |
| **TensorRT** | NVIDIA GPU ì „ìš© ì¶”ë¡  ìµœì í™” ì—”ì§„, ìµœê³  ì„±ëŠ¥ |
| **Execution Provider** | ONNX Runtimeì˜ ë°±ì—”ë“œ, TensorRT/CUDA/CPU ì„ íƒ ê°€ëŠ¥ |
| **ë ˆì´ì–´ í“¨ì „** | ì—¬ëŸ¬ ì—°ì‚°ì„ í•˜ë‚˜ë¡œ í•©ì³ ë©”ëª¨ë¦¬ ëŒ€ì—­í­ ì ˆì•½ |
| **trtexec** | TensorRT CLI ë„êµ¬, ë³€í™˜/ë²¤ì¹˜ë§ˆí¬/í”„ë¡œíŒŒì¼ë§ |

## ë‹¤ìŒ ì„¹ì…˜ ë¯¸ë¦¬ë³´ê¸°

ì„œë²„ì—ì„œ ìµœì í™”ëœ ëª¨ë¸ì„ ë§Œë“¤ì—ˆë‹¤ë©´, ì´ì œ **ì—£ì§€ ë””ë°”ì´ìŠ¤**ë¡œ ë°°í¬í•  ì°¨ë¡€ì…ë‹ˆë‹¤. ë‹¤ìŒ ì„¹ì…˜ [ì—£ì§€ ë°°í¬](./03-edge-deployment.md)ì—ì„œëŠ” NVIDIA Jetson, ë¼ì¦ˆë² ë¦¬íŒŒì´, ëª¨ë°”ì¼ ê¸°ê¸°ì—ì„œ ë¹„ì „ ëª¨ë¸ì„ ì‹¤í–‰í•˜ëŠ” ë°©ë²•ì„ ë°°ì›ë‹ˆë‹¤.

## ì°¸ê³  ìë£Œ

- [ONNX Runtime - TensorRT Execution Provider](https://onnxruntime.ai/docs/execution-providers/TensorRT-ExecutionProvider.html) - ê³µì‹ í†µí•© ë¬¸ì„œ
- [TensorRT Architecture Overview](https://docs.nvidia.com/deeplearning/tensorrt/latest/architecture/architecture-overview.html) - NVIDIA ê³µì‹ ë¬¸ì„œ
- [NVIDIA Model Optimizer](https://github.com/NVIDIA/Model-Optimizer) - í†µí•© ìµœì í™” ë„êµ¬
- [Accelerating AI Inference with ONNX and TensorRT](https://medium.com/@bskkim2022/accelerating-ai-inference-with-onnx-and-tensorrt-f9f43bd26854) - ì‹¤ì „ ê°€ì´ë“œ
- [ONNX Runtime + TensorRT Performance Guide](https://www.gurustartups.com/reports/onnx-runtime-tensorrt-execution-provider-performance-guide) - ì„±ëŠ¥ íŠœë‹ ê°€ì´ë“œ
