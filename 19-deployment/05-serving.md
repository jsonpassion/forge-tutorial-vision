# ëª¨ë¸ ì„œë¹™

> Triton, TorchServe, FastAPI

## ê°œìš”

MLOps ì¸í”„ë¼ë¥¼ ê°–ì¶”ì—ˆë‹¤ë©´, ì´ì œ **ì‚¬ìš©ìì—ê²Œ ëª¨ë¸ì„ ì œê³µ**í•  ì°¨ë¡€ì…ë‹ˆë‹¤. ì´ ì„¹ì…˜ì—ì„œëŠ” ë¹„ì „ ëª¨ë¸ì„ **REST APIë‚˜ gRPC ì„œë¹„ìŠ¤**ë¡œ ë°°í¬í•˜ëŠ” ë°©ë²•ì„ ë°°ì›ë‹ˆë‹¤. ê°„ë‹¨í•œ FastAPI ì„œë²„ë¶€í„° ëŒ€ê·œëª¨ íŠ¸ë˜í”½ì„ ì²˜ë¦¬í•˜ëŠ” Triton Inference Serverê¹Œì§€, í”„ë¡œë•ì…˜ ìˆ˜ì¤€ì˜ ëª¨ë¸ ì„œë¹™ì„ êµ¬í˜„í•©ë‹ˆë‹¤.

**ì„ ìˆ˜ ì§€ì‹**:
- [CV MLOps](./04-mlops.md)
- [ONNXì™€ TensorRT](./02-onnx-tensorrt.md)
- ê¸°ë³¸ì ì¸ ì›¹ API ê°œë…

**í•™ìŠµ ëª©í‘œ**:
- ëª¨ë¸ ì„œë¹™ ì•„í‚¤í…ì²˜ì˜ í•µì‹¬ ê°œë… ì´í•´í•˜ê¸°
- FastAPIë¡œ ê°„ë‹¨í•œ ì¶”ë¡  API êµ¬ì¶•í•˜ê¸°
- TorchServeì™€ Tritonìœ¼ë¡œ í”„ë¡œë•ì…˜ ì„œë¹™ êµ¬í˜„í•˜ê¸°

## ì™œ ì•Œì•„ì•¼ í• ê¹Œ?

> ğŸ’¡ **ë¹„ìœ **: ìµœê³ ì˜ ì…°í”„ê°€ ìˆì–´ë„ **ë ˆìŠ¤í† ë‘**ì´ ì—†ìœ¼ë©´ ì†ë‹˜ì—ê²Œ ìš”ë¦¬ë¥¼ ì œê³µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì£¼ë°©(ëª¨ë¸)ë§Œìœ¼ë¡œëŠ” ë¶€ì¡±í•˜ê³ , í™€ ì„œë¹„ìŠ¤(ì„œë¹™ ì‹œìŠ¤í…œ), ì£¼ë¬¸ ì‹œìŠ¤í…œ(API), ì›¨ì´í„°(ë¡œë“œ ë°¸ëŸ°ì„œ)ê°€ í•„ìš”í•©ë‹ˆë‹¤. ëª¨ë¸ ì„œë¹™ì€ **ML ë ˆìŠ¤í† ë‘**ì„ ìš´ì˜í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

**ì„œë¹™ ë°©ì‹ ë¹„êµ:**

| ë°©ì‹ | ì¥ì  | ë‹¨ì  | ì í•©í•œ ê²½ìš° |
|------|------|------|-------------|
| **FastAPI** | ê°„ë‹¨, ìœ ì—° | ìµœì í™” ìˆ˜ë™ | í”„ë¡œí† íƒ€ì…, ì†Œê·œëª¨ |
| **TorchServe** | PyTorch íŠ¹í™” | ëŸ¬ë‹ ì»¤ë¸Œ | PyTorch ëª¨ë¸ |
| **Triton** | ê³ ì„±ëŠ¥, ë‹¤ì¤‘ ëª¨ë¸ | ë³µì¡í•œ ì„¤ì • | ëŒ€ê·œëª¨ í”„ë¡œë•ì…˜ |
| **BentoML** | íŒ¨í‚¤ì§• í¸ë¦¬ | ì„±ëŠ¥ ì œí•œ | ë¹ ë¥¸ ë°°í¬ |

**ì„œë¹™ ì‹œìŠ¤í…œì˜ í•µì‹¬ ì§€í‘œ:**

| ì§€í‘œ | ì„¤ëª… | ëª©í‘œ |
|------|------|------|
| **Latency (P50/P99)** | ì‘ë‹µ ì‹œê°„ | < 100ms |
| **Throughput** | ì´ˆë‹¹ ìš”ì²­ ìˆ˜ | ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ |
| **Availability** | ê°€ìš©ì„± | 99.9%+ |
| **Scalability** | í™•ì¥ì„± | ìˆ˜í‰ í™•ì¥ ê°€ëŠ¥ |

## í•µì‹¬ ê°œë…

### ê°œë… 1: FastAPIë¡œ ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘

> ğŸ’¡ **ë¹„ìœ **: FastAPIëŠ” **í¬ì¥ë§ˆì°¨**ì™€ ê°™ìŠµë‹ˆë‹¤. ë¹ ë¥´ê²Œ ì—´ê³ , ê°„ë‹¨í•œ ë©”ë‰´ë¡œ ì†ë‹˜ì„ ë§ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ëŒ€ê·œëª¨ ë ˆìŠ¤í† ë‘ì€ ì•„ë‹ˆì§€ë§Œ, ì‹œì‘í•˜ê¸°ì—” ì™„ë²½í•©ë‹ˆë‹¤.

```python
# pip install fastapi uvicorn python-multipart pillow torch torchvision

from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.responses import JSONResponse
from PIL import Image
import torch
from torchvision import models, transforms
import io
import time

app = FastAPI(
    title="Image Classification API",
    description="ResNet-18 ê¸°ë°˜ ì´ë¯¸ì§€ ë¶„ë¥˜ ì„œë¹„ìŠ¤",
    version="1.0.0"
)

# ëª¨ë¸ ë¡œë“œ (ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ)
model = models.resnet18(pretrained=True)
model.eval()

# ImageNet í´ë˜ìŠ¤ (ì¼ë¶€)
IMAGENET_CLASSES = {0: 'tench', 1: 'goldfish', ...}  # ì‹¤ì œë¡œëŠ” 1000ê°œ

# ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]
    )
])

@app.post("/predict")
async def predict(file: UploadFile = File(...)):
    """
    ì´ë¯¸ì§€ ë¶„ë¥˜ ì˜ˆì¸¡

    - **file**: ì´ë¯¸ì§€ íŒŒì¼ (JPEG, PNG)
    - **returns**: ì˜ˆì¸¡ í´ë˜ìŠ¤ì™€ ì‹ ë¢°ë„
    """
    # íŒŒì¼ ê²€ì¦
    if not file.content_type.startswith('image/'):
        raise HTTPException(status_code=400, detail="ì´ë¯¸ì§€ íŒŒì¼ë§Œ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤")

    try:
        # ì´ë¯¸ì§€ ë¡œë“œ
        contents = await file.read()
        image = Image.open(io.BytesIO(contents)).convert('RGB')

        # ì „ì²˜ë¦¬
        input_tensor = preprocess(image).unsqueeze(0)

        # ì¶”ë¡ 
        start_time = time.time()
        with torch.no_grad():
            output = model(input_tensor)
        inference_time = (time.time() - start_time) * 1000  # ms

        # ê²°ê³¼ ì²˜ë¦¬
        probabilities = torch.nn.functional.softmax(output[0], dim=0)
        top5_prob, top5_idx = torch.topk(probabilities, 5)

        results = [
            {"class_id": idx.item(), "confidence": prob.item()}
            for prob, idx in zip(top5_prob, top5_idx)
        ]

        return JSONResponse({
            "success": True,
            "predictions": results,
            "inference_time_ms": round(inference_time, 2)
        })

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    return {"status": "healthy", "model": "resnet18"}

# ì‹¤í–‰: uvicorn main:app --host 0.0.0.0 --port 8000
```

```python
# ë°°ì¹˜ ì¶”ë¡  ì§€ì› (ì„±ëŠ¥ í–¥ìƒ)
from fastapi import BackgroundTasks
from asyncio import Queue, create_task
import asyncio

class BatchInferenceServer:
    """ë°°ì¹˜ ì¶”ë¡ ìœ¼ë¡œ ì²˜ë¦¬ëŸ‰ í–¥ìƒ"""

    def __init__(self, model, batch_size=8, timeout=0.1):
        self.model = model
        self.batch_size = batch_size
        self.timeout = timeout  # ì´ˆ
        self.queue = Queue()
        self.results = {}

    async def start_batch_processor(self):
        """ë°±ê·¸ë¼ìš´ë“œ ë°°ì¹˜ ì²˜ë¦¬"""
        while True:
            batch = []
            request_ids = []

            # ë°°ì¹˜ ìˆ˜ì§‘ (timeout ë˜ëŠ” batch_sizeê¹Œì§€)
            try:
                while len(batch) < self.batch_size:
                    item = await asyncio.wait_for(
                        self.queue.get(),
                        timeout=self.timeout
                    )
                    batch.append(item['tensor'])
                    request_ids.append(item['id'])
            except asyncio.TimeoutError:
                pass

            if batch:
                # ë°°ì¹˜ ì¶”ë¡ 
                batch_tensor = torch.stack(batch)
                with torch.no_grad():
                    outputs = self.model(batch_tensor)

                # ê²°ê³¼ ì €ì¥
                for req_id, output in zip(request_ids, outputs):
                    self.results[req_id] = output

    async def predict(self, tensor, request_id):
        """ê°œë³„ ì˜ˆì¸¡ ìš”ì²­"""
        await self.queue.put({'tensor': tensor, 'id': request_id})

        # ê²°ê³¼ ëŒ€ê¸°
        while request_id not in self.results:
            await asyncio.sleep(0.01)

        result = self.results.pop(request_id)
        return result
```

> ğŸ”¥ **ì‹¤ë¬´ íŒ**: FastAPIëŠ” ê°œë°œê³¼ í”„ë¡œí† íƒ€ì´í•‘ì— ì™„ë²½í•˜ì§€ë§Œ, ëŒ€ê·œëª¨ íŠ¸ë˜í”½(1000 RPS+)ì—ëŠ” TorchServeë‚˜ Tritonì„ ê¶Œì¥í•©ë‹ˆë‹¤. ë‹¨, GPU ë©”ëª¨ë¦¬ ê´€ë¦¬ì™€ ë°°ì¹˜ ì²˜ë¦¬ëŠ” ì§ì ‘ êµ¬í˜„í•´ì•¼ í•©ë‹ˆë‹¤.

### ê°œë… 2: TorchServeë¡œ PyTorch ëª¨ë¸ ì„œë¹™

TorchServeëŠ” **PyTorch ê³µì‹ ì„œë¹™ ì†”ë£¨ì…˜**ì…ë‹ˆë‹¤. ëª¨ë¸ íŒ¨í‚¤ì§•, ë²„ì „ ê´€ë¦¬, A/B í…ŒìŠ¤íŒ…ì„ ì§€ì›í•©ë‹ˆë‹¤.

**TorchServe ì•„í‚¤í…ì²˜:**

| êµ¬ì„±ìš”ì†Œ | ì—­í•  |
|----------|------|
| **Frontend** | HTTP/gRPC ìš”ì²­ ì²˜ë¦¬ |
| **Backend Worker** | ëª¨ë¸ ì¶”ë¡  ìˆ˜í–‰ |
| **Model Store** | ëª¨ë¸ ì•„ì¹´ì´ë¸Œ(.mar) ì €ì¥ |
| **Inference API** | ì˜ˆì¸¡ ìš”ì²­ ì²˜ë¦¬ |
| **Management API** | ëª¨ë¸ ë“±ë¡/ì‚­ì œ/ìŠ¤ì¼€ì¼ë§ |

```bash
# TorchServe ì„¤ì¹˜
pip install torchserve torch-model-archiver torch-workflow-archiver

# ëª¨ë¸ í•¸ë“¤ëŸ¬ ì‘ì„± (handler.py)
```

```python
# handler.py - ì»¤ìŠ¤í…€ í•¸ë“¤ëŸ¬
from ts.torch_handler.base_handler import BaseHandler
import torch
from torchvision import transforms
from PIL import Image
import io
import json

class ImageClassificationHandler(BaseHandler):
    """ì´ë¯¸ì§€ ë¶„ë¥˜ ì»¤ìŠ¤í…€ í•¸ë“¤ëŸ¬"""

    def initialize(self, context):
        """ëª¨ë¸ ì´ˆê¸°í™” (ì„œë²„ ì‹œì‘ ì‹œ í˜¸ì¶œ)"""
        super().initialize(context)

        # ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
        self.transform = transforms.Compose([
            transforms.Resize(256),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize(
                mean=[0.485, 0.456, 0.406],
                std=[0.229, 0.224, 0.225]
            )
        ])

        # í´ë˜ìŠ¤ ë¼ë²¨ ë¡œë“œ
        mapping_file = context.manifest.get('model', {}).get('mapping')
        if mapping_file:
            with open(mapping_file, 'r') as f:
                self.mapping = json.load(f)
        else:
            self.mapping = {str(i): f"class_{i}" for i in range(1000)}

    def preprocess(self, data):
        """ìš”ì²­ ë°ì´í„° ì „ì²˜ë¦¬"""
        images = []
        for row in data:
            # ë°”ì´ë„ˆë¦¬ â†’ ì´ë¯¸ì§€
            image_data = row.get('data') or row.get('body')
            image = Image.open(io.BytesIO(image_data)).convert('RGB')

            # ë³€í™˜
            tensor = self.transform(image)
            images.append(tensor)

        return torch.stack(images)

    def inference(self, data):
        """ëª¨ë¸ ì¶”ë¡ """
        with torch.no_grad():
            outputs = self.model(data)
        return outputs

    def postprocess(self, data):
        """ê²°ê³¼ í›„ì²˜ë¦¬"""
        results = []
        probs = torch.nn.functional.softmax(data, dim=1)

        for prob in probs:
            top5_prob, top5_idx = torch.topk(prob, 5)
            result = [
                {
                    "class": self.mapping.get(str(idx.item()), f"class_{idx.item()}"),
                    "confidence": round(p.item(), 4)
                }
                for p, idx in zip(top5_prob, top5_idx)
            ]
            results.append(result)

        return results
```

```bash
# ëª¨ë¸ ì•„ì¹´ì´ë¸Œ ìƒì„±
torch-model-archiver --model-name resnet18 \
                     --version 1.0 \
                     --serialized-file model.pt \
                     --handler handler.py \
                     --extra-files index_to_name.json \
                     --export-path model_store

# TorchServe ì‹œì‘
torchserve --start --model-store model_store \
           --models resnet18=resnet18.mar \
           --ts-config config.properties

# ì¶”ë¡  ìš”ì²­
curl -X POST http://localhost:8080/predictions/resnet18 \
     -T cat.jpg

# ê´€ë¦¬ API
curl http://localhost:8081/models  # ëª¨ë¸ ëª©ë¡
curl -X PUT "http://localhost:8081/models/resnet18?min_worker=2"  # ì›Œì»¤ ìŠ¤ì¼€ì¼ë§
```

```properties
# config.properties - TorchServe ì„¤ì •
inference_address=http://0.0.0.0:8080
management_address=http://0.0.0.0:8081
metrics_address=http://0.0.0.0:8082

# GPU ì„¤ì •
number_of_gpu=1

# ì›Œì»¤ ì„¤ì •
default_workers_per_model=4

# ë°°ì¹˜ ì„¤ì •
batch_size=8
max_batch_delay=100

# ë©”ëª¨ë¦¬ ì„¤ì •
max_request_size=10485760  # 10MB
```

### ê°œë… 3: Triton Inference Server

> ğŸ’¡ **ë¹„ìœ **: Tritonì€ **ëŒ€í˜• í˜¸í…” ë·”í˜**ì™€ ê°™ìŠµë‹ˆë‹¤. ë‹¤ì–‘í•œ ìš”ë¦¬(ë‹¤ì¤‘ ëª¨ë¸)ë¥¼ ë™ì‹œì— ì„œë¹™í•˜ê³ , ë§ì€ ì†ë‹˜(ê³  íŠ¸ë˜í”½)ì„ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤. ì£¼ë°©(GPU)ì„ ìµœëŒ€í•œ í™œìš©í•˜ëŠ” ì „ë¬¸ ìš´ì˜ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

**Tritonì˜ í•µì‹¬ ê¸°ëŠ¥:**

| ê¸°ëŠ¥ | ì„¤ëª… |
|------|------|
| **ë‹¤ì¤‘ í”„ë ˆì„ì›Œí¬** | PyTorch, TensorFlow, ONNX, TensorRT ì§€ì› |
| **ë™ì  ë°°ì¹­** | ìš”ì²­ì„ ìë™ìœ¼ë¡œ ë°°ì¹˜ ì²˜ë¦¬ |
| **ëª¨ë¸ ì•™ìƒë¸”** | ì—¬ëŸ¬ ëª¨ë¸ì„ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ì—°ê²° |
| **ë™ì‹œ ì‹¤í–‰** | GPUì—ì„œ ì—¬ëŸ¬ ëª¨ë¸ ë³‘ë ¬ ì‹¤í–‰ |
| **ëª¨ë¸ ë²„ì €ë‹** | ì—¬ëŸ¬ ë²„ì „ ë™ì‹œ ì„œë¹™ |

```bash
# Triton ëª¨ë¸ ì €ì¥ì†Œ êµ¬ì¡°
model_repository/
â”œâ”€â”€ resnet18/
â”‚   â”œâ”€â”€ config.pbtxt          # ëª¨ë¸ ì„¤ì •
â”‚   â””â”€â”€ 1/                    # ë²„ì „ 1
â”‚       â””â”€â”€ model.onnx        # ONNX ëª¨ë¸
â”œâ”€â”€ yolov8/
â”‚   â”œâ”€â”€ config.pbtxt
â”‚   â””â”€â”€ 1/
â”‚       â””â”€â”€ model.plan        # TensorRT ì—”ì§„
â””â”€â”€ ensemble/
    â”œâ”€â”€ config.pbtxt
    â””â”€â”€ 1/                    # ë¹ˆ ë””ë ‰í† ë¦¬ (ì•™ìƒë¸”)
```

```protobuf
# config.pbtxt - ResNet18 ì„¤ì •
name: "resnet18"
platform: "onnxruntime_onnx"
max_batch_size: 32

input [
  {
    name: "input"
    data_type: TYPE_FP32
    dims: [ 3, 224, 224 ]
  }
]

output [
  {
    name: "output"
    data_type: TYPE_FP32
    dims: [ 1000 ]
  }
]

# ë™ì  ë°°ì¹­ ì„¤ì •
dynamic_batching {
  preferred_batch_size: [ 8, 16, 32 ]
  max_queue_delay_microseconds: 100000  # 100ms
}

# ì¸ìŠ¤í„´ìŠ¤ ì„¤ì •
instance_group [
  {
    count: 2                    # GPUë‹¹ 2ê°œ ì¸ìŠ¤í„´ìŠ¤
    kind: KIND_GPU
    gpus: [ 0 ]
  }
]

# ìµœì í™” ì„¤ì •
optimization {
  input_pinned_memory { enable: true }
  output_pinned_memory { enable: true }
}
```

```python
# Triton Python í´ë¼ì´ì–¸íŠ¸
# pip install tritonclient[all]

import tritonclient.grpc as grpcclient
import tritonclient.http as httpclient
import numpy as np
from PIL import Image

class TritonClient:
    """Triton Inference Server í´ë¼ì´ì–¸íŠ¸"""

    def __init__(self, url="localhost:8001", protocol="grpc"):
        if protocol == "grpc":
            self.client = grpcclient.InferenceServerClient(url)
        else:
            self.client = httpclient.InferenceServerClient(url)
        self.protocol = protocol

    def preprocess(self, image_path):
        """ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
        image = Image.open(image_path).convert('RGB')
        image = image.resize((224, 224))
        img_array = np.array(image).astype(np.float32)

        # ì •ê·œí™”
        mean = np.array([0.485, 0.456, 0.406])
        std = np.array([0.229, 0.224, 0.225])
        img_array = (img_array / 255.0 - mean) / std

        # CHW í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        img_array = img_array.transpose(2, 0, 1)
        return img_array

    def predict(self, image_path, model_name="resnet18"):
        """ì¶”ë¡  ìš”ì²­"""
        # ì „ì²˜ë¦¬
        input_data = self.preprocess(image_path)
        input_data = np.expand_dims(input_data, axis=0)  # ë°°ì¹˜ ì°¨ì› ì¶”ê°€

        if self.protocol == "grpc":
            # gRPC ìš”ì²­
            inputs = [
                grpcclient.InferInput("input", input_data.shape, "FP32")
            ]
            inputs[0].set_data_from_numpy(input_data)

            outputs = [grpcclient.InferRequestedOutput("output")]

            result = self.client.infer(
                model_name=model_name,
                inputs=inputs,
                outputs=outputs
            )

            output = result.as_numpy("output")
        else:
            # HTTP ìš”ì²­
            inputs = [
                httpclient.InferInput("input", input_data.shape, "FP32")
            ]
            inputs[0].set_data_from_numpy(input_data)

            outputs = [httpclient.InferRequestedOutput("output")]

            result = self.client.infer(
                model_name=model_name,
                inputs=inputs,
                outputs=outputs
            )

            output = result.as_numpy("output")

        # ê²°ê³¼ ì²˜ë¦¬
        probs = self.softmax(output[0])
        top5_idx = np.argsort(probs)[-5:][::-1]
        top5_probs = probs[top5_idx]

        return list(zip(top5_idx.tolist(), top5_probs.tolist()))

    @staticmethod
    def softmax(x):
        exp_x = np.exp(x - np.max(x))
        return exp_x / exp_x.sum()

# ì‚¬ìš© ì˜ˆì‹œ
client = TritonClient("localhost:8001", protocol="grpc")
results = client.predict("cat.jpg", "resnet18")
for class_id, confidence in results:
    print(f"Class {class_id}: {confidence:.4f}")
```

```bash
# Dockerë¡œ Triton ì‹¤í–‰
docker run --gpus=all -it --rm \
  -p 8000:8000 -p 8001:8001 -p 8002:8002 \
  -v $(pwd)/model_repository:/models \
  nvcr.io/nvidia/tritonserver:24.01-py3 \
  tritonserver --model-repository=/models

# ìƒíƒœ í™•ì¸
curl localhost:8000/v2/health/ready
curl localhost:8000/v2/models/resnet18
```

> ğŸ’¡ **ì•Œê³  ê³„ì…¨ë‚˜ìš”?**: Tritonì˜ **ë™ì  ë°°ì¹­**ì€ ë§ˆë²•ì²˜ëŸ¼ ì‘ë™í•©ë‹ˆë‹¤. ê°œë³„ ìš”ì²­ì´ ë“¤ì–´ì™€ë„ ì ê¹(ìµœëŒ€ 100ms) ê¸°ë‹¤ë ¸ë‹¤ê°€ ì—¬ëŸ¬ ìš”ì²­ì„ í•œ ë²ˆì— ì²˜ë¦¬í•©ë‹ˆë‹¤. ì´ ë•ë¶„ì— GPU í™œìš©ë¥ ì´ í¬ê²Œ ì˜¬ë¼ê°€ê³ , ì²˜ë¦¬ëŸ‰ì´ 2-5ë°° í–¥ìƒë©ë‹ˆë‹¤.

### ê°œë… 4: ì„œë¹™ íŒ¨í„´ê³¼ ìµœì í™”

```python
# ëª¨ë¸ ì•™ìƒë¸” íŒ¨í„´ (ì „ì²˜ë¦¬ â†’ ì¶”ë¡  â†’ í›„ì²˜ë¦¬)
"""
Triton Model Ensemble ì„¤ì • (config.pbtxt)

name: "image_pipeline"
platform: "ensemble"
max_batch_size: 32

input [
  { name: "raw_image" data_type: TYPE_UINT8 dims: [ -1, -1, 3 ] }
]
output [
  { name: "classification" data_type: TYPE_FP32 dims: [ 1000 ] }
]

ensemble_scheduling {
  step [
    {
      model_name: "preprocess"
      model_version: -1
      input_map { key: "raw_image" value: "raw_image" }
      output_map { key: "processed_image" value: "preprocessed" }
    },
    {
      model_name: "resnet18"
      model_version: -1
      input_map { key: "input" value: "preprocessed" }
      output_map { key: "output" value: "classification" }
    }
  ]
}
"""
```

```python
# A/B í…ŒìŠ¤íŒ… íŒ¨í„´
from fastapi import FastAPI, Header
import random

app = FastAPI()

# ë‘ ë²„ì „ì˜ ëª¨ë¸
model_v1 = load_model("v1")
model_v2 = load_model("v2")

@app.post("/predict")
async def predict(file: UploadFile, x_experiment: str = Header(None)):
    """A/B í…ŒìŠ¤íŒ… ì§€ì› ì¶”ë¡ """

    # ì‹¤í—˜ ê·¸ë£¹ ê²°ì •
    if x_experiment:
        variant = x_experiment
    else:
        variant = "v1" if random.random() < 0.9 else "v2"  # 90% v1, 10% v2

    # í•´ë‹¹ ëª¨ë¸ë¡œ ì¶”ë¡ 
    if variant == "v1":
        result = model_v1.predict(file)
    else:
        result = model_v2.predict(file)

    # ë©”íŠ¸ë¦­ ë¡œê¹…
    log_experiment_result(variant, result)

    return {"variant": variant, "prediction": result}
```

```python
# ìºì‹± íŒ¨í„´ (ë™ì¼ ì…ë ¥ ë¹ ë¥¸ ì‘ë‹µ)
from functools import lru_cache
import hashlib
from redis import Redis

redis = Redis(host='localhost', port=6379)

def get_image_hash(image_bytes):
    """ì´ë¯¸ì§€ í•´ì‹œ ìƒì„±"""
    return hashlib.sha256(image_bytes).hexdigest()

@app.post("/predict_cached")
async def predict_cached(file: UploadFile):
    """ìºì‹œ ì§€ì› ì¶”ë¡ """
    contents = await file.read()
    image_hash = get_image_hash(contents)

    # ìºì‹œ í™•ì¸
    cached = redis.get(f"prediction:{image_hash}")
    if cached:
        return {"cached": True, "prediction": json.loads(cached)}

    # ì¶”ë¡ 
    result = model.predict(contents)

    # ìºì‹œ ì €ì¥ (1ì‹œê°„ TTL)
    redis.setex(
        f"prediction:{image_hash}",
        3600,
        json.dumps(result)
    )

    return {"cached": False, "prediction": result}
```

> âš ï¸ **í”í•œ ì˜¤í•´**: "GPUê°€ í•­ìƒ 100% í™œìš©ëœë‹¤" â€” ì‹¤ì œë¡œ ë°°ì¹˜ í¬ê¸°ê°€ ì‘ê±°ë‚˜, ì „ì²˜ë¦¬/í›„ì²˜ë¦¬ê°€ CPU ë°”ìš´ë“œë©´ GPUëŠ” ë†€ê³  ìˆìŠµë‹ˆë‹¤. ë™ì  ë°°ì¹­ê³¼ ë¹„ë™ê¸° ì²˜ë¦¬ë¡œ ìµœì í™”í•´ì•¼ í•©ë‹ˆë‹¤.

### ê°œë… 5: í”„ë¡œë•ì…˜ ì²´í¬ë¦¬ìŠ¤íŠ¸

```python
# í”„ë¡œë•ì…˜ ë°°í¬ ì²´í¬ë¦¬ìŠ¤íŠ¸
PRODUCTION_CHECKLIST = {
    "ì„±ëŠ¥": [
        "ì§€ì—° ì‹œê°„ P99 < 100ms",
        "ì²˜ë¦¬ëŸ‰ ëª©í‘œ ë‹¬ì„±",
        "GPU í™œìš©ë¥  > 70%",
        "ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ì—†ìŒ",
    ],
    "ì•ˆì •ì„±": [
        "í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸",
        "ê·¸ë ˆì´ìŠ¤í’€ ì…§ë‹¤ìš´",
        "ì¬ì‹œì‘ ì •ì±… ì„¤ì •",
        "ìë™ ë³µêµ¬ (Kubernetes)",
    ],
    "ë³´ì•ˆ": [
        "ì…ë ¥ ê²€ì¦ (í¬ê¸°, íƒ€ì…)",
        "Rate limiting",
        "ì¸ì¦/ì¸ê°€ (API í‚¤, OAuth)",
        "HTTPS/TLS",
    ],
    "ëª¨ë‹ˆí„°ë§": [
        "Prometheus ë©”íŠ¸ë¦­",
        "ë¡œê·¸ ìˆ˜ì§‘ (ELK, Loki)",
        "ì•Œë¦¼ ì„¤ì • (PagerDuty, Slack)",
        "íŠ¸ë ˆì´ì‹± (Jaeger, Zipkin)",
    ],
    "ìš´ì˜": [
        "ìë™ ìŠ¤ì¼€ì¼ë§ (HPA)",
        "ë¡¤ë§ ì—…ë°ì´íŠ¸",
        "ë¡¤ë°± ì ˆì°¨",
        "ë°±ì—…/ë³µêµ¬ ê³„íš",
    ],
}
```

```yaml
# Kubernetes ë°°í¬ ì˜ˆì‹œ (deployment.yaml)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cv-inference-server
spec:
  replicas: 3
  selector:
    matchLabels:
      app: cv-inference
  template:
    metadata:
      labels:
        app: cv-inference
    spec:
      containers:
      - name: triton
        image: nvcr.io/nvidia/tritonserver:24.01-py3
        args: ["tritonserver", "--model-repository=/models"]
        ports:
        - containerPort: 8000
        - containerPort: 8001
        - containerPort: 8002
        resources:
          limits:
            nvidia.com/gpu: 1
            memory: "16Gi"
          requests:
            memory: "8Gi"
        livenessProbe:
          httpGet:
            path: /v2/health/live
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /v2/health/ready
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        volumeMounts:
        - name: model-volume
          mountPath: /models
      volumes:
      - name: model-volume
        persistentVolumeClaim:
          claimName: model-pvc
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: cv-inference-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: cv-inference-server
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
```

## í•µì‹¬ ì •ë¦¬

| ê°œë… | ì„¤ëª… |
|------|------|
| **FastAPI** | Python ì›¹ í”„ë ˆì„ì›Œí¬, ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘ì— ì í•© |
| **TorchServe** | PyTorch ê³µì‹ ì„œë¹™ ë„êµ¬, ëª¨ë¸ ë²„ì €ë‹ ì§€ì› |
| **Triton** | NVIDIA ê³ ì„±ëŠ¥ ì„œë²„, ë‹¤ì¤‘ í”„ë ˆì„ì›Œí¬/ëª¨ë¸ ì§€ì› |
| **ë™ì  ë°°ì¹­** | ìš”ì²­ì„ ëª¨ì•„ í•œ ë²ˆì— ì²˜ë¦¬, GPU íš¨ìœ¨ â†‘ |
| **ëª¨ë¸ ì•™ìƒë¸”** | ì „ì²˜ë¦¬â†’ì¶”ë¡ â†’í›„ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ |
| **gRPC** | HTTP/2 ê¸°ë°˜ ê³ ì„±ëŠ¥ RPC, ë°”ì´ë„ˆë¦¬ í”„ë¡œí† ì½œ |

## íŠœí† ë¦¬ì–¼ì„ ë§ˆì¹˜ë©°

**ì¶•í•˜í•©ë‹ˆë‹¤!** ğŸ‰

19ê°œ ì±•í„°, 93ê°œ ì„¹ì…˜ì— ê±¸ì³ **"í”½ì…€ì˜ ì´í•´ë¶€í„° ë©€í‹°ëª¨ë‹¬ AIê¹Œì§€"** ì™„ì „ ì •ë³µí•˜ì…¨ìŠµë‹ˆë‹¤.

**ì´ íŠœí† ë¦¬ì–¼ì—ì„œ ë°°ìš´ ê²ƒë“¤:**
- **ê¸°ì´ˆ**: ì´ë¯¸ì§€, ìƒ‰ìƒ, í•„í„°, ì—ì§€ ê²€ì¶œ
- **ë”¥ëŸ¬ë‹**: CNN, ë¶„ë¥˜, ê°ì²´ íƒì§€, ì„¸ê·¸ë©˜í…Œì´ì…˜
- **íŠ¸ëœìŠ¤í¬ë¨¸**: ViT, Swin, ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜
- **ë©€í‹°ëª¨ë‹¬**: CLIP, VLM, í†µí•© ëª¨ë¸
- **ìƒì„± AI**: VAE, GAN, Diffusion, Stable Diffusion
- **ë¹„ë””ì˜¤/3D**: ë¹„ë””ì˜¤ ìƒì„±, NeRF, 3D Gaussian Splatting
- **ë°°í¬**: ìµœì í™”, TensorRT, ì—£ì§€, MLOps, ì„œë¹™

**ë‹¤ìŒ ë‹¨ê³„:**
1. **í”„ë¡œì íŠ¸ ì‹œì‘**: ë°°ìš´ ê²ƒì„ ì‹¤ì œ í”„ë¡œì íŠ¸ì— ì ìš©
2. **ë…¼ë¬¸ ì½ê¸°**: arXiv, CVPR, NeurIPS ìµœì‹  ì—°êµ¬ íŒ”ë¡œìš°
3. **ì˜¤í”ˆì†ŒìŠ¤ ê¸°ì—¬**: Hugging Face, PyTorch ìƒíƒœê³„ ì°¸ì—¬
4. **ì»¤ë®¤ë‹ˆí‹° ì°¸ì—¬**: ë°‹ì—…, ì»¨í¼ëŸ°ìŠ¤, ì˜¨ë¼ì¸ í¬ëŸ¼

> ğŸ’¡ "ë°°ì›€ì˜ ëì€ ì—†ê³ , ì‹¤ì²œì˜ ì‹œì‘ë§Œ ìˆì„ ë¿ì…ë‹ˆë‹¤."

**ê³„ì† ë°°ìš°ê³ , ë§Œë“¤ê³ , ê³µìœ í•˜ì„¸ìš”!**

## ì°¸ê³  ìë£Œ

- [NVIDIA Triton Documentation](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/) - ê³µì‹ ë¬¸ì„œ
- [TorchServe Guide](https://pytorch.org/serve/) - PyTorch ê³µì‹ ì„œë¹™ ê°€ì´ë“œ
- [Model Serving Frameworks Comparison](https://apxml.com/courses/how-to-build-a-large-language-model/chapter-29-serving-llms-at-scale/model-serving-frameworks) - í”„ë ˆì„ì›Œí¬ ë¹„êµ
- [PyTriton](https://triton-inference-server.github.io/pytriton/latest/) - Python-friendly Triton ì¸í„°í˜ì´ìŠ¤
- [Best Tools for ML Model Serving](https://neptune.ai/blog/ml-model-serving-best-tools) - ì„œë¹™ ë„êµ¬ ê°€ì´ë“œ
