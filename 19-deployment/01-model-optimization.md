# ëª¨ë¸ ìµœì í™”

> ì–‘ìí™”, í”„ë£¨ë‹, ì§€ì‹ ì¦ë¥˜

## ê°œìš”

ì§€ê¸ˆê¹Œì§€ ìš°ë¦¬ëŠ” ê°•ë ¥í•œ ë¹„ì „ ëª¨ë¸ë“¤ì„ í•™ìŠµí•˜ê³  í‰ê°€í•˜ëŠ” ë°©ë²•ì„ ë°°ì› ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì‹¤ì œ ì„œë¹„ìŠ¤ì—ì„œëŠ” **ì •í™•ë„ë§Œí¼ ì†ë„ì™€ ë¹„ìš©ë„ ì¤‘ìš”**í•©ë‹ˆë‹¤. ì´ ì„¹ì…˜ì—ì„œëŠ” í•™ìŠµëœ ëª¨ë¸ì„ ë” ì‘ê³ , ë” ë¹ ë¥´ê²Œ ë§Œë“œëŠ” ì„¸ ê°€ì§€ í•µì‹¬ ê¸°ìˆ â€”**ì–‘ìí™”(Quantization)**, **í”„ë£¨ë‹(Pruning)**, **ì§€ì‹ ì¦ë¥˜(Knowledge Distillation)**â€”ì„ ë°°ì›ë‹ˆë‹¤.

**ì„ ìˆ˜ ì§€ì‹**:
- [CNN ê¸°ì´ˆ](../04-cnn-fundamentals/01-convolution.md)
- [ëª¨ë¸ í•™ìŠµê³¼ í‰ê°€](../06-image-classification/03-training.md)

**í•™ìŠµ ëª©í‘œ**:
- ëª¨ë¸ ì••ì¶•ì˜ í•„ìš”ì„±ê³¼ ì„¸ ê°€ì§€ ì£¼ìš” ê¸°ë²• ì´í•´í•˜ê¸°
- PyTorchì—ì„œ ì–‘ìí™”, í”„ë£¨ë‹ ì§ì ‘ êµ¬í˜„í•˜ê¸°
- ì§€ì‹ ì¦ë¥˜ë¡œ ì‘ì€ ëª¨ë¸ í•™ìŠµì‹œí‚¤ê¸°

## ì™œ ì•Œì•„ì•¼ í• ê¹Œ?

> ğŸ’¡ **ë¹„ìœ **: ì—¬í–‰ ê°€ë°©ì„ ì‹¸ëŠ” ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤. í° ìºë¦¬ì–´ì— ëª¨ë“  ì§ì„ ë„£ìœ¼ë©´ í¸í•˜ì§€ë§Œ, ê¸°ë‚´ ìˆ˜í•˜ë¬¼ë§Œ ë“¤ê³  ê°€ì•¼ í•œë‹¤ë©´? ê¼­ í•„ìš”í•œ ê²ƒë§Œ **ì••ì¶•í•´ì„œ í¬ì¥**í•´ì•¼ í•©ë‹ˆë‹¤. ëª¨ë¸ë„ ë§ˆì°¬ê°€ì§€ë¡œ, ì„œë²„ì—ì„œëŠ” í° ëª¨ë¸ì„ ëŒë¦´ ìˆ˜ ìˆì§€ë§Œ ìŠ¤ë§ˆíŠ¸í°ì´ë‚˜ IoT ê¸°ê¸°ì—ì„œëŠ” **ì‘ê³  ê°€ë²¼ìš´ ëª¨ë¸**ì´ í•„ìˆ˜ì…ë‹ˆë‹¤.

ì‹¤ì œ í˜„ì—…ì—ì„œ ëª¨ë¸ ìµœì í™”ê°€ í•„ìˆ˜ì¸ ì´ìœ :

| ë°°í¬ í™˜ê²½ | ì œì•½ ì¡°ê±´ | ìµœì í™” í•„ìš”ì„± |
|-----------|-----------|---------------|
| ëª¨ë°”ì¼ ì•± | ì•± í¬ê¸° 100MB ì œí•œ, ë°°í„°ë¦¬ | ëª¨ë¸ í¬ê¸° â†“, ì—°ì‚°ëŸ‰ â†“ |
| ì—£ì§€ ë””ë°”ì´ìŠ¤ | RAM 2-8GB, GPU ì—†ìŒ | INT8 ì–‘ìí™” í•„ìˆ˜ |
| í´ë¼ìš°ë“œ ì„œë²„ | GPU ë¹„ìš© $2-4/ì‹œê°„ | ì²˜ë¦¬ëŸ‰ â†‘, ë¹„ìš© â†“ |
| ì‹¤ì‹œê°„ ì„œë¹„ìŠ¤ | ì§€ì—° ì‹œê°„ < 50ms | ì¶”ë¡  ì†ë„ ìµœì í™” |

ResNet-50 í•˜ë‚˜ë¥¼ ì˜ˆë¡œ ë“¤ë©´:
- **ì›ë³¸**: 98MB, FP32, 4.1G FLOPs
- **ìµœì í™” í›„**: 24MB, INT8, 1.0G FLOPs (4ë°° ì‘ê³ , 2-4ë°° ë¹ ë¦„)

## í•µì‹¬ ê°œë…

### ê°œë… 1: ì–‘ìí™”(Quantization)

> ğŸ’¡ **ë¹„ìœ **: ì‚¬ì§„ì˜ ìƒ‰ìƒ ìˆ˜ë¥¼ ì¤„ì´ëŠ” ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤. 24ë¹„íŠ¸ ì»¬ëŸ¬(1600ë§Œ ìƒ‰)ë¥¼ 8ë¹„íŠ¸(256ìƒ‰)ë¡œ ì¤„ì´ë©´ íŒŒì¼ í¬ê¸°ê°€ í™• ì¤„ì§€ë§Œ, ëˆˆì—ëŠ” ë¹„ìŠ·í•´ ë³´ì´ì£ . ë§ˆì°¬ê°€ì§€ë¡œ 32ë¹„íŠ¸ ë¶€ë™ì†Œìˆ˜ì  ê°€ì¤‘ì¹˜ë¥¼ 8ë¹„íŠ¸ ì •ìˆ˜ë¡œ ë°”ê¾¸ë©´ ëª¨ë¸ì´ 4ë°° ì‘ì•„ì§‘ë‹ˆë‹¤.

**ì–‘ìí™”ì˜ í•µì‹¬ ì•„ì´ë””ì–´:**

ì¼ë°˜ì ì¸ ë”¥ëŸ¬ë‹ ëª¨ë¸ì€ ê°€ì¤‘ì¹˜ë¥¼ **FP32(32ë¹„íŠ¸ ë¶€ë™ì†Œìˆ˜ì )**ë¡œ ì €ì¥í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ëŒ€ë¶€ë¶„ì˜ ê°€ì¤‘ì¹˜ëŠ” -1~1 ì‚¬ì´ì˜ ì‘ì€ ê°’ì´ë¯€ë¡œ, ì´ë¥¼ **INT8(8ë¹„íŠ¸ ì •ìˆ˜)**ë¡œ í‘œí˜„í•´ë„ ì •í™•ë„ ì†ì‹¤ì´ ë¯¸ë¯¸í•©ë‹ˆë‹¤.

**ìˆ˜ì‹**:
$$Q(x) = \text{round}\left(\frac{x}{s}\right) + z$$

- $x$: ì›ë³¸ FP32 ê°’
- $s$: ìŠ¤ì¼€ì¼ íŒ©í„° (ê°’ì˜ ë²”ìœ„ë¥¼ ì¡°ì ˆ)
- $z$: ì œë¡œ í¬ì¸íŠ¸ (0ì˜ ìœ„ì¹˜ ì¡°ì •)
- $Q(x)$: ì–‘ìí™”ëœ ì •ìˆ˜ ê°’

**ì–‘ìí™”ì˜ ì¢…ë¥˜:**

| ë°©ì‹ | ì„¤ëª… | ì •í™•ë„ ì†ì‹¤ | êµ¬í˜„ ë‚œì´ë„ |
|------|------|------------|------------|
| **Post-Training Quantization (PTQ)** | í•™ìŠµ í›„ ë°”ë¡œ ì–‘ìí™” | ì¤‘ê°„ | ì‰¬ì›€ |
| **Quantization-Aware Training (QAT)** | í•™ìŠµ ì¤‘ ì–‘ìí™” ì‹œë®¬ë ˆì´ì…˜ | ë‚®ìŒ | ì¤‘ê°„ |
| **Dynamic Quantization** | ì¶”ë¡  ì‹œ ë™ì ìœ¼ë¡œ ì–‘ìí™” | ì¤‘ê°„ | ë§¤ìš° ì‰¬ì›€ |

```python
import torch
import torch.nn as nn
from torchvision import models

# 1. ë™ì  ì–‘ìí™” (ê°€ì¥ ê°„ë‹¨)
# ì¶”ë¡  ì‹œ ê°€ì¤‘ì¹˜ë§Œ ì–‘ìí™”, ì…ë ¥ì€ FP32 ìœ ì§€
model = models.resnet18(pretrained=True)
model.eval()

# Linear ë ˆì´ì–´ì— ë™ì  ì–‘ìí™” ì ìš©
quantized_model = torch.quantization.quantize_dynamic(
    model,
    {nn.Linear},  # ì–‘ìí™”í•  ë ˆì´ì–´ íƒ€ì…
    dtype=torch.qint8  # INT8ë¡œ ë³€í™˜
)

# ëª¨ë¸ í¬ê¸° ë¹„êµ
import os
torch.save(model.state_dict(), 'original.pth')
torch.save(quantized_model.state_dict(), 'quantized.pth')
print(f"ì›ë³¸ í¬ê¸°: {os.path.getsize('original.pth') / 1e6:.1f} MB")
print(f"ì–‘ìí™” í›„: {os.path.getsize('quantized.pth') / 1e6:.1f} MB")
# ì›ë³¸ í¬ê¸°: 44.7 MB
# ì–‘ìí™” í›„: 11.3 MB (ì•½ 4ë°° ê°ì†Œ!)
```

```python
# 2. ì •ì  ì–‘ìí™” (PTQ) - ë” ì •êµí•œ ë°©ë²•
import torch.quantization as quant

# ì–‘ìí™” ì„¤ì •
model = models.resnet18(pretrained=True)
model.eval()

# í“¨ì¦ˆ: Conv-BN-ReLUë¥¼ í•˜ë‚˜ë¡œ í•©ì¹¨ (ì†ë„ í–¥ìƒ)
model_fused = torch.quantization.fuse_modules(
    model,
    [['conv1', 'bn1', 'relu']],
    inplace=False
)

# ì–‘ìí™” ì¤€ë¹„
model_fused.qconfig = quant.get_default_qconfig('x86')  # CPUìš©
model_prepared = quant.prepare(model_fused)

# ìº˜ë¦¬ë¸Œë ˆì´ì…˜: ëŒ€í‘œ ë°ì´í„°ë¡œ ìŠ¤ì¼€ì¼ ê²°ì •
calibration_data = torch.randn(100, 3, 224, 224)  # ì‹¤ì œë¡œëŠ” validation ë°ì´í„° ì‚¬ìš©
with torch.no_grad():
    for i in range(10):
        model_prepared(calibration_data[i*10:(i+1)*10])

# ì–‘ìí™” ìˆ˜í–‰
model_quantized = quant.convert(model_prepared)
print("ì •ì  ì–‘ìí™” ì™„ë£Œ!")
```

> âš ï¸ **í”í•œ ì˜¤í•´**: "ì–‘ìí™”í•˜ë©´ ì •í™•ë„ê°€ í¬ê²Œ ë–¨ì–´ì§„ë‹¤" â€” ì‹¤ì œë¡œ ì˜ ëœ ì–‘ìí™”ëŠ” **1% ë¯¸ë§Œ**ì˜ ì •í™•ë„ ì†ì‹¤ë§Œ ë°œìƒí•©ë‹ˆë‹¤. íŠ¹íˆ QATë¥¼ ì‚¬ìš©í•˜ë©´ ê±°ì˜ ì†ì‹¤ ì—†ì´ 4ë°° ì••ì¶•ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.

### ê°œë… 2: í”„ë£¨ë‹(Pruning)

> ğŸ’¡ **ë¹„ìœ **: ë‚˜ë¬´ ê°€ì§€ì¹˜ê¸°ì™€ ê°™ìŠµë‹ˆë‹¤. ê±´ê°•í•œ ë‚˜ë¬´ëŠ” ëª¨ë“  ê°€ì§€ê°€ í•„ìš”í•˜ì§€ ì•Šì•„ìš”. ì£½ì€ ê°€ì§€ë‚˜ ì•½í•œ ê°€ì§€ë¥¼ ì˜ë¼ë‚´ë©´ ë‚˜ë¬´ê°€ ë” ê±´ê°•í•´ì§€ê³ , ë‚¨ì€ ê°€ì§€ë¡œ ë” ë§ì€ ì—´ë§¤ë¥¼ ë§ºìŠµë‹ˆë‹¤. ì‹ ê²½ë§ë„ **ì“¸ëª¨ì—†ëŠ” ì—°ê²°(ì‘ì€ ê°€ì¤‘ì¹˜)**ì„ ì˜ë¼ë‚´ë©´ ë” íš¨ìœ¨ì ìœ¼ë¡œ ë™ì‘í•©ë‹ˆë‹¤.

**í”„ë£¨ë‹ì˜ í•µì‹¬ ì•„ì´ë””ì–´:**

ì‹ ê²½ë§ì˜ ë§ì€ ê°€ì¤‘ì¹˜ëŠ” 0ì— ê°€ê¹Œìš´ ê°’ìœ¼ë¡œ, ì¶œë ¥ì— ê±°ì˜ ê¸°ì—¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì´ëŸ° **"ê²Œìœ¼ë¥¸" ê°€ì¤‘ì¹˜**ë¥¼ 0ìœ¼ë¡œ ë§Œë“¤ê±°ë‚˜ ì•„ì˜ˆ ì œê±°í•˜ë©´ ëª¨ë¸ì´ ê°€ë²¼ì›Œì§‘ë‹ˆë‹¤.

**í”„ë£¨ë‹ì˜ ì¢…ë¥˜:**

| ë°©ì‹ | ì„¤ëª… | ì••ì¶•ë¥  | í•˜ë“œì›¨ì–´ íš¨ìœ¨ |
|------|------|--------|-------------|
| **ë¹„êµ¬ì¡°ì (Unstructured)** | ê°œë³„ ê°€ì¤‘ì¹˜ ì œê±° | ë†’ìŒ (90%+) | ë‚®ìŒ |
| **êµ¬ì¡°ì (Structured)** | í•„í„°/ì±„ë„ ë‹¨ìœ„ ì œê±° | ì¤‘ê°„ (50-70%) | ë†’ìŒ |

```python
import torch
import torch.nn.utils.prune as prune
from torchvision import models

# ëª¨ë¸ ì¤€ë¹„
model = models.resnet18(pretrained=True)

# 1. ë¹„êµ¬ì¡°ì  í”„ë£¨ë‹: ê°€ì¥ ì‘ì€ ê°€ì¤‘ì¹˜ 30% ì œê±°
conv1 = model.conv1
prune.l1_unstructured(conv1, name='weight', amount=0.3)

# í”„ë£¨ë‹ ê²°ê³¼ í™•ì¸
print(f"í”„ë£¨ë‹ ë§ˆìŠ¤í¬ ëª¨ì–‘: {conv1.weight_mask.shape}")
print(f"0ì¸ ê°€ì¤‘ì¹˜ ë¹„ìœ¨: {(conv1.weight_mask == 0).float().mean():.1%}")
# 0ì¸ ê°€ì¤‘ì¹˜ ë¹„ìœ¨: 30.0%

# í”„ë£¨ë‹ì„ ì˜êµ¬ì ìœ¼ë¡œ ì ìš©
prune.remove(conv1, 'weight')
```

```python
# 2. ì „ì²´ ëª¨ë¸ì— ê¸€ë¡œë²Œ í”„ë£¨ë‹ ì ìš©
model = models.resnet18(pretrained=True)

# í”„ë£¨ë‹í•  ë ˆì´ì–´ì™€ íŒŒë¼ë¯¸í„° ì§€ì •
parameters_to_prune = [
    (model.layer1[0].conv1, 'weight'),
    (model.layer1[0].conv2, 'weight'),
    (model.layer2[0].conv1, 'weight'),
    (model.layer2[0].conv2, 'weight'),
]

# ê¸€ë¡œë²Œ L1 í”„ë£¨ë‹: ì „ì²´ì—ì„œ ê°€ì¥ ì‘ì€ 50% ì œê±°
prune.global_unstructured(
    parameters_to_prune,
    pruning_method=prune.L1Unstructured,
    amount=0.5,  # 50% í”„ë£¨ë‹
)

# í¬ì†Œì„±(Sparsity) í™•ì¸
def compute_sparsity(model):
    total_zeros = 0
    total_params = 0
    for name, param in model.named_parameters():
        if 'weight' in name:
            total_zeros += (param == 0).sum().item()
            total_params += param.numel()
    return total_zeros / total_params

print(f"ëª¨ë¸ í¬ì†Œì„±: {compute_sparsity(model):.1%}")
# ëª¨ë¸ í¬ì†Œì„±: ì•½ 50%
```

```python
# 3. êµ¬ì¡°ì  í”„ë£¨ë‹: ì±„ë„ ë‹¨ìœ„ ì œê±° (ì‹¤ì œ ì†ë„ í–¥ìƒ)
def structured_pruning(model, amount=0.3):
    """Ln ë…¸ë¦„ ê¸°ë°˜ ì±„ë„ í”„ë£¨ë‹"""
    for name, module in model.named_modules():
        if isinstance(module, torch.nn.Conv2d):
            prune.ln_structured(
                module,
                name='weight',
                amount=amount,
                n=2,  # L2 ë…¸ë¦„ ì‚¬ìš©
                dim=0  # ì¶œë ¥ ì±„ë„ ë°©í–¥ìœ¼ë¡œ í”„ë£¨ë‹
            )
    return model

model_pruned = structured_pruning(model, amount=0.3)
print("êµ¬ì¡°ì  í”„ë£¨ë‹ ì™„ë£Œ!")
```

> ğŸ”¥ **ì‹¤ë¬´ íŒ**: ë¹„êµ¬ì¡°ì  í”„ë£¨ë‹ì€ 90%ê¹Œì§€ ì••ì¶•í•´ë„ ì •í™•ë„ê°€ ìœ ì§€ë˜ì§€ë§Œ, **ì‹¤ì œ ì†ë„ í–¥ìƒ**ì„ ìœ„í•´ì„œëŠ” êµ¬ì¡°ì  í”„ë£¨ë‹ì´ í•„ìš”í•©ë‹ˆë‹¤. ëŒ€ë¶€ë¶„ì˜ í•˜ë“œì›¨ì–´ëŠ” í¬ì†Œ ì—°ì‚°ì„ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ì§€ ëª»í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.

### ê°œë… 3: ì§€ì‹ ì¦ë¥˜(Knowledge Distillation)

> ğŸ’¡ **ë¹„ìœ **: ìˆ™ë ¨ëœ **ì¥ì¸(Teacher)**ì´ **ê²¬ìŠµìƒ(Student)**ì—ê²Œ ê¸°ìˆ ì„ ì „ìˆ˜í•˜ëŠ” ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤. ê²¬ìŠµìƒì€ ì¥ì¸ì˜ ëª¨ë“  ê²½í—˜ì„ ê°€ì§ˆ ìˆ˜ ì—†ì§€ë§Œ, í•µì‹¬ ë…¸í•˜ìš°ë¥¼ ë°°ì›Œ 80%ì˜ ì‹¤ë ¥ì„ ë¹ ë¥´ê²Œ ì–»ì„ ìˆ˜ ìˆì£ . ì‘ì€ ëª¨ë¸(Student)ì´ í° ëª¨ë¸(Teacher)ì˜ "ì§€ì‹"ì„ ë°°ì›Œ ë¹„ìŠ·í•œ ì„±ëŠ¥ì„ ë‚´ëŠ” ê²ƒì…ë‹ˆë‹¤.

**ì§€ì‹ ì¦ë¥˜ì˜ í•µì‹¬ ì•„ì´ë””ì–´:**

ì¼ë°˜ì ì¸ í•™ìŠµì€ **í•˜ë“œ ë¼ë²¨(hard label)** â€” [0, 0, 1, 0, 0] ê°™ì€ ì›-í•« ë²¡í„° â€” ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ Teacher ëª¨ë¸ì˜ **ì†Œí”„íŠ¸ ë¼ë²¨(soft label)** â€” [0.05, 0.1, 0.7, 0.1, 0.05] â€” ì—ëŠ” í´ë˜ìŠ¤ ê°„ ê´€ê³„ ì •ë³´ê°€ ë‹´ê²¨ ìˆìŠµë‹ˆë‹¤. "ê³ ì–‘ì´"ì™€ "ê°•ì•„ì§€"ê°€ ë¹„ìŠ·í•˜ê³  "ìë™ì°¨"ì™€ëŠ” ë‹¤ë¥´ë‹¤ëŠ” ì •ë³´ì£ .

**ì†ì‹¤ í•¨ìˆ˜**:
$$L = \alpha \cdot L_{CE}(y_{student}, y_{true}) + (1-\alpha) \cdot T^2 \cdot L_{KL}(y_{student}^{(T)}, y_{teacher}^{(T)})$$

- $L_{CE}$: ì •ë‹µê³¼ì˜ Cross-Entropy Loss
- $L_{KL}$: Teacherì™€ì˜ KL Divergence Loss
- $T$: Temperature (ì†Œí”„íŠ¸ë‹ ì •ë„, ë³´í†µ 3-20)
- $\alpha$: ë‘ ì†ì‹¤ì˜ ê· í˜• (ë³´í†µ 0.1-0.5)

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import models

class KnowledgeDistillation:
    def __init__(self, teacher, student, temperature=4.0, alpha=0.3):
        """
        Args:
            teacher: í° ì‚¬ì „í•™ìŠµ ëª¨ë¸ (ì˜ˆ: ResNet-50)
            student: ì‘ì€ ëª¨ë¸ (ì˜ˆ: ResNet-18 ë˜ëŠ” MobileNet)
            temperature: ì†Œí”„íŠ¸ ë¼ë²¨ ì˜¨ë„ (ë†’ì„ìˆ˜ë¡ ë¶€ë“œëŸ¬ì›€)
            alpha: hard label ì†ì‹¤ ë¹„ì¤‘ (1-alphaê°€ soft label ë¹„ì¤‘)
        """
        self.teacher = teacher.eval()  # TeacherëŠ” ê³ ì •
        self.student = student
        self.temperature = temperature
        self.alpha = alpha

    def distillation_loss(self, student_logits, teacher_logits, labels):
        """ì¦ë¥˜ ì†ì‹¤ ê³„ì‚°"""
        # 1. Hard Label Loss: ì •ë‹µê³¼ì˜ CE Loss
        hard_loss = F.cross_entropy(student_logits, labels)

        # 2. Soft Label Loss: Teacherì˜ ì†Œí”„íŠ¸ ì¶œë ¥ê³¼ì˜ KL Divergence
        # Temperatureë¡œ ë‚˜ëˆ ì„œ ë¶„í¬ë¥¼ ë¶€ë“œëŸ½ê²Œ ë§Œë“¦
        soft_student = F.log_softmax(student_logits / self.temperature, dim=1)
        soft_teacher = F.softmax(teacher_logits / self.temperature, dim=1)
        soft_loss = F.kl_div(soft_student, soft_teacher, reduction='batchmean')

        # Temperature^2ë¥¼ ê³±í•´ì„œ ê·¸ë˜ë””ì–¸íŠ¸ ìŠ¤ì¼€ì¼ ë³´ì •
        soft_loss = soft_loss * (self.temperature ** 2)

        # ìµœì¢… ì†ì‹¤: ë‘ ì†ì‹¤ì˜ ê°€ì¤‘ í•©
        total_loss = self.alpha * hard_loss + (1 - self.alpha) * soft_loss
        return total_loss, hard_loss.item(), soft_loss.item()

# ì‚¬ìš© ì˜ˆì‹œ
teacher = models.resnet50(pretrained=True)  # í° ëª¨ë¸
student = models.resnet18(pretrained=False)  # ì‘ì€ ëª¨ë¸

# ë¶„ë¥˜ í—¤ë“œ ë§ì¶”ê¸° (ì˜ˆ: CIFAR-10)
num_classes = 10
teacher.fc = nn.Linear(teacher.fc.in_features, num_classes)
student.fc = nn.Linear(student.fc.in_features, num_classes)

kd = KnowledgeDistillation(teacher, student, temperature=4.0, alpha=0.3)
```

```python
# ì „ì²´ í•™ìŠµ ë£¨í”„
def train_with_distillation(kd, train_loader, epochs=10, lr=0.001):
    """ì§€ì‹ ì¦ë¥˜ë¡œ Student ëª¨ë¸ í•™ìŠµ"""
    optimizer = torch.optim.Adam(kd.student.parameters(), lr=lr)
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    kd.teacher.to(device)
    kd.student.to(device)

    for epoch in range(epochs):
        kd.student.train()
        total_loss = 0
        correct = 0
        total = 0

        for images, labels in train_loader:
            images, labels = images.to(device), labels.to(device)

            # Teacherì˜ ì˜ˆì¸¡ (ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° X)
            with torch.no_grad():
                teacher_logits = kd.teacher(images)

            # Studentì˜ ì˜ˆì¸¡
            student_logits = kd.student(images)

            # ì¦ë¥˜ ì†ì‹¤ ê³„ì‚°
            loss, hard_l, soft_l = kd.distillation_loss(
                student_logits, teacher_logits, labels
            )

            # ì—­ì „íŒŒ
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            _, predicted = student_logits.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

        acc = 100. * correct / total
        print(f"Epoch {epoch+1}: Loss={total_loss/len(train_loader):.4f}, Acc={acc:.2f}%")

    return kd.student

# í•™ìŠµ ì‹¤í–‰ (ë°ì´í„°ë¡œë” í•„ìš”)
# trained_student = train_with_distillation(kd, train_loader)
```

> ğŸ’¡ **ì•Œê³  ê³„ì…¨ë‚˜ìš”?**: ì§€ì‹ ì¦ë¥˜ëŠ” 2015ë…„ Hintonì˜ ë…¼ë¬¸ "Distilling the Knowledge in a Neural Network"ì—ì„œ ì²˜ìŒ ì œì•ˆë˜ì—ˆìŠµë‹ˆë‹¤. ì¬ë¯¸ìˆê²Œë„, ì´ ì•„ì´ë””ì–´ëŠ” Hintonì´ "dark knowledge(ì•”ë¬µì§€)"ë¼ê³  ë¶€ë¥¸ Teacherì˜ ì†Œí”„íŠ¸ ì¶œë ¥ì—ì„œ í•™ìŠµ ê°€ëŠ¥í•œ ì •ë³´ê°€ í›¨ì”¬ ë§ë‹¤ëŠ” í†µì°°ì—ì„œ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.

### ê°œë… 4: ì„¸ ê¸°ë²•ì˜ ì¡°í•©

ì‹¤ë¬´ì—ì„œëŠ” ì„¸ ê¸°ë²•ì„ **í•¨ê»˜ ì‚¬ìš©**í•˜ëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤.

**ìµœì í™” íŒŒì´í”„ë¼ì¸ ì˜ˆì‹œ:**

1. **ì§€ì‹ ì¦ë¥˜**: ResNet-50 â†’ MobileNetV3 (ëª¨ë¸ ì¶•ì†Œ)
2. **í”„ë£¨ë‹**: 50% êµ¬ì¡°ì  í”„ë£¨ë‹ (ì—°ì‚°ëŸ‰ ê°ì†Œ)
3. **ì–‘ìí™”**: INT8 ë³€í™˜ (ë©”ëª¨ë¦¬ 4ë°° ê°ì†Œ)

| ë‹¨ê³„ | ëª¨ë¸ í¬ê¸° | ì •í™•ë„ | ì¶”ë¡  ì‹œê°„ |
|------|-----------|--------|-----------|
| ì›ë³¸ (ResNet-50) | 98 MB | 76.1% | 50 ms |
| ì¦ë¥˜ (MobileNetV3) | 12 MB | 74.5% | 15 ms |
| +í”„ë£¨ë‹ (50%) | 8 MB | 73.8% | 10 ms |
| +ì–‘ìí™” (INT8) | 2 MB | 73.2% | 5 ms |

**ìµœì¢… ê²°ê³¼**: 98 MB â†’ 2 MB (49ë°° ì••ì¶•), 50 ms â†’ 5 ms (10ë°° ë¹ ë¦„), ì •í™•ë„ ì†ì‹¤ 3% ë¯¸ë§Œ

```python
# ì„¸ ê¸°ë²• ì¡°í•© íŒŒì´í”„ë¼ì¸
def full_optimization_pipeline(teacher_model, calibration_loader):
    """
    1. ì§€ì‹ ì¦ë¥˜
    2. í”„ë£¨ë‹
    3. ì–‘ìí™”
    """
    from torchvision import models
    import torch.nn.utils.prune as prune
    import torch.quantization as quant

    # 1ë‹¨ê³„: ì§€ì‹ ì¦ë¥˜ (Student ëª¨ë¸ í•™ìŠµ)
    print("1ë‹¨ê³„: ì§€ì‹ ì¦ë¥˜ ì‹œì‘...")
    student = models.mobilenet_v3_small(pretrained=False)
    # ... ì¦ë¥˜ í•™ìŠµ ìˆ˜í–‰ (ìœ„ì˜ ì½”ë“œ ì°¸ì¡°)

    # 2ë‹¨ê³„: êµ¬ì¡°ì  í”„ë£¨ë‹
    print("2ë‹¨ê³„: í”„ë£¨ë‹ ì ìš©...")
    for name, module in student.named_modules():
        if isinstance(module, nn.Conv2d):
            prune.ln_structured(module, 'weight', amount=0.3, n=2, dim=0)
            prune.remove(module, 'weight')

    # 3ë‹¨ê³„: ì–‘ìí™”
    print("3ë‹¨ê³„: ì–‘ìí™” ì ìš©...")
    student.eval()
    student.qconfig = quant.get_default_qconfig('qnnpack')  # ëª¨ë°”ì¼ìš©
    student_prepared = quant.prepare(student)

    # ìº˜ë¦¬ë¸Œë ˆì´ì…˜
    with torch.no_grad():
        for images, _ in calibration_loader:
            student_prepared(images)

    optimized_model = quant.convert(student_prepared)
    print("ìµœì í™” ì™„ë£Œ!")

    return optimized_model
```

## ë” ê¹Šì´ ì•Œì•„ë³´ê¸°: NVIDIA Model Optimizer

2025ë…„ 12ì›”, NVIDIAëŠ” **TensorRT Model Optimizer**ë¥¼ **NVIDIA Model Optimizer**ë¡œ í™•ì¥ ë¦¬ë¸Œëœë”©í–ˆìŠµë‹ˆë‹¤. ì´ ë„êµ¬ëŠ” ì–‘ìí™”, í”„ë£¨ë‹, ì§€ì‹ ì¦ë¥˜ë¥¼ í†µí•©í•˜ì—¬ TensorRT, TensorRT-LLM, vLLM ë“± ë‹¤ì–‘í•œ ë°°í¬ í”„ë ˆì„ì›Œí¬ë¥¼ ìœ„í•œ ìµœì í™”ë¥¼ ì§€ì›í•©ë‹ˆë‹¤.

```python
# NVIDIA Model Optimizer ì‚¬ìš© ì˜ˆì‹œ (ê°œë…ì )
# pip install nvidia-modelopt

# import modelopt as mo
#
# # PTQ ì–‘ìí™”
# quantized_model = mo.torch.quantize(
#     model,
#     config=mo.torch.QuantConfig(precision="int8"),
#     calibration_data=calibration_loader
# )
#
# # ìŠ¤íŒŒì‹œí‹° ì ìš©
# sparse_model = mo.torch.sparsify(
#     model,
#     config=mo.torch.SparseConfig(sparsity=0.5)
# )
```

## í•µì‹¬ ì •ë¦¬

| ê¸°ë²• | ì›ë¦¬ | ì••ì¶•ë¥  | ì •í™•ë„ ì†ì‹¤ | ë‚œì´ë„ |
|------|------|--------|------------|--------|
| **ì–‘ìí™”** | FP32 â†’ INT8 ë³€í™˜ | 4ë°° | 0.5-1% | ì‰¬ì›€ |
| **í”„ë£¨ë‹** | ì‘ì€ ê°€ì¤‘ì¹˜ ì œê±° | 2-10ë°° | 1-3% | ì¤‘ê°„ |
| **ì§€ì‹ ì¦ë¥˜** | í° ëª¨ë¸ â†’ ì‘ì€ ëª¨ë¸ | ê°€ë³€ | 1-5% | ì¤‘ê°„ |
| **ì¡°í•©** | ì„¸ ê¸°ë²• í†µí•© | 10-50ë°° | 2-5% | ë†’ìŒ |

## ë‹¤ìŒ ì„¹ì…˜ ë¯¸ë¦¬ë³´ê¸°

ëª¨ë¸ì„ ìµœì í™”í–ˆë‹¤ë©´, ì´ì œ **ë” ë¹ ë¥¸ ì¶”ë¡  ì—”ì§„**ìœ¼ë¡œ ë³€í™˜í•  ì°¨ë¡€ì…ë‹ˆë‹¤. ë‹¤ìŒ ì„¹ì…˜ [ONNXì™€ TensorRT](./02-onnx-tensorrt.md)ì—ì„œëŠ” PyTorch ëª¨ë¸ì„ ONNX í¬ë§·ìœ¼ë¡œ ë³€í™˜í•˜ê³ , NVIDIA TensorRTë¡œ ìµœì í™”í•˜ì—¬ **2-5ë°° ì¶”ê°€ ì†ë„ í–¥ìƒ**ì„ ì–»ëŠ” ë°©ë²•ì„ ë°°ì›ë‹ˆë‹¤.

## ì°¸ê³  ìë£Œ

- [PyTorch Knowledge Distillation Tutorial](https://docs.pytorch.org/tutorials/beginner/knowledge_distillation_tutorial.html) - ê³µì‹ ì§€ì‹ ì¦ë¥˜ íŠœí† ë¦¬ì–¼
- [Model Compression: Quantization, Pruning, Distillation](https://towardsdatascience.com/model-compression-make-your-machine-learning-models-lighter-and-faster/) - ì„¸ ê¸°ë²• ì¢…í•© ì„¤ëª…
- [NVIDIA Model Optimizer](https://github.com/NVIDIA/Model-Optimizer) - NVIDIA ê³µì‹ ìµœì í™” ë„êµ¬
- [Deepgram: Model Pruning, Distillation, and Quantization](https://deepgram.com/learn/model-pruning-distillation-and-quantization-part-1) - ì‹¤ë¬´ ê´€ì  í•´ì„¤
- [AI Model Optimization 2025](https://aether-nexus.vercel.app/blog/ai-model-optimization-2025) - ìµœì‹  íŠ¸ë Œë“œ
