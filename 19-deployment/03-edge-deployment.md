# ì—£ì§€ ë°°í¬

> Jetson, ë¼ì¦ˆë² ë¦¬íŒŒì´, ëª¨ë°”ì¼

## ê°œìš”

ì§€ê¸ˆê¹Œì§€ ì„œë²„ì—ì„œ ëª¨ë¸ì„ ìµœì í™”í•˜ê³  ê°€ì†í™”í•˜ëŠ” ë°©ë²•ì„ ë°°ì› ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ëª¨ë“  AIê°€ í´ë¼ìš°ë“œì—ì„œ ëŒì•„ê°ˆ í•„ìš”ëŠ” ì—†ìŠµë‹ˆë‹¤. **ììœ¨ì£¼í–‰ì°¨, ë“œë¡ , ìŠ¤ë§ˆíŠ¸ ì¹´ë©”ë¼, ìŠ¤ë§ˆíŠ¸í°**ì€ ì¸í„°ë„· ì—†ì´ë„ ì‹¤ì‹œê°„ AIë¥¼ ì‹¤í–‰í•´ì•¼ í•©ë‹ˆë‹¤. ì´ ì„¹ì…˜ì—ì„œëŠ” **ì—£ì§€ ë””ë°”ì´ìŠ¤**ì—ì„œ ë¹„ì „ ëª¨ë¸ì„ ë°°í¬í•˜ëŠ” ë°©ë²•ì„ ë°°ì›ë‹ˆë‹¤.

**ì„ ìˆ˜ ì§€ì‹**:
- [ONNXì™€ TensorRT](./02-onnx-tensorrt.md)
- ê¸°ë³¸ì ì¸ Linux ëª…ë ¹ì–´

**í•™ìŠµ ëª©í‘œ**:
- ì—£ì§€ AIì˜ ê°œë…ê³¼ ì¥ì  ì´í•´í•˜ê¸°
- NVIDIA Jetsonì—ì„œ ë¹„ì „ ëª¨ë¸ ì‹¤í–‰í•˜ê¸°
- ë¼ì¦ˆë² ë¦¬íŒŒì´ì™€ ëª¨ë°”ì¼ ë°°í¬ ë°©ë²• ìµíˆê¸°

## ì™œ ì•Œì•„ì•¼ í• ê¹Œ?

> ğŸ’¡ **ë¹„ìœ **: í´ë¼ìš°ë“œ AIëŠ” **ì¤‘ì•™ ë°œì „ì†Œ**ì™€ ê°™ê³ , ì—£ì§€ AIëŠ” **íƒœì–‘ê´‘ íŒ¨ë„**ê³¼ ê°™ìŠµë‹ˆë‹¤. ë°œì „ì†ŒëŠ” ëŒ€ê·œëª¨ ì „ë ¥ì„ ìƒì‚°í•˜ì§€ë§Œ ì†¡ì „ì„ ì´ í•„ìš”í•©ë‹ˆë‹¤. íƒœì–‘ê´‘ íŒ¨ë„ì€ ì‘ì§€ë§Œ í˜„ì¥ì—ì„œ ë°”ë¡œ ì „ê¸°ë¥¼ ë§Œë“¤ì£ . ììœ¨ì£¼í–‰ì°¨ê°€ "ì ê¹, ì„œë²„ì— ë¬¼ì–´ë³¼ê²Œìš”"ë¼ê³  í•  ìˆ˜ëŠ” ì—†ê² ì£ ?

**í´ë¼ìš°ë“œ vs ì—£ì§€ ë¹„êµ:**

| í•­ëª© | í´ë¼ìš°ë“œ AI | ì—£ì§€ AI |
|------|-------------|---------|
| **ì§€ì—° ì‹œê°„** | 50-500ms (ë„¤íŠ¸ì›Œí¬) | 5-50ms (ë¡œì»¬) |
| **ì—°ê²° í•„ìš”** | í•„ìˆ˜ | ë¶ˆí•„ìš” |
| **í”„ë¼ì´ë²„ì‹œ** | ë°ì´í„° ì „ì†¡ | ë¡œì»¬ ì²˜ë¦¬ |
| **ë¹„ìš©** | ì‚¬ìš©ëŸ‰ ê³¼ê¸ˆ | ì´ˆê¸° í•˜ë“œì›¨ì–´ |
| **í™•ì¥ì„±** | ë¬´ì œí•œ | ë””ë°”ì´ìŠ¤ í•œê³„ |
| **ì „ë ¥** | ë¬´ì œí•œ | ë°°í„°ë¦¬/ì œí•œì  |

**ì—£ì§€ AI ì‹œì¥ ê·œëª¨ (2025ë…„ ê¸°ì¤€):**
- ììœ¨ì£¼í–‰: $50B+
- ìŠ¤ë§ˆíŠ¸ ì¹´ë©”ë¼/ë³´ì•ˆ: $20B+
- ë“œë¡ /ë¡œë´‡: $15B+
- ìŠ¤ë§ˆíŠ¸í° AI: $30B+

## í•µì‹¬ ê°œë…

### ê°œë… 1: ì—£ì§€ ë””ë°”ì´ìŠ¤ ìŠ¤í™íŠ¸ëŸ¼

> ğŸ’¡ **ë¹„ìœ **: ì—£ì§€ ë””ë°”ì´ìŠ¤ëŠ” **ìš´ë™ì„ ìˆ˜ì˜ ì²´ê¸‰**ê³¼ ê°™ìŠµë‹ˆë‹¤. í—¤ë¹„ê¸‰(Jetson AGX), ë¯¸ë“¤ê¸‰(Jetson Orin Nano), ë¼ì´íŠ¸ê¸‰(ë¼ì¦ˆë² ë¦¬íŒŒì´), í”Œë¼ì´ê¸‰(ë§ˆì´í¬ë¡œì»¨íŠ¸ë¡¤ëŸ¬)ì´ ìˆê³ , ê°ê° ë‹¤ë¥¸ ê²½ê¸°ì— ì í•©í•©ë‹ˆë‹¤.

**2025ë…„ ì£¼ìš” ì—£ì§€ í”Œë«í¼:**

| í”Œë«í¼ | AI ì„±ëŠ¥ | ì „ë ¥ | ê°€ê²© | ìš©ë„ |
|--------|---------|------|------|------|
| **Jetson AGX Orin** | 275 TOPS | 15-60W | $1,999 | ììœ¨ì£¼í–‰, ëŒ€í˜• ë¡œë´‡ |
| **Jetson Orin Nano Super** | 67 TOPS | 7-25W | $249 | ë“œë¡ , ìŠ¤ë§ˆíŠ¸ ì¹´ë©”ë¼ |
| **Jetson Orin NX** | 100 TOPS | 10-25W | $699 | ì¤‘í˜• ë¡œë´‡, AMR |
| **Jetson Thor** | 2,070 TOPS | 40-130W | TBD | íœ´ë¨¸ë…¸ì´ë“œ ë¡œë´‡ |
| **Raspberry Pi 5** | 13 TOPS* | 5-12W | $80 | êµìœ¡, í”„ë¡œí† íƒ€ì… |
| **Google Coral** | 4 TOPS | 2W | $150 | ì´ˆì €ì „ë ¥ ì¶”ë¡  |

*ë¼ì¦ˆë² ë¦¬íŒŒì´ 5 + Hailo-8L ê°€ì†ê¸° ì‚¬ìš© ì‹œ

### ê°œë… 2: NVIDIA Jetson ë°°í¬

Jetsonì€ NVIDIA GPUë¥¼ íƒ‘ì¬í•œ **ê°€ì¥ ê°•ë ¥í•œ ì—£ì§€ í”Œë«í¼**ì…ë‹ˆë‹¤. TensorRT, CUDAê°€ ë„¤ì´í‹°ë¸Œë¡œ ì§€ì›ë©ë‹ˆë‹¤.

**Jetson ê°œë°œ í™˜ê²½ ì„¤ì •:**

```bash
# JetPack SDK ì„¤ì¹˜ í™•ì¸
cat /etc/nv_tegra_release
# R35 (release), REVISION: 3.1

# CUDA, TensorRT ë²„ì „ í™•ì¸
nvcc --version  # CUDA 11.4+
dpkg -l | grep TensorRT  # TensorRT 8.5+

# Python í™˜ê²½ ì„¤ì •
python3 -m pip install --upgrade pip
pip3 install torch torchvision  # Jetsonìš© wheel

# ONNX Runtime (Jetson ì „ìš© ë¹Œë“œ)
pip3 install onnxruntime-gpu
```

```python
# Jetsonì—ì„œ YOLOv8 ì‹¤í–‰ ì˜ˆì‹œ
from ultralytics import YOLO
import cv2

# ëª¨ë¸ ë¡œë“œ (ìë™ìœ¼ë¡œ Jetson ìµœì í™”)
model = YOLO('yolov8n.pt')

# TensorRTë¡œ ë³€í™˜ (Jetson GPU ìµœì í™”)
model.export(format='engine', device=0, half=True)

# ë³€í™˜ëœ ì—”ì§„ìœ¼ë¡œ ì¶”ë¡ 
model_trt = YOLO('yolov8n.engine')

# ì›¹ìº  ì‹¤ì‹œê°„ ì¶”ë¡ 
cap = cv2.VideoCapture(0)
cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)

while True:
    ret, frame = cap.read()
    if not ret:
        break

    # ì¶”ë¡ 
    results = model_trt(frame)

    # ê²°ê³¼ ì‹œê°í™”
    annotated = results[0].plot()
    cv2.imshow('YOLOv8 on Jetson', annotated)

    # FPS ê³„ì‚°
    fps = 1000 / results[0].speed['inference']
    print(f"FPS: {fps:.1f}")

    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()
```

```python
# Jetson ì „ìš© ìµœì í™”: DeepStream SDK
# GStreamer ê¸°ë°˜ ê³ ì„±ëŠ¥ ë¹„ë””ì˜¤ íŒŒì´í”„ë¼ì¸

import gi
gi.require_version('Gst', '1.0')
from gi.repository import Gst, GLib

# DeepStream íŒŒì´í”„ë¼ì¸ ì˜ˆì‹œ (ê°œë…ì )
# ì‹¤ì œ ì‚¬ìš©ì‹œ NVIDIA DeepStream SDK ì„¤ì¹˜ í•„ìš”

DEEPSTREAM_PIPELINE = """
    filesrc location=video.mp4 !
    decodebin !
    nvstreammux batch-size=1 !
    nvinfer config-file-path=yolo_config.txt !
    nvvideoconvert !
    nvdsosd !
    nvegltransform !
    nveglglessink
"""

# íŒŒì´í”„ë¼ì¸ ì¥ì :
# - í•˜ë“œì›¨ì–´ ë””ì½”ë”© (NVDEC)
# - ë°°ì¹˜ ì¶”ë¡ 
# - ë©€í‹° ìŠ¤íŠ¸ë¦¼ ì§€ì›
# - GPU ë©”ëª¨ë¦¬ ì§ì ‘ ì „ë‹¬ (zero-copy)
```

> ğŸ’¡ **ì•Œê³  ê³„ì…¨ë‚˜ìš”?**: 2025ë…„ NVIDIA Jetson ThorëŠ” **2,070 TOPS**ì˜ ì—°ì‚° ì„±ëŠ¥ì„ ìë‘í•©ë‹ˆë‹¤. ì´ëŠ” ë¶ˆê³¼ 5ë…„ ì „ ë°ì´í„°ì„¼í„° GPU ìˆ˜ì¤€ì˜ ì„±ëŠ¥ì„ ì†ë°”ë‹¥ í¬ê¸° ë³´ë“œì—ì„œ êµ¬í˜„í•œ ê²ƒì…ë‹ˆë‹¤. íœ´ë¨¸ë…¸ì´ë“œ ë¡œë´‡ì˜ ì‹¤ì‹œê°„ ì¶”ë¡ ì„ ëª©í‘œë¡œ ê°œë°œë˜ì—ˆìŠµë‹ˆë‹¤.

### ê°œë… 3: ë¼ì¦ˆë² ë¦¬íŒŒì´ ë°°í¬

ë¼ì¦ˆë² ë¦¬íŒŒì´ëŠ” **ì €ë ´í•œ ê°€ê²©ê³¼ í’ë¶€í•œ ìƒíƒœê³„**ë¡œ í”„ë¡œí† íƒ€ì´í•‘ê³¼ êµìœ¡ì— ì í•©í•©ë‹ˆë‹¤. AI ê°€ì†ê¸°ë¥¼ ì¶”ê°€í•˜ë©´ ì‹¤ìš©ì ì¸ ì¶”ë¡ ë„ ê°€ëŠ¥í•©ë‹ˆë‹¤.

**ë¼ì¦ˆë² ë¦¬íŒŒì´ 5 + AI HAT ì„¤ì •:**

```bash
# ë¼ì¦ˆë² ë¦¬íŒŒì´ 5 ê¸°ë³¸ ì„¤ì •
sudo apt update && sudo apt upgrade -y
sudo apt install python3-opencv python3-pip

# Hailo AI HAT ì„¤ì¹˜ (13 TOPS ê°€ì†ê¸°)
# https://hailo.ai/products/hailo-rpi5-hat/
pip3 install hailort

# ë˜ëŠ” Google Coral USB ê°€ì†ê¸°
pip3 install pycoral
```

```python
# ë¼ì¦ˆë² ë¦¬íŒŒì´ + TFLite ì¶”ë¡ 
import numpy as np
from PIL import Image

try:
    # TFLite Runtime (ê²½ëŸ‰ ë²„ì „)
    import tflite_runtime.interpreter as tflite
except ImportError:
    import tensorflow.lite as tflite

class RaspberryPiInference:
    def __init__(self, model_path, num_threads=4):
        """ë¼ì¦ˆë² ë¦¬íŒŒì´ ìµœì í™” ì¶”ë¡ """
        # TFLite ì¸í„°í”„ë¦¬í„° ìƒì„±
        self.interpreter = tflite.Interpreter(
            model_path=model_path,
            num_threads=num_threads  # ë©€í‹°ì½”ì–´ í™œìš©
        )
        self.interpreter.allocate_tensors()

        # ì…ì¶œë ¥ ì •ë³´
        self.input_details = self.interpreter.get_input_details()
        self.output_details = self.interpreter.get_output_details()

        # ì…ë ¥ í˜•íƒœ
        self.input_shape = self.input_details[0]['shape']
        print(f"ì…ë ¥ í˜•íƒœ: {self.input_shape}")

    def preprocess(self, image_path):
        """ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
        img = Image.open(image_path).convert('RGB')
        img = img.resize((self.input_shape[2], self.input_shape[1]))
        img_array = np.array(img, dtype=np.float32)

        # ì •ê·œí™” (ëª¨ë¸ì— ë”°ë¼ ì¡°ì •)
        img_array = img_array / 255.0
        img_array = np.expand_dims(img_array, axis=0)
        return img_array

    def infer(self, input_data):
        """ì¶”ë¡  ì‹¤í–‰"""
        self.interpreter.set_tensor(
            self.input_details[0]['index'],
            input_data
        )
        self.interpreter.invoke()

        output = self.interpreter.get_tensor(
            self.output_details[0]['index']
        )
        return output

# ì‚¬ìš© ì˜ˆì‹œ
# rpi_model = RaspberryPiInference('mobilenet_v2.tflite')
# result = rpi_model.infer(rpi_model.preprocess('cat.jpg'))
```

```python
# Coral USB ê°€ì†ê¸° ì‚¬ìš© (4 TOPS Edge TPU)
from pycoral.utils import edgetpu
from pycoral.adapters import common, classify

def coral_inference(model_path, image_path):
    """Google Coral Edge TPU ì¶”ë¡ """

    # Edge TPU ì¸í„°í”„ë¦¬í„° ìƒì„±
    interpreter = edgetpu.make_interpreter(model_path)
    interpreter.allocate_tensors()

    # ì´ë¯¸ì§€ ë¡œë“œ ë° ì „ì²˜ë¦¬
    image = Image.open(image_path).convert('RGB')
    _, scale = common.set_resized_input(
        interpreter,
        image.size,
        lambda size: image.resize(size, Image.LANCZOS)
    )

    # ì¶”ë¡ 
    interpreter.invoke()

    # ê²°ê³¼ (ë¶„ë¥˜ ì˜ˆì‹œ)
    classes = classify.get_classes(interpreter, top_k=5)
    for c in classes:
        print(f"í´ë˜ìŠ¤ {c.id}: {c.score:.3f}")

    return classes

# ì‚¬ìš© ì˜ˆì‹œ (Edge TPU ì»´íŒŒì¼ëœ ëª¨ë¸ í•„ìš”)
# coral_inference('mobilenet_edgetpu.tflite', 'cat.jpg')
```

> âš ï¸ **í”í•œ ì˜¤í•´**: "ë¼ì¦ˆë² ë¦¬íŒŒì´ì—ì„œëŠ” ë”¥ëŸ¬ë‹ì„ ëª» ëŒë¦°ë‹¤" â€” í‹€ë ¸ìŠµë‹ˆë‹¤! ë¼ì¦ˆë² ë¦¬íŒŒì´ 5ëŠ” **CPUë§Œìœ¼ë¡œë„** MobileNet ìˆ˜ì¤€ ëª¨ë¸ì„ 10-20 FPSë¡œ ëŒë¦´ ìˆ˜ ìˆê³ , AI ê°€ì†ê¸°ë¥¼ ì¶”ê°€í•˜ë©´ **30+ FPS**ë„ ê°€ëŠ¥í•©ë‹ˆë‹¤.

### ê°œë… 4: ëª¨ë°”ì¼ ë°°í¬ (iOS/Android)

ìŠ¤ë§ˆíŠ¸í°ì€ ê°€ì¥ ë„ë¦¬ ë³´ê¸‰ëœ ì—£ì§€ ë””ë°”ì´ìŠ¤ì…ë‹ˆë‹¤. **Neural Engine(iOS)**, **NPU(Android)**ê°€ íƒ‘ì¬ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

**ëª¨ë°”ì¼ ë°°í¬ ì˜µì…˜:**

| í”„ë ˆì„ì›Œí¬ | iOS | Android | ì¥ì  |
|------------|-----|---------|------|
| **CoreML** | âœ… | âŒ | Apple ìµœì í™”, Swift í†µí•© |
| **TFLite** | âœ… | âœ… | í¬ë¡œìŠ¤ í”Œë«í¼, ê²½ëŸ‰ |
| **PyTorch Mobile** | âœ… | âœ… | PyTorch ìƒíƒœê³„ |
| **ONNX Runtime Mobile** | âœ… | âœ… | ë²”ìš©ì„± |
| **MediaPipe** | âœ… | âœ… | ì‚¬ì „ í›ˆë ¨ ì†”ë£¨ì…˜ |

```python
# PyTorch â†’ CoreML ë³€í™˜ (iOS)
import torch
import coremltools as ct
from torchvision import models

# ëª¨ë¸ ì¤€ë¹„
model = models.mobilenet_v2(pretrained=True)
model.eval()

# TorchScriptë¡œ ë³€í™˜
example_input = torch.rand(1, 3, 224, 224)
traced_model = torch.jit.trace(model, example_input)

# CoreMLë¡œ ë³€í™˜
mlmodel = ct.convert(
    traced_model,
    inputs=[ct.ImageType(
        name="image",
        shape=example_input.shape,
        scale=1/255.0,
        bias=[-0.485/0.229, -0.456/0.224, -0.406/0.225]
    )],
    minimum_deployment_target=ct.target.iOS15
)

# ë©”íƒ€ë°ì´í„° ì¶”ê°€
mlmodel.author = 'CV Tutorial'
mlmodel.short_description = 'MobileNetV2 ì´ë¯¸ì§€ ë¶„ë¥˜'

# ì €ì¥
mlmodel.save('MobileNetV2.mlpackage')
print("CoreML ëª¨ë¸ ì €ì¥ ì™„ë£Œ!")
```

```python
# PyTorch â†’ TFLite ë³€í™˜ (Android/iOS)
import torch
import tensorflow as tf

# 1ë‹¨ê³„: PyTorch â†’ ONNX
model = models.mobilenet_v2(pretrained=True)
model.eval()

dummy_input = torch.randn(1, 3, 224, 224)
torch.onnx.export(model, dummy_input, "mobilenet.onnx")

# 2ë‹¨ê³„: ONNX â†’ TensorFlow (onnx-tf ì‚¬ìš©)
# pip install onnx-tf
import onnx
from onnx_tf.backend import prepare

onnx_model = onnx.load("mobilenet.onnx")
tf_rep = prepare(onnx_model)
tf_rep.export_graph("mobilenet_tf")

# 3ë‹¨ê³„: TensorFlow â†’ TFLite
converter = tf.lite.TFLiteConverter.from_saved_model("mobilenet_tf")

# ìµœì í™” ì˜µì…˜
converter.optimizations = [tf.lite.Optimize.DEFAULT]  # ê¸°ë³¸ ì–‘ìí™”

# ì„ íƒ: ì „ì²´ ì •ìˆ˜ ì–‘ìí™” (ë” ë¹ ë¦„)
# converter.target_spec.supported_types = [tf.int8]

tflite_model = converter.convert()

# ì €ì¥
with open('mobilenet.tflite', 'wb') as f:
    f.write(tflite_model)
print("TFLite ëª¨ë¸ ì €ì¥ ì™„ë£Œ!")
```

```kotlin
// Androidì—ì„œ TFLite ì¶”ë¡  (Kotlin ì˜ˆì‹œ)
import org.tensorflow.lite.Interpreter

class ImageClassifier(context: Context) {
    private val interpreter: Interpreter

    init {
        // ëª¨ë¸ ë¡œë“œ
        val model = loadModelFile(context, "mobilenet.tflite")
        val options = Interpreter.Options().apply {
            setNumThreads(4)
            // GPU ë¸ë¦¬ê²Œì´íŠ¸ (ì„ íƒ)
            // addDelegate(GpuDelegate())
        }
        interpreter = Interpreter(model, options)
    }

    fun classify(bitmap: Bitmap): FloatArray {
        // ì „ì²˜ë¦¬
        val input = preprocessImage(bitmap)

        // ì¶œë ¥ ë²„í¼
        val output = Array(1) { FloatArray(1000) }

        // ì¶”ë¡ 
        interpreter.run(input, output)

        return output[0]
    }
}
```

> ğŸ”¥ **ì‹¤ë¬´ íŒ**: ëª¨ë°”ì¼ ë°°í¬ ì‹œ **ëª¨ë¸ í¬ê¸°**ê°€ ì¤‘ìš”í•©ë‹ˆë‹¤. ì•± ìŠ¤í† ì–´ëŠ” 100MB ì´ìƒ ì•±ì€ Wi-Fiì—ì„œë§Œ ë‹¤ìš´ë¡œë“œë¥¼ í—ˆìš©í•©ë‹ˆë‹¤. MobileNet(14MB), EfficientNet-Lite(20MB) ê°™ì€ ê²½ëŸ‰ ëª¨ë¸ì„ ì„ íƒí•˜ê±°ë‚˜, ëª¨ë¸ì„ ì•± ë²ˆë“¤ì´ ì•„ë‹Œ ì„œë²„ì—ì„œ ë‹¤ìš´ë¡œë“œí•˜ëŠ” ë°©ì‹ì„ ê³ ë ¤í•˜ì„¸ìš”.

### ê°œë… 5: ì—£ì§€ ë°°í¬ ìµœì í™” ì „ëµ

```python
# ì—£ì§€ ìµœì í™” ì²´í¬ë¦¬ìŠ¤íŠ¸
class EdgeOptimizationPipeline:
    """ì—£ì§€ ë°°í¬ë¥¼ ìœ„í•œ ìµœì í™” íŒŒì´í”„ë¼ì¸"""

    def __init__(self, model, target_device):
        self.model = model
        self.target = target_device  # 'jetson', 'rpi', 'mobile'

    def optimize(self):
        """ë””ë°”ì´ìŠ¤ë³„ ìµœì í™” ìˆ˜í–‰"""
        steps = []

        # 1. ê³µí†µ: ëª¨ë¸ ê²½ëŸ‰í™”
        steps.append(('í”„ë£¨ë‹', self.apply_pruning))
        steps.append(('ì–‘ìí™”', self.apply_quantization))

        # 2. ë””ë°”ì´ìŠ¤ë³„ ë³€í™˜
        if self.target == 'jetson':
            steps.append(('TensorRT ë³€í™˜', self.convert_tensorrt))
        elif self.target == 'rpi':
            steps.append(('TFLite ë³€í™˜', self.convert_tflite))
        elif self.target == 'mobile':
            steps.append(('CoreML/TFLite ë³€í™˜', self.convert_mobile))

        # 3. ë²¤ì¹˜ë§ˆí¬
        steps.append(('ì„±ëŠ¥ ì¸¡ì •', self.benchmark))

        for name, func in steps:
            print(f"[{name}] ì‹œì‘...")
            func()
            print(f"[{name}] ì™„ë£Œ!")

        return self.model

    def apply_pruning(self):
        """30% êµ¬ì¡°ì  í”„ë£¨ë‹"""
        import torch.nn.utils.prune as prune
        for module in self.model.modules():
            if isinstance(module, torch.nn.Conv2d):
                prune.ln_structured(module, 'weight', 0.3, n=2, dim=0)

    def apply_quantization(self):
        """INT8 ì–‘ìí™”"""
        self.model = torch.quantization.quantize_dynamic(
            self.model, {torch.nn.Conv2d, torch.nn.Linear}, torch.qint8
        )

    def convert_tensorrt(self):
        """TensorRT ì—”ì§„ ìƒì„±"""
        # ONNX â†’ TensorRT (ì´ì „ ì„¹ì…˜ ì°¸ì¡°)
        pass

    def convert_tflite(self):
        """TFLite ë³€í™˜"""
        # PyTorch â†’ ONNX â†’ TF â†’ TFLite
        pass

    def convert_mobile(self):
        """ëª¨ë°”ì¼ í¬ë§· ë³€í™˜"""
        # CoreML (iOS) ë˜ëŠ” TFLite (Android)
        pass

    def benchmark(self):
        """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
        import time
        dummy = torch.randn(1, 3, 224, 224)

        # ì›Œë°ì—…
        for _ in range(10):
            self.model(dummy)

        # ì¸¡ì •
        start = time.time()
        for _ in range(100):
            self.model(dummy)
        elapsed = (time.time() - start) / 100 * 1000

        print(f"í‰ê·  ì¶”ë¡  ì‹œê°„: {elapsed:.2f} ms")
        print(f"FPS: {1000/elapsed:.1f}")
```

**ë””ë°”ì´ìŠ¤ë³„ ê¶Œì¥ ëª¨ë¸:**

| ë””ë°”ì´ìŠ¤ | ê¶Œì¥ ëª¨ë¸ | ì˜ˆìƒ FPS | ì •í™•ë„ |
|----------|-----------|----------|--------|
| Jetson Orin Nano | YOLOv8-s | 60+ | ë†’ìŒ |
| Jetson Orin Nano | ResNet-50 | 100+ | ë†’ìŒ |
| ë¼ì¦ˆë² ë¦¬íŒŒì´ 5 | MobileNetV3 | 30+ | ì¤‘ê°„ |
| ë¼ì¦ˆë² ë¦¬íŒŒì´ 5 + Hailo | YOLOv8-n | 50+ | ì¤‘ê°„ |
| ìŠ¤ë§ˆíŠ¸í° (2024+) | EfficientNet-Lite | 60+ | ì¤‘ê°„ |

## ë” ê¹Šì´ ì•Œì•„ë³´ê¸°: Jetsonê³¼ ë¼ì¦ˆë² ë¦¬íŒŒì´ì˜ ìœµí•©

í¥ë¯¸ë¡­ê²Œë„, 2025ë…„ì—ëŠ” **ë¼ì¦ˆë² ë¦¬íŒŒì´ + Jetsonì˜ ìœµí•© í”„ë¡œì íŠ¸**ê°€ ì¸ê¸°ì…ë‹ˆë‹¤. ë¼ì¦ˆë² ë¦¬íŒŒì´ì˜ ì €ë ´í•œ ì£¼ë³€ì¥ì¹˜ ì—°ê²°ê³¼ Jetsonì˜ AI ì„±ëŠ¥ì„ ì¡°í•©í•˜ëŠ” ê²ƒì´ì£ .

ì˜ˆ: ë¼ì¦ˆë² ë¦¬íŒŒì´ë¡œ ì¹´ë©”ë¼, ì„¼ì„œ, ëª¨í„°ë¥¼ ì œì–´í•˜ê³ , Jetson Orin Nano Super($249)ë¡œ ì‹¤ì‹œê°„ ê°ì²´ íƒì§€ë¥¼ ìˆ˜í–‰í•˜ëŠ” AI ë¡œë²„. ì „ì²´ ë¹„ìš© $400 ë¯¸ë§Œìœ¼ë¡œ ììœ¨ì£¼í–‰ í”„ë¡œí† íƒ€ì…ì„ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## í•µì‹¬ ì •ë¦¬

| ê°œë… | ì„¤ëª… |
|------|------|
| **ì—£ì§€ AI** | ë¡œì»¬ ë””ë°”ì´ìŠ¤ì—ì„œ ì‹¤í–‰ë˜ëŠ” AI, ì €ì§€ì—°/í”„ë¼ì´ë²„ì‹œ ì¥ì  |
| **Jetson** | NVIDIAì˜ ì—£ì§€ AI í”Œë«í¼, TensorRT ë„¤ì´í‹°ë¸Œ ì§€ì› |
| **ë¼ì¦ˆë² ë¦¬íŒŒì´** | ì €ë ´í•œ í”„ë¡œí† íƒ€ì´í•‘ í”Œë«í¼, AI HATë¡œ ì„±ëŠ¥ ë³´ì™„ |
| **TFLite** | Googleì˜ ê²½ëŸ‰ ì¶”ë¡  ëŸ°íƒ€ì„, ëª¨ë°”ì¼/ì„ë² ë””ë“œ ìµœì í™” |
| **CoreML** | Appleì˜ ML í”„ë ˆì„ì›Œí¬, Neural Engine í™œìš© |
| **DeepStream** | NVIDIAì˜ ë¹„ë””ì˜¤ ë¶„ì„ SDK, ë©€í‹° ìŠ¤íŠ¸ë¦¼ ì§€ì› |

## ë‹¤ìŒ ì„¹ì…˜ ë¯¸ë¦¬ë³´ê¸°

ëª¨ë¸ì„ ë°°í¬í–ˆë‹¤ë©´, ì´ì œ **ì§€ì†ì ì¸ ê´€ë¦¬**ê°€ í•„ìš”í•©ë‹ˆë‹¤. ë‹¤ìŒ ì„¹ì…˜ [CV MLOps](./04-mlops.md)ì—ì„œëŠ” í•™ìŠµ íŒŒì´í”„ë¼ì¸ ìë™í™”, ëª¨ë¸ ë²„ì „ ê´€ë¦¬, ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§, ë“œë¦¬í”„íŠ¸ ê°ì§€ ë“± **í”„ë¡œë•ì…˜ ML ì‹œìŠ¤í…œ ìš´ì˜** ë°©ë²•ì„ ë°°ì›ë‹ˆë‹¤.

## ì°¸ê³  ìë£Œ

- [NVIDIA Jetson in 2025](https://tannatechbiz.com/blog/post/nvidia-jetson) - ìµœì‹  Jetson ê°€ì´ë“œ
- [Getting Started with Edge AI on Jetson](https://developer.nvidia.com/blog/getting-started-with-edge-ai-on-nvidia-jetson-llms-vlms-and-foundation-models-for-robotics/) - NVIDIA ê³µì‹ íŠœí† ë¦¬ì–¼
- [How to Choose Edge AI Platform 2025](https://promwad.com/news/choose-edge-ai-platform-jetson-kria-coral-2025) - í”Œë«í¼ ë¹„êµ ê°€ì´ë“œ
- [Computer Vision on the Edge](https://www.amazon.com/Computer-Vision-Edge-Deploying-detection/dp/B0G4DP3ZHX) - ì—£ì§€ CV ë°°í¬ ì„œì 
- [Edge AI: Real-Time Inference](https://dev.to/vaib/edge-ai-revolutionizing-real-time-inference-on-resource-constrained-devices-58mf) - ì—£ì§€ AI ê°œìš”
