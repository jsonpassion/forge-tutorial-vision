# íŒŒì¸ íŠœë‹ ì „ëµ

> íš¨ê³¼ì ì¸ ëª¨ë¸ ë¯¸ì„¸ ì¡°ì •

## ê°œìš”

[ì „ì´ í•™ìŠµ](./03-transfer-learning.md)ì—ì„œ ì‚¬ì „ í•™ìŠµ ëª¨ë¸ì„ ìƒˆ ì‘ì—…ì— ì ìš©í•˜ëŠ” ë‘ ê°€ì§€ ë°©ì‹(íŠ¹ì§• ì¶”ì¶œ vs íŒŒì¸ íŠœë‹)ì„ ë°°ì› ìŠµë‹ˆë‹¤. ì´ë²ˆ ì„¹ì…˜ì—ì„œëŠ” íŒŒì¸ íŠœë‹ì„ **ì œëŒ€ë¡œ** í•˜ëŠ” ë°©ë²•ì— ì§‘ì¤‘í•©ë‹ˆë‹¤. ë ˆì´ì–´ë³„ í•™ìŠµë¥  ì°¨ë“± ì ìš©, ì ì§„ì  í•´ë™(Gradual Unfreezing), í•™ìŠµë¥  ì›Œë°ì—… ë“± **íŒŒì¸ íŠœë‹ì˜ ì„±ëŠ¥ì„ ê·¹ëŒ€í™”í•˜ëŠ” ê³ ê¸‰ í…Œí¬ë‹‰**ì„ ë‹¤ë£¹ë‹ˆë‹¤.

**ì„ ìˆ˜ ì§€ì‹**: [ì „ì´ í•™ìŠµ](./03-transfer-learning.md), [ì†ì‹¤ í•¨ìˆ˜ì™€ ì˜µí‹°ë§ˆì´ì €](../03-deep-learning-basics/04-loss-optimizer.md)
**í•™ìŠµ ëª©í‘œ**:
- íŒŒì¸ íŠœë‹ ì‹œ í•™ìŠµë¥  ì„¤ì • ì „ëµì„ ì´í•´í•˜ê³  ì ìš©í•  ìˆ˜ ìˆë‹¤
- ì ì§„ì  í•´ë™(Gradual Unfreezing)ì˜ ì›ë¦¬ì™€ íš¨ê³¼ë¥¼ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤
- ë ˆì´ì–´ë³„ ì°¨ë“± í•™ìŠµë¥ (Discriminative Learning Rate)ì„ êµ¬í˜„í•  ìˆ˜ ìˆë‹¤

## ì™œ ì•Œì•„ì•¼ í• ê¹Œ?

íŒŒì¸ íŠœë‹ì€ ë‹¨ìˆœíˆ `requires_grad = True`ë¡œ ì„¤ì •í•˜ê³  í•™ìŠµí•˜ë©´ ëë‚˜ëŠ” ê²Œ ì•„ë‹™ë‹ˆë‹¤. ì‚¬ì „ í•™ìŠµëœ ê°€ì¤‘ì¹˜ì—ëŠ” ìˆ˜ë°±ë§Œ ì¥ì˜ ì´ë¯¸ì§€ì—ì„œ ì–»ì€ ê·€ì¤‘í•œ ì§€ì‹ì´ ë‹´ê²¨ ìˆëŠ”ë°, ë„ˆë¬´ í° í•™ìŠµë¥ ë¡œ í•™ìŠµí•˜ë©´ ì´ ì§€ì‹ì´ **íŒŒê´´**ë©ë‹ˆë‹¤. ë°˜ëŒ€ë¡œ ë„ˆë¬´ ì¡°ì‹¬ìŠ¤ëŸ½ê²Œ í•˜ë©´ ìƒˆ ë°ì´í„°ì— **ì ì‘í•˜ì§€ ëª»í•˜ì£ **.

íŒŒì¸ íŠœë‹ì€ "ë³´ì¡´"ê³¼ "ì ì‘" ì‚¬ì´ì˜ ê· í˜•ì„ ë§ì¶”ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤. ì´ ê· í˜•ì„ ì˜ ì¡ëŠëƒì— ë”°ë¼ ê°™ì€ ëª¨ë¸, ê°™ì€ ë°ì´í„°ì—ì„œë„ **ì •í™•ë„ê°€ 2~5%** ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## í•µì‹¬ ê°œë…

### 1. íŒŒì¸ íŠœë‹ì˜ í•µì‹¬ ë”œë ˆë§ˆ â€” ë³´ì¡´ vs ì ì‘

> ğŸ’¡ **ë¹„ìœ **: ì™¸êµ­ì–´ë¥¼ ì˜í•˜ëŠ” í†µì—­ì‚¬(ì‚¬ì „ í•™ìŠµ ëª¨ë¸)ì—ê²Œ íŠ¹ì • ë¶„ì•¼ì˜ ì „ë¬¸ ìš©ì–´ë¥¼ ê°€ë¥´ì¹˜ëŠ” ìƒí™©ì„ ìƒê°í•´ë³´ì„¸ìš”. ë„ˆë¬´ ë¹¡ì„¸ê²Œ ì£¼ì…í•˜ë©´ ê¸°ë³¸ ì–¸ì–´ ì‹¤ë ¥ì´ ííŠ¸ëŸ¬ì§€ê³ (catastrophic forgetting), ë„ˆë¬´ ì‚´ì‚´ ê°€ë¥´ì¹˜ë©´ ì „ë¬¸ ìš©ì–´ë¥¼ ì œëŒ€ë¡œ ëª» ìµí™ë‹ˆë‹¤. **ê¸°ë³¸ê¸°ëŠ” ìœ ì§€í•˜ë©´ì„œ ì „ë¬¸ì„±ì„ ì¶”ê°€í•˜ëŠ”** ì ˆë¬˜í•œ ê· í˜•ì´ í•„ìš”í•˜ì£ .

ì´ ë”œë ˆë§ˆë¥¼ **ì¹˜ëª…ì  ë§ê°(Catastrophic Forgetting)**ì´ë¼ê³  í•©ë‹ˆë‹¤. ìƒˆ ì‘ì—…ì„ í•™ìŠµí•˜ë©´ì„œ ì´ì „ì— ë°°ìš´ ì§€ì‹ì„ ìŠì–´ë²„ë¦¬ëŠ” í˜„ìƒì´ì£ . íŒŒì¸ íŠœë‹ì˜ ëª¨ë“  í…Œí¬ë‹‰ì€ ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì¡´ì¬í•©ë‹ˆë‹¤.

### 2. ì „ëµ 1: ì‘ì€ í•™ìŠµë¥  ì‚¬ìš©

ê°€ì¥ ê¸°ë³¸ì ì´ë©´ì„œë„ ì¤‘ìš”í•œ ì „ëµì…ë‹ˆë‹¤. ì‚¬ì „ í•™ìŠµ ëª¨ë¸ì„ íŒŒì¸ íŠœë‹í•  ë•ŒëŠ” **ì²˜ìŒë¶€í„° í•™ìŠµí•  ë•Œë³´ë‹¤ 10~100ë°° ì‘ì€ í•™ìŠµë¥ **ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

| í•™ìŠµ ë°©ì‹ | ì¼ë°˜ì ì¸ í•™ìŠµë¥  |
|-----------|---------------|
| ì²˜ìŒë¶€í„° í•™ìŠµ (SGD) | 0.1 |
| íŒŒì¸ íŠœë‹ (SGD) | 0.001 ~ 0.01 |
| ì²˜ìŒë¶€í„° í•™ìŠµ (Adam) | 0.001 |
| íŒŒì¸ íŠœë‹ (Adam) | 0.00001 ~ 0.0001 |

ì™œ ì‘ì€ í•™ìŠµë¥ ì´ í•„ìš”í• ê¹Œìš”? ì‚¬ì „ í•™ìŠµëœ ê°€ì¤‘ì¹˜ëŠ” ì´ë¯¸ **ì¢‹ì€ ì§€ì ** ê·¼ì²˜ì— ìˆìŠµë‹ˆë‹¤. í° í•™ìŠµë¥ ë¡œ ì—…ë°ì´íŠ¸í•˜ë©´ ì´ ì¢‹ì€ ì§€ì ì—ì„œ **ë©€ë¦¬ íŠ•ê²¨ë‚˜ê°ˆ** ìˆ˜ ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.

### 3. ì „ëµ 2: ë ˆì´ì–´ë³„ ì°¨ë“± í•™ìŠµë¥  (Discriminative Learning Rate)

CNNì˜ ê° ë ˆì´ì–´ëŠ” ì„œë¡œ ë‹¤ë¥¸ ìˆ˜ì¤€ì˜ íŠ¹ì§•ì„ í•™ìŠµí•©ë‹ˆë‹¤. ì´ˆê¸° ë ˆì´ì–´(ê°€ì¥ìë¦¬, ì§ˆê°)ëŠ” ë²”ìš©ì ì´ë¯€ë¡œ ê±°ì˜ ê±´ë“œë¦¬ì§€ ì•Šê³ , í›„ê¸° ë ˆì´ì–´(ê°ì²´ ë¶€ë¶„, ì¡°í•©)ëŠ” ìƒˆ ë°ì´í„°ì— ë§ê²Œ ë” ë§ì´ ì¡°ì •í•˜ëŠ” ê²ƒì´ í•©ë¦¬ì ì…ë‹ˆë‹¤.

> ğŸ’¡ **ë¹„ìœ **: ì§‘ì„ ë¦¬ëª¨ë¸ë§í•  ë•Œ, ê¸°ì´ˆ ê³µì‚¬(ì´ˆê¸° ë ˆì´ì–´)ëŠ” ê·¸ëŒ€ë¡œ ë‘ê³ , ì¸í…Œë¦¬ì–´(ì¤‘ê°„ ë ˆì´ì–´)ëŠ” ì•½ê°„ ì†ë³´ê³ , ê°€êµ¬ì™€ ì†Œí’ˆ(í›„ê¸° ë ˆì´ì–´)ì€ ì™„ì „íˆ ë°”ê¾¸ëŠ” ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤. ëª¨ë“  ê²ƒì„ ë™ì¼í•œ ê°•ë„ë¡œ ë°”ê¿€ í•„ìš”ê°€ ì—†ì£ .

êµ¬ì²´ì ìœ¼ë¡œëŠ” ë ˆì´ì–´ ê·¸ë£¹ë³„ë¡œ **ë‹¤ë¥¸ í•™ìŠµë¥ **ì„ ì ìš©í•©ë‹ˆë‹¤:

| ë ˆì´ì–´ ê·¸ë£¹ | í•™ìŠµë¥  | ì´ìœ  |
|------------|--------|------|
| ì´ˆê¸° ë ˆì´ì–´ (conv1, layer1) | base_lr Ã— 0.01 | ë²”ìš© íŠ¹ì§• ë³´ì¡´ |
| ì¤‘ê°„ ë ˆì´ì–´ (layer2, layer3) | base_lr Ã— 0.1 | ì•½ê°„ì˜ ì¡°ì • |
| í›„ê¸° ë ˆì´ì–´ (layer4) | base_lr Ã— 1.0 | ì ê·¹ì  ì ì‘ |
| ìƒˆ ë¶„ë¥˜ ë ˆì´ì–´ (fc) | base_lr Ã— 10.0 | ì²˜ìŒë¶€í„° í•™ìŠµ |

```python
import torch.optim as optim
from torchvision import models
from torchvision.models import ResNet18_Weights
import torch.nn as nn

# ì‚¬ì „ í•™ìŠµ ResNet-18 ë¡œë“œ
model = models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)
model.fc = nn.Linear(model.fc.in_features, 10)  # 10í´ë˜ìŠ¤ë¡œ êµì²´

# === ë ˆì´ì–´ë³„ ì°¨ë“± í•™ìŠµë¥  ì„¤ì • ===
base_lr = 1e-3

param_groups = [
    # ì´ˆê¸° ë ˆì´ì–´: ë§¤ìš° ì‘ì€ í•™ìŠµë¥  (ë²”ìš© íŠ¹ì§• ë³´ì¡´)
    {'params': list(model.conv1.parameters()) +
               list(model.bn1.parameters()) +
               list(model.layer1.parameters()),
     'lr': base_lr * 0.01},

    # ì¤‘ê°„ ë ˆì´ì–´: ì‘ì€ í•™ìŠµë¥ 
    {'params': list(model.layer2.parameters()) +
               list(model.layer3.parameters()),
     'lr': base_lr * 0.1},

    # í›„ê¸° ë ˆì´ì–´: ê¸°ë³¸ í•™ìŠµë¥ 
    {'params': model.layer4.parameters(),
     'lr': base_lr},

    # ìƒˆ ë¶„ë¥˜ ë ˆì´ì–´: ê°€ì¥ í° í•™ìŠµë¥  (ì²˜ìŒë¶€í„° í•™ìŠµ)
    {'params': model.fc.parameters(),
     'lr': base_lr * 10},
]

optimizer = optim.Adam(param_groups, weight_decay=1e-4)

# ê° ê·¸ë£¹ì˜ í•™ìŠµë¥  í™•ì¸
for i, group in enumerate(optimizer.param_groups):
    num_params = sum(p.numel() for p in group['params'])
    print(f"ê·¸ë£¹ {i}: lr={group['lr']:.6f}, íŒŒë¼ë¯¸í„° ìˆ˜={num_params:,}")

# ê·¸ë£¹ 0: lr=0.000010, íŒŒë¼ë¯¸í„° ìˆ˜=296,896   (ì´ˆê¸°)
# ê·¸ë£¹ 1: lr=0.000100, íŒŒë¼ë¯¸í„° ìˆ˜=3,408,384  (ì¤‘ê°„)
# ê·¸ë£¹ 2: lr=0.001000, íŒŒë¼ë¯¸í„° ìˆ˜=7,079,424  (í›„ê¸°)
# ê·¸ë£¹ 3: lr=0.010000, íŒŒë¼ë¯¸í„° ìˆ˜=5,130      (ë¶„ë¥˜ê¸°)
```

### 4. ì „ëµ 3: ì ì§„ì  í•´ë™ (Gradual Unfreezing)

ì²˜ìŒì—ëŠ” ë¶„ë¥˜ ë ˆì´ì–´ë§Œ í•™ìŠµí•˜ê³ , ì¼ì • ì—í¬í¬ë§ˆë‹¤ **ì•„ë˜ìª½ ë ˆì´ì–´ë¥¼ í•˜ë‚˜ì”© í•´ë™**í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. 2018ë…„ ì œë ˆë¯¸ í•˜ì›Œë“œ(Jeremy Howard)ì™€ ì„¸ë°”ìŠ¤ì°¬ ë£¨ë”(Sebastian Ruder)ê°€ **ULMFiT** ë…¼ë¬¸ì—ì„œ ì œì•ˆí•œ ê¸°ë²•ìœ¼ë¡œ, NLPì—ì„œ ë¨¼ì € ì„±ê³µì„ ê±°ë’€ì§€ë§Œ ë¹„ì „ì—ì„œë„ íš¨ê³¼ì ì…ë‹ˆë‹¤.

> ğŸ’¡ **ë¹„ìœ **: ìƒˆ íŒ€ì— í•©ë¥˜í•œ ì‹ ì…ì‚¬ì›(ìƒˆ ë¶„ë¥˜ ë ˆì´ì–´)ì´ ë¨¼ì € ì—…ë¬´ì— ì ì‘í•˜ê³ , ê·¸ ë‹¤ìŒ ê¸°ì¡´ íŒ€ì›ë“¤(í›„ê¸° ë ˆì´ì–´)ì´ ì¡°ê¸ˆì”© ì‘ì—… ë°©ì‹ì„ ì¡°ì •í•˜ê³ , ë§ˆì§€ë§‰ìœ¼ë¡œ íŒ€ ì „ì²´(ì „ì²´ ëª¨ë¸)ê°€ ìƒˆ í”„ë¡œì íŠ¸ì— ë§ì¶°ê°€ëŠ” ê³¼ì •ê³¼ ê°™ìŠµë‹ˆë‹¤.

```python
import torch
import torch.nn as nn
from torchvision import models
from torchvision.models import ResNet18_Weights

DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

def gradual_unfreeze_demo():
    """ì ì§„ì  í•´ë™ ì „ëµ êµ¬í˜„"""
    model = models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)
    model.fc = nn.Linear(model.fc.in_features, 10)
    model = model.to(DEVICE)

    # Step 1: ëª¨ë“  ë ˆì´ì–´ ê³ ì •
    for param in model.parameters():
        param.requires_grad = False

    # Step 2: ë¶„ë¥˜ ë ˆì´ì–´ë§Œ í•´ë™
    for param in model.fc.parameters():
        param.requires_grad = True

    # ë ˆì´ì–´ ê·¸ë£¹ ì •ì˜ (ì•„ë˜ì—ì„œ ìœ„ë¡œ í•´ë™í•  ìˆœì„œ)
    layer_groups = [
        ('fc', model.fc),
        ('layer4', model.layer4),
        ('layer3', model.layer3),
        ('layer2', model.layer2),
        ('layer1', model.layer1),
    ]

    def unfreeze_group(group_name, group_module):
        """íŠ¹ì • ë ˆì´ì–´ ê·¸ë£¹ì˜ í•™ìŠµì„ í™œì„±í™”"""
        for param in group_module.parameters():
            param.requires_grad = True
        count = sum(p.numel() for p in group_module.parameters() if p.requires_grad)
        print(f"  [{group_name}] í•´ë™ ì™„ë£Œ â€” {count:,}ê°œ íŒŒë¼ë¯¸í„° í•™ìŠµ ê°€ëŠ¥")

    def count_trainable():
        return sum(p.numel() for p in model.parameters() if p.requires_grad)

    # === ì ì§„ì  í•´ë™ ìŠ¤ì¼€ì¤„ ===
    EPOCHS = 15

    for epoch in range(1, EPOCHS + 1):
        # ì—í¬í¬ì— ë”°ë¼ ë ˆì´ì–´ í•´ë™
        if epoch == 1:
            print(f"\n[Epoch {epoch}] ë¶„ë¥˜ ë ˆì´ì–´ë§Œ í•™ìŠµ")
            # fcëŠ” ì´ë¯¸ í•´ë™ë¨
        elif epoch == 4:
            print(f"\n[Epoch {epoch}] layer4 í•´ë™")
            unfreeze_group('layer4', model.layer4)
        elif epoch == 7:
            print(f"\n[Epoch {epoch}] layer3 í•´ë™")
            unfreeze_group('layer3', model.layer3)
        elif epoch == 10:
            print(f"\n[Epoch {epoch}] layer2 í•´ë™")
            unfreeze_group('layer2', model.layer2)

        trainable = count_trainable()
        total = sum(p.numel() for p in model.parameters())
        # ì—¬ê¸°ì„œ ì‹¤ì œ í•™ìŠµ ë£¨í”„ ì‹¤í–‰
        # train_one_epoch(model, train_loader, criterion, optimizer, DEVICE)
        print(f"  Epoch {epoch}: í•™ìŠµ ê°€ëŠ¥ {trainable:,}/{total:,} "
              f"({100*trainable/total:.1f}%)")

gradual_unfreeze_demo()

# ì¶œë ¥:
# [Epoch 1] ë¶„ë¥˜ ë ˆì´ì–´ë§Œ í•™ìŠµ
#   Epoch 1: í•™ìŠµ ê°€ëŠ¥ 5,130/11,181,642 (0.0%)
#   ...
# [Epoch 4] layer4 í•´ë™
#   [layer4] í•´ë™ ì™„ë£Œ â€” 7,079,424ê°œ íŒŒë¼ë¯¸í„° í•™ìŠµ ê°€ëŠ¥
#   Epoch 4: í•™ìŠµ ê°€ëŠ¥ 7,084,554/11,181,642 (63.4%)
#   ...
# [Epoch 7] layer3 í•´ë™
#   [layer3] í•´ë™ ì™„ë£Œ â€” 2,359,808ê°œ íŒŒë¼ë¯¸í„° í•™ìŠµ ê°€ëŠ¥
#   Epoch 7: í•™ìŠµ ê°€ëŠ¥ 9,444,362/11,181,642 (84.5%)
```

### 5. ì „ëµ 4: í•™ìŠµë¥  ì›Œë°ì—… (Learning Rate Warmup)

íŒŒì¸ íŠœë‹ ì‹œì‘ ì‹œ **ë§¤ìš° ì‘ì€ í•™ìŠµë¥ ì—ì„œ ì‹œì‘í•´ì„œ ì„œì„œíˆ ì˜¬ë¦¬ëŠ”** ê¸°ë²•ì…ë‹ˆë‹¤. ì´ˆê¸°ì— í° í•™ìŠµë¥ ë¡œ ì‹œì‘í•˜ë©´ ì‚¬ì „ í•™ìŠµ ê°€ì¤‘ì¹˜ê°€ ê¸‰ê²©íˆ ë³€í•´ì„œ ì¢‹ì€ íŠ¹ì§•ì„ ìƒì„ ìˆ˜ ìˆê±°ë“ ìš”.

```python
import torch.optim as optim

# ì›Œë°ì—… + ì½”ì‚¬ì¸ ê°ì†Œ ìŠ¤ì¼€ì¤„ëŸ¬ ì¡°í•©
def create_warmup_cosine_scheduler(optimizer, warmup_epochs, total_epochs):
    """ì›Œë°ì—… í›„ ì½”ì‚¬ì¸ ê°ì†Œí•˜ëŠ” ìŠ¤ì¼€ì¤„ëŸ¬"""
    def lr_lambda(epoch):
        if epoch < warmup_epochs:
            # ì›Œë°ì—…: 0ì—ì„œ 1ê¹Œì§€ ì„ í˜• ì¦ê°€
            return (epoch + 1) / warmup_epochs
        else:
            # ì½”ì‚¬ì¸ ê°ì†Œ: 1ì—ì„œ 0ê¹Œì§€
            import math
            progress = (epoch - warmup_epochs) / (total_epochs - warmup_epochs)
            return 0.5 * (1 + math.cos(math.pi * progress))
    return optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)

# ì‚¬ìš© ì˜ˆì‹œ
optimizer = optim.Adam(model.parameters(), lr=1e-3)
scheduler = create_warmup_cosine_scheduler(optimizer,
                                            warmup_epochs=3,
                                            total_epochs=20)

# í•™ìŠµë¥  ë³€í™” í™•ì¸
for epoch in range(20):
    lr = optimizer.param_groups[0]['lr']
    if epoch < 5 or epoch >= 17:  # ì²˜ìŒê³¼ ë ëª‡ ê°œë§Œ ì¶œë ¥
        print(f"Epoch {epoch:2d}: lr = {lr:.6f}")
    scheduler.step()

# Epoch  0: lr = 0.000333  (ì›Œë°ì—… 1/3)
# Epoch  1: lr = 0.000667  (ì›Œë°ì—… 2/3)
# Epoch  2: lr = 0.001000  (ì›Œë°ì—… ì™„ë£Œ, ìµœëŒ€ í•™ìŠµë¥ )
# Epoch  3: lr = 0.000972  (ì½”ì‚¬ì¸ ê°ì†Œ ì‹œì‘)
# Epoch  4: lr = 0.000890
# ...
# Epoch 17: lr = 0.000110
# Epoch 18: lr = 0.000028
# Epoch 19: lr = 0.000000
```

í•™ìŠµë¥ ì´ ì›Œë°ì—… 3 ì—í¬í¬ ë™ì•ˆ 0ì—ì„œ ìµœëŒ€ê°’ê¹Œì§€ ì˜¬ë¼ê°„ ë’¤, ì½”ì‚¬ì¸ ê³¡ì„ ì„ ë”°ë¼ ë¶€ë“œëŸ½ê²Œ ê°ì†Œí•©ë‹ˆë‹¤.

### 6. ì „ëµ ì¢…í•© â€” ì‹¤ì „ íŒŒì¸ íŠœë‹ ë ˆì‹œí”¼

ì§€ê¸ˆê¹Œì§€ ë°°ìš´ ì „ëµì„ ì¡°í•©í•œ **ì‹¤ì „ íŒŒì¸ íŠœë‹ ì½”ë“œ**ì…ë‹ˆë‹¤:

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms, models
from torchvision.models import ResNet18_Weights
import math

DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
BATCH_SIZE = 64
EPOCHS = 20
WARMUP_EPOCHS = 3
BASE_LR = 1e-3

# === ë°ì´í„° ì¤€ë¹„ (ImageNet ì „ì²˜ë¦¬) ===
train_transform = transforms.Compose([
    transforms.Resize(224),
    transforms.RandomCrop(224, padding=8),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
test_transform = transforms.Compose([
    transforms.Resize(224),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

train_dataset = datasets.CIFAR10('./data', train=True, download=True,
                                  transform=train_transform)
test_dataset = datasets.CIFAR10('./data', train=False, download=True,
                                 transform=test_transform)
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,
                          shuffle=True, num_workers=2)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,
                         shuffle=False, num_workers=2)

# === ëª¨ë¸ + ì°¨ë“± í•™ìŠµë¥  ===
model = models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)
model.fc = nn.Linear(model.fc.in_features, 10)
model = model.to(DEVICE)

param_groups = [
    {'params': list(model.conv1.parameters()) +
               list(model.bn1.parameters()) +
               list(model.layer1.parameters()),
     'lr': BASE_LR * 0.01, 'name': 'early'},
    {'params': list(model.layer2.parameters()) +
               list(model.layer3.parameters()),
     'lr': BASE_LR * 0.1, 'name': 'middle'},
    {'params': model.layer4.parameters(),
     'lr': BASE_LR, 'name': 'late'},
    {'params': model.fc.parameters(),
     'lr': BASE_LR * 10, 'name': 'head'},
]

optimizer = optim.AdamW(param_groups, weight_decay=1e-2)

# === ì›Œë°ì—… + ì½”ì‚¬ì¸ ìŠ¤ì¼€ì¤„ëŸ¬ ===
def lr_lambda(epoch):
    if epoch < WARMUP_EPOCHS:
        return (epoch + 1) / WARMUP_EPOCHS
    progress = (epoch - WARMUP_EPOCHS) / (EPOCHS - WARMUP_EPOCHS)
    return 0.5 * (1 + math.cos(math.pi * progress))

scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)
criterion = nn.CrossEntropyLoss()

# === í•™ìŠµ ë° í‰ê°€ í•¨ìˆ˜ ===
def train_one_epoch(model, loader, criterion, optimizer, device):
    model.train()
    total_loss, correct, total = 0, 0, 0
    for images, labels in loader:
        images, labels = images.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        total_loss += loss.item() * images.size(0)
        _, predicted = outputs.max(1)
        correct += predicted.eq(labels).sum().item()
        total += labels.size(0)
    return total_loss / total, 100. * correct / total

def evaluate(model, loader, criterion, device):
    model.eval()
    total_loss, correct, total = 0, 0, 0
    with torch.no_grad():
        for images, labels in loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            loss = criterion(outputs, labels)
            total_loss += loss.item() * images.size(0)
            _, predicted = outputs.max(1)
            correct += predicted.eq(labels).sum().item()
            total += labels.size(0)
    return total_loss / total, 100. * correct / total

# === í•™ìŠµ ì‹¤í–‰ ===
best_acc = 0
for epoch in range(1, EPOCHS + 1):
    train_loss, train_acc = train_one_epoch(
        model, train_loader, criterion, optimizer, DEVICE)
    test_loss, test_acc = evaluate(model, test_loader, criterion, DEVICE)
    scheduler.step()

    if test_acc > best_acc:
        best_acc = test_acc
        torch.save(model.state_dict(), 'best_finetuned.pth')

    if epoch % 5 == 0 or epoch == 1:
        lrs = [f"{g['name']}={g['lr']*lr_lambda(epoch-1):.6f}"
               for g in param_groups]
        print(f"Epoch {epoch:2d} | Train: {train_acc:.1f}% | "
              f"Test: {test_acc:.1f}% | LR: {', '.join(lrs)}")

print(f"\nìµœê³  í…ŒìŠ¤íŠ¸ ì •í™•ë„: {best_acc:.2f}%")

# ì˜ˆìƒ ì¶œë ¥:
# Epoch  1 | Train: 85.2% | Test: 88.4% | LR: early=0.000003, ...
# Epoch  5 | Train: 95.1% | Test: 94.3% | LR: early=0.000009, ...
# Epoch 10 | Train: 97.8% | Test: 95.2% | LR: early=0.000005, ...
# Epoch 15 | Train: 98.9% | Test: 95.6% | LR: early=0.000002, ...
# Epoch 20 | Train: 99.3% | Test: 95.8% | LR: early=0.000000, ...
# ìµœê³  í…ŒìŠ¤íŠ¸ ì •í™•ë„: 95.80%
```

## ë” ê¹Šì´ ì•Œì•„ë³´ê¸°

### ULMFiT â€” íŒŒì¸ íŠœë‹ ì „ëµì˜ í˜ëª…

í˜„ëŒ€ íŒŒì¸ íŠœë‹ ì „ëµì˜ ë§ì€ ë¶€ë¶„ì€ 2018ë…„ **ì œë ˆë¯¸ í•˜ì›Œë“œ(Jeremy Howard)**ì™€ **ì„¸ë°”ìŠ¤ì°¬ ë£¨ë”(Sebastian Ruder)**ì˜ **ULMFiT(Universal Language Model Fine-tuning)** ë…¼ë¬¸ì—ì„œ ë¹„ë¡¯ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ ë…¼ë¬¸ì€ NLPë¥¼ ëŒ€ìƒìœ¼ë¡œ í–ˆì§€ë§Œ, ì œì•ˆëœ ì„¸ ê°€ì§€ í•µì‹¬ ê¸°ë²• â€” ì°¨ë“± í•™ìŠµë¥ , ì ì§„ì  í•´ë™, ì‚¼ê° í•™ìŠµë¥ (Slanted Triangular LR) â€” ì€ ì»´í“¨í„° ë¹„ì „ì—ì„œë„ ê·¸ëŒ€ë¡œ ì ìš©ë©ë‹ˆë‹¤.

í¥ë¯¸ë¡œìš´ ì ì€, í•˜ì›Œë“œê°€ ì´ ì—°êµ¬ë¥¼ í•  ë•Œ ëŒ€í˜• ì—°êµ¬ì†Œê°€ ì•„ë‹Œ **fast.ai**ë¼ëŠ” ì‘ì€ êµìœ¡ ì¡°ì§ì—ì„œ ì§„í–‰í–ˆë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ê±°ëŒ€ ìë³¸ì´ ì•„ë‹Œ **ì˜ë¦¬í•œ ì „ëµ**ìœ¼ë¡œ ì„±ëŠ¥ì„ ëŒì–´ì˜¬ë¦° ì‚¬ë¡€ì´ì£ . ì´ ë…¼ë¬¸ì€ ì´í›„ BERT, GPT ì‹œëŒ€ì˜ "ì‚¬ì „ í•™ìŠµ â†’ íŒŒì¸ íŠœë‹" íŒ¨ëŸ¬ë‹¤ì„ì— í° ì˜í–¥ì„ ë¯¸ì³¤ìŠµë‹ˆë‹¤.

> ğŸ’¡ **ì•Œê³  ê³„ì…¨ë‚˜ìš”?**: fast.ai ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œëŠ” `learn.fine_tune(epochs)` í•œ ì¤„ë¡œ ì ì§„ì  í•´ë™ + ì°¨ë“± í•™ìŠµë¥  + ì›Œë°ì—…ì´ ìë™ ì ìš©ë©ë‹ˆë‹¤. í•˜ì›Œë“œê°€ ë…¼ë¬¸ì—ì„œ ì œì•ˆí•œ ì „ëµì„ ê¸°ë³¸ê°’ìœ¼ë¡œ ë‚´ì¥í•œ ê²ƒì´ì£ . ì´ "í•©ë¦¬ì ì¸ ê¸°ë³¸ê°’(sensible defaults)" ì² í•™ì€ ì‹¤ë¬´ìë“¤ ì‚¬ì´ì—ì„œ í° í˜¸ì‘ì„ ë°›ì•˜ìŠµë‹ˆë‹¤.

### íŒŒì¸ íŠœë‹ ì²´í¬ë¦¬ìŠ¤íŠ¸

ì‹¤ë¬´ì—ì„œ íŒŒì¸ íŠœë‹ì„ í•  ë•Œ ìˆœì„œëŒ€ë¡œ ë”°ë¼ê°€ë©´ ì¢‹ì€ ì²´í¬ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤:

1. **ì‚¬ì „ í•™ìŠµ ëª¨ë¸ ì„ íƒ**: ResNet-50ì´ ë²”ìš©ì  ì¶œë°œì 
2. **ë¶„ë¥˜ ë ˆì´ì–´ êµì²´**: `model.fc = nn.Linear(in_features, num_classes)`
3. **ImageNet ì „ì²˜ë¦¬ ì ìš©**: mean, std, ì…ë ¥ í¬ê¸°(224)
4. **ì‘ì€ í•™ìŠµë¥  ì‚¬ìš©**: ì²˜ìŒë¶€í„° í•™ìŠµì˜ 1/10 ~ 1/100
5. **ì°¨ë“± í•™ìŠµë¥  ì ìš©**: ì´ˆê¸° ë ˆì´ì–´ < í›„ê¸° ë ˆì´ì–´ < ìƒˆ ë ˆì´ì–´
6. **ì›Œë°ì—… ì‚¬ìš©**: 3~5 ì—í¬í¬ ì›Œë°ì—…ìœ¼ë¡œ ì•ˆì •ì  ì‹œì‘
7. **Weight Decay ì ìš©**: AdamW + weight_decay=1e-2
8. **ëª¨ë‹ˆí„°ë§**: Train-Test ê°­ì´ ë²Œì–´ì§€ë©´ ì •ê·œí™” ê°•í™”

## í”í•œ ì˜¤í•´ì™€ íŒ

> âš ï¸ **í”í•œ ì˜¤í•´**: "íŒŒì¸ íŠœë‹ì€ ë¬´ì¡°ê±´ ì „ì²´ ë ˆì´ì–´ë¥¼ í•™ìŠµí•´ì•¼ í•œë‹¤" â€” ì˜¤íˆë ¤ ë°ì´í„°ê°€ ì ì„ ë•Œ ì „ì²´ë¥¼ íŒŒì¸ íŠœë‹í•˜ë©´ ê³¼ì í•©ì´ ì‹¬í•´ì§‘ë‹ˆë‹¤. í›„ê¸° ë ˆì´ì–´ ëª‡ ê°œë§Œ íŒŒì¸ íŠœë‹í•˜ëŠ” ê²ƒì´ ë” ë‚˜ì€ ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤.

> âš ï¸ **í”í•œ ì˜¤í•´**: "íŒŒì¸ íŠœë‹ í•™ìŠµë¥ ì€ ê³ ì •í•´ì•¼ í•œë‹¤" â€” ì›Œë°ì—…ê³¼ ìŠ¤ì¼€ì¤„ë§ì„ ì ìš©í•˜ë©´ ê°™ì€ ì—í¬í¬ì—ì„œë„ ì„±ëŠ¥ì´ í¬ê²Œ í–¥ìƒë©ë‹ˆë‹¤. íŠ¹íˆ ì›Œë°ì—…ì€ ì´ˆê¸° í•™ìŠµ ë¶ˆì•ˆì •ì„±ì„ í¬ê²Œ ì¤„ì—¬ì¤ë‹ˆë‹¤.

> ğŸ”¥ **ì‹¤ë¬´ íŒ**: íŒŒì¸ íŠœë‹ ì‹œ **AdamW**ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”. ì¼ë°˜ Adamì˜ weight decayëŠ” ê°€ì¤‘ì¹˜ ì •ê·œí™”ê°€ ì œëŒ€ë¡œ ë™ì‘í•˜ì§€ ì•ŠëŠ” ë°˜ë©´, AdamWëŠ” ì´ë¥¼ ì˜¬ë°”ë¥´ê²Œ êµ¬í˜„í•©ë‹ˆë‹¤. ì´ ì°¨ì´ê°€ í…ŒìŠ¤íŠ¸ ì •í™•ë„ì—ì„œ 0.5~1% ì°¨ì´ë¥¼ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

> ğŸ”¥ **ì‹¤ë¬´ íŒ**: íŒŒì¸ íŠœë‹ì´ ì˜ ì•ˆ ë˜ë©´, **ë¨¼ì € íŠ¹ì§• ì¶”ì¶œê¸°ë¡œ 5 ì—í¬í¬ â†’ ê·¸ ë‹¤ìŒ ì ì§„ì  í•´ë™**ìœ¼ë¡œ ì „í™˜í•´ë³´ì„¸ìš”. ë¶„ë¥˜ ë ˆì´ì–´ê°€ ì–´ëŠ ì •ë„ ìˆ˜ë ´í•œ í›„ì— í•˜ìœ„ ë ˆì´ì–´ë¥¼ ê±´ë“œë¦¬ëŠ” ê²ƒì´ ì•ˆì •ì ì…ë‹ˆë‹¤.

## í•µì‹¬ ì •ë¦¬

| ì „ëµ | ì„¤ëª… | íš¨ê³¼ |
|------|------|------|
| ì‘ì€ í•™ìŠµë¥  | ì²˜ìŒë¶€í„° í•™ìŠµì˜ 1/10~1/100 ì‚¬ìš© | ì‚¬ì „ í•™ìŠµ ì§€ì‹ ë³´ì¡´ |
| ì°¨ë“± í•™ìŠµë¥  | ì´ˆê¸° ë ˆì´ì–´ < í›„ê¸° ë ˆì´ì–´ < ìƒˆ ë ˆì´ì–´ | ë ˆì´ì–´ë³„ ìµœì  ì ì‘ |
| ì ì§„ì  í•´ë™ | ë¶„ë¥˜ê¸°ë¶€í„° ì‹œì‘, ì ì°¨ í•˜ìœ„ ë ˆì´ì–´ í•´ë™ | ì•ˆì •ì  í•™ìŠµ, ë§ê° ë°©ì§€ |
| í•™ìŠµë¥  ì›Œë°ì—… | ì‘ì€ ê°’ì—ì„œ ì‹œì‘í•´ ì„œì„œíˆ ì¦ê°€ | ì´ˆê¸° í•™ìŠµ ì•ˆì •í™” |
| AdamW | weight decayë¥¼ ì˜¬ë°”ë¥´ê²Œ ì ìš©í•˜ëŠ” ì˜µí‹°ë§ˆì´ì € | ë” ë‚˜ì€ ì¼ë°˜í™” |
| ì¹˜ëª…ì  ë§ê° | ìƒˆ í•™ìŠµì´ ì´ì „ ì§€ì‹ì„ íŒŒê´´í•˜ëŠ” í˜„ìƒ | íŒŒì¸ íŠœë‹ì˜ í•µì‹¬ ë„ì „ ê³¼ì œ |

## ë‹¤ìŒ ì„¹ì…˜ ë¯¸ë¦¬ë³´ê¸°

íŒŒì¸ íŠœë‹ìœ¼ë¡œ ëª¨ë¸ì„ ìµœì í™”í•˜ëŠ” ë°©ë²•ì„ ë°°ì› ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ëª¨ë¸ë§Œí¼ ì¤‘ìš”í•œ ê²ƒì´ **ë°ì´í„°**ì…ë‹ˆë‹¤. [ë°ì´í„° ì¦ê°•](./05-data-augmentation.md)ì—ì„œëŠ” ê¸°ì¡´ ë°ì´í„°ë¥¼ ë³€í˜•í•˜ì—¬ í•™ìŠµ ë°ì´í„°ì˜ ë‹¤ì–‘ì„±ì„ ë†’ì´ëŠ” ë‹¤ì–‘í•œ ê¸°ë²• â€” Albumentations, RandAugment, MixUp, CutMix ë“± â€” ì„ ë‹¤ë£¨ë©°, ì´ ê¸°ë²•ë“¤ì´ ëª¨ë¸ ì„±ëŠ¥ì— ì–´ë–¤ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ì§€ ì‹¤í—˜í•©ë‹ˆë‹¤.

## ì°¸ê³  ìë£Œ

- [PyTorch ê³µì‹ ì „ì´ í•™ìŠµ íŠœí† ë¦¬ì–¼](https://docs.pytorch.org/tutorials/beginner/transfer_learning_tutorial.html) - íŠ¹ì§• ì¶”ì¶œ vs íŒŒì¸ íŠœë‹ ì‹¤ìŠµ ê°€ì´ë“œ
- [ULMFiT: Universal Language Model Fine-tuning (Howard & Ruder, 2018)](https://arxiv.org/abs/1801.06146) - ì°¨ë“± í•™ìŠµë¥ ê³¼ ì ì§„ì  í•´ë™ì„ ì œì•ˆí•œ í•µì‹¬ ë…¼ë¬¸
- [Ultimate Guide to Fine-Tuning in PyTorch](https://rumn.medium.com/part-1-ultimate-guide-to-fine-tuning-in-pytorch-pre-trained-model-and-its-configuration-8990194b71e) - ì‹¤ì „ íŒŒì¸ íŠœë‹ ì¢…í•© ê°€ì´ë“œ
- [Discriminative Fine-Tuning: A Comprehensive Guide](https://www.shadecoder.com/topics/discriminative-fine-tuning-a-comprehensive-guide-for-2025) - ì°¨ë“± í•™ìŠµë¥ ì˜ ì´ë¡ ê³¼ ì‹¤ì „
- [Advanced Techniques for Fine-tuning Transformers](https://towardsdatascience.com/advanced-techniques-for-fine-tuning-transformers-82e4e61e16e/) - Transformer ì‹œëŒ€ì˜ íŒŒì¸ íŠœë‹ ê³ ê¸‰ ê¸°ë²•
