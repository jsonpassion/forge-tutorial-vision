# 생성 모델 개론

> 판별 vs 생성 모델

## 개요

지금까지 우리는 이미지를 **분류**하고, **탐지**하고, **분할**하는 방법을 배웠습니다. 이 모든 작업의 공통점이 뭘까요? 바로 "이미 있는 이미지를 이해하는 것"이죠. 이번 챕터부터는 완전히 다른 질문을 던집니다 — **"이미지를 만들 수 있을까?"**

**선수 지식**: [신경망의 구조](../03-deep-learning-basics/01-neural-network.md), [손실 함수와 옵티마이저](../03-deep-learning-basics/04-loss-optimizer.md)
**학습 목표**:
- 판별 모델과 생성 모델의 근본적인 차이를 이해한다
- 생성 모델의 주요 분류(VAE, GAN, Diffusion 등)를 파악한다
- 확률 분포 관점에서 생성 모델이 무엇을 학습하는지 직관적으로 이해한다

## 왜 알아야 할까?

생성 모델은 현대 AI의 가장 뜨거운 영역입니다. Stable Diffusion으로 이미지를 그리고, ChatGPT로 글을 쓰고, Sora로 영상을 만드는 시대가 왔죠. 하지만 생성 모델의 원리를 이해하지 못하면, 이 도구들을 "마법의 블랙박스"로만 쓸 수밖에 없습니다.

앞서 [이미지 분류](../06-image-classification/01-mnist.md)에서 "이 숫자가 3인지 7인지" 맞추는 모델을 만들었죠? 생성 모델은 그 반대 — "3처럼 보이는 숫자를 새로 그려봐"라고 하는 겁니다. 이 차이를 이해하는 것이 이번 챕터의 출발점입니다.

## 핵심 개념

### 개념 1: 판별 모델 vs 생성 모델 — 감정사와 화가

> 💡 **비유**: 미술관에 감정사(판별 모델)와 화가(생성 모델)가 있다고 상상해보세요. 감정사는 그림을 보고 "이건 모네의 작품입니다"라고 판별합니다. 화가는 "모네 스타일로 새 그림을 그려달라"는 주문을 받고 실제로 그림을 만들어냅니다. 둘 다 모네의 화풍을 이해해야 하지만, 감정사는 **구별**하는 능력이, 화가는 **창조**하는 능력이 핵심이죠.

**판별 모델(Discriminative Model)**은 입력 데이터가 주어졌을 때, 그것이 어떤 클래스에 속하는지 예측합니다. 수학적으로는 조건부 확률 $P(Y|X)$를 학습하는 거죠. 우리가 지금까지 배운 대부분의 모델이 여기에 해당합니다:

- [이미지 분류](../06-image-classification/01-mnist.md) → "이 이미지는 고양이다"
- [객체 탐지](../07-object-detection/01-detection-basics.md) → "여기에 자동차가 있다"
- [세그멘테이션](../08-segmentation/01-semantic-segmentation.md) → "이 픽셀은 도로다"

**생성 모델(Generative Model)**은 데이터의 분포 자체, 즉 $P(X)$를 학습합니다. "고양이 사진들은 이런 패턴을 가지고 있구나"를 이해한 다음, 그 패턴에 맞는 새로운 고양이 사진을 만들어내는 거죠.

| 구분 | 판별 모델 | 생성 모델 |
|------|----------|----------|
| **학습 목표** | $P(Y \mid X)$ — 클래스 구별 | $P(X)$ — 데이터 분포 자체 |
| **비유** | 감정사, 시험 채점관 | 화가, 작곡가 |
| **입력 → 출력** | 이미지 → 레이블 | 랜덤 노이즈/조건 → 이미지 |
| **대표 모델** | ResNet, YOLO, DETR | VAE, GAN, Diffusion |
| **난이도** | 상대적으로 쉬움 | 매우 어려움 |

왜 생성이 더 어려울까요? 감정사는 "이건 모네가 아니다"라고만 말하면 되지만, 화가는 모네의 붓터치, 색감, 구도를 **완벽하게 재현**해야 하니까요. 데이터의 전체 분포를 이해하는 것은, 경계선만 긋는 것보다 훨씬 복잡한 일입니다.

### 개념 2: 생성 모델이 풀어야 할 문제

> 💡 **비유**: 생성 모델의 학습은 마치 **외계인이 지구의 요리를 배우는 과정**과 비슷합니다. 외계인은 수천 가지 한식을 맛보며 "아, 이 행성의 음식은 이런 맛과 향의 패턴을 가지고 있구나"를 파악합니다. 충분히 학습한 후에는 한 번도 먹어본 적 없는 새로운 한식 메뉴를 창조할 수 있죠.

생성 모델이 학습하는 것은 결국 **확률 분포**입니다. 예를 들어 사람 얼굴 이미지의 데이터셋이 있다면:

- 눈은 보통 얼굴 중간쯤에 위치한다
- 피부색은 다양하지만 일정한 범위 안에 있다
- 대칭적인 구조를 가진다

이런 패턴들의 총합이 바로 "사람 얼굴 이미지의 확률 분포"입니다. 생성 모델은 이 분포를 학습한 뒤, 분포에서 **샘플링(Sampling)**하여 새로운 이미지를 만듭니다.

핵심 수식으로 표현하면:

$$p_\theta(x) \approx p_{data}(x)$$

> 여기서 $p_{data}(x)$는 실제 데이터의 분포, $p_\theta(x)$는 모델이 학습한 분포입니다. 파라미터 $\theta$를 조정하여 두 분포를 최대한 가깝게 만드는 것이 목표죠.

### 개념 3: 생성 모델의 세 가지 큰 흐름

컴퓨터 비전에서 생성 모델은 크게 세 가지 접근법으로 나뉩니다. 각각의 전략이 완전히 다르다는 점이 재미있어요.

**1. VAE (Variational Autoencoder) — 압축과 복원의 달인**

> 이미지를 **작은 코드로 압축**했다가 다시 복원하는 방식입니다. 이 "작은 코드"(잠재 벡터)를 살짝 변형하면 새로운 이미지가 탄생하죠. 마치 레시피를 핵심 재료 목록으로 요약했다가, 재료 비율을 살짝 바꿔 새 요리를 만드는 것과 같습니다.

**2. GAN (Generative Adversarial Network) — 위조범과 감정사의 대결**

> **생성자(위조범)**와 **판별자(감정사)**가 서로 경쟁하며 실력을 높여가는 방식입니다. 생성자는 점점 더 정교한 가짜를 만들고, 판별자는 점점 더 미세한 차이를 잡아냅니다. 이 경쟁이 균형에 도달하면, 진짜와 구별할 수 없는 이미지가 만들어지죠.

**3. Diffusion Model — 노이즈에서 예술로**

> 깨끗한 이미지에 **점진적으로 노이즈를 추가**하는 과정을 학습한 뒤, 그 **역과정**(노이즈 제거)을 수행하여 이미지를 생성합니다. 마치 모래바람에 묻힌 조각상을 한 겹 한 겹 복원하는 것과 같은 원리죠.

| 접근법 | 핵심 아이디어 | 장점 | 단점 | 대표 모델 |
|--------|-------------|------|------|----------|
| **VAE** | 압축 → 잠재 공간 → 복원 | 학습 안정적, 잠재 공간 해석 가능 | 이미지가 흐릿한 경향 | VAE, VQ-VAE |
| **GAN** | 생성자 vs 판별자 경쟁 | 매우 선명한 이미지 | 학습 불안정, 모드 붕괴 | StyleGAN, CycleGAN |
| **Diffusion** | 노이즈 추가/제거 반복 | 최고 품질, 학습 안정 | 생성 속도 느림 | DDPM, Stable Diffusion |

> ⚠️ **흔한 오해**: "GAN이 Diffusion에 밀려서 사라졌다"고 생각하는 분이 있는데, 사실이 아닙니다. StyleGAN은 여전히 얼굴 생성에서 강력하고, CycleGAN은 도메인 변환에서 활발히 사용됩니다. 각 모델은 서로 다른 강점이 있어서 용도에 따라 선택해야 합니다.

### 개념 4: 생성 모델의 진화 타임라인

생성 모델의 역사를 간단히 짚어보면, 놀라울 정도로 빠르게 발전해왔습니다:

- **2013년** — VAE 등장 (Kingma & Welling)
- **2014년** — GAN 등장 (Ian Goodfellow)
- **2015년** — DCGAN으로 안정적 학습 가능
- **2017년** — Pix2Pix, CycleGAN으로 이미지 변환 혁명
- **2019년** — StyleGAN2로 포토리얼리스틱 얼굴 생성
- **2020년** — DDPM으로 Diffusion 모델 부활
- **2021년** — DALL-E, CLIP 등장 — [텍스트-이미지 연결의 시작](../10-vision-language/02-clip.md)
- **2022년** — Stable Diffusion 공개로 이미지 생성 대중화
- **2023~2024년** — SDXL, FLUX, Stable Diffusion 3 등 고품질 생성 경쟁

## 더 깊이 알아보기

### GAN의 탄생 — 술집에서 시작된 혁명

2014년, Ian Goodfellow는 몬트리올의 한 술집에서 친구들과 생성 모델에 대해 토론하고 있었습니다. 기존 방법들의 한계에 대한 대화가 오가던 중, 문득 "생성자와 판별자가 서로 경쟁하면 어떨까?"라는 아이디어가 떠올랐죠.

집에 돌아온 그는 그날 밤 바로 코드를 짰고, **놀랍게도 첫 시도에서 바로 작동했습니다!** 이것이 바로 GAN(Generative Adversarial Network)의 탄생 순간입니다. Yann LeCun은 GAN을 "지난 10년간 머신러닝에서 가장 흥미로운 아이디어"라고 극찬했죠.

### 왜 "확률 분포를 학습한다"고 하는 걸까?

이미지를 픽셀값의 배열로 생각하면, 28×28 흑백 이미지는 784차원 공간의 한 점입니다. 하지만 784차원 공간의 모든 점이 의미 있는 이미지를 만드는 건 아니죠 — 랜덤 노이즈는 그냥 지지직 화면이 될 뿐입니다.

실제 이미지들은 이 거대한 공간의 아주 작은 영역(매니폴드)에 밀집해 있습니다. 생성 모델은 바로 이 매니폴드를 찾아내는 겁니다. "의미 있는 이미지가 살고 있는 영역"을 학습하는 거죠.

## 실습: 판별 vs 생성의 차이 느껴보기

```python
import torch
import torch.nn as nn

# ========================================
# 1. 판별 모델: 이미지 → 클래스
# ========================================
class Discriminator(nn.Module):
    """이미지를 보고 '어떤 숫자인지' 판별하는 모델"""
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(
            nn.Flatten(),                    # 28x28 → 784
            nn.Linear(784, 256),
            nn.ReLU(),
            nn.Linear(256, 10)               # 10개 클래스로 분류
        )

    def forward(self, x):
        return self.net(x)                   # 출력: 클래스 확률


# ========================================
# 2. 생성 모델: 랜덤 노이즈 → 이미지
# ========================================
class Generator(nn.Module):
    """랜덤 노이즈를 받아 '숫자 이미지를 생성'하는 모델"""
    def __init__(self, latent_dim=64):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(latent_dim, 256),      # 64차원 노이즈 → 256
            nn.ReLU(),
            nn.Linear(256, 784),             # 256 → 784 (28x28)
            nn.Tanh()                        # 픽셀값을 -1~1로 제한
        )

    def forward(self, z):
        return self.net(z).view(-1, 1, 28, 28)  # 이미지 형태로 복원


# 두 모델의 차이를 확인해봅시다
disc = Discriminator()
gen = Generator(latent_dim=64)

# 판별 모델: 이미지 입력 → 클래스 출력
fake_image = torch.randn(1, 1, 28, 28)     # 가짜 이미지 입력
class_logits = disc(fake_image)
print(f"판별 모델 입력: {fake_image.shape}")   # [1, 1, 28, 28]
print(f"판별 모델 출력: {class_logits.shape}")  # [1, 10] — 10개 클래스

# 생성 모델: 랜덤 노이즈 입력 → 이미지 출력
z = torch.randn(1, 64)                      # 64차원 랜덤 노이즈
generated = gen(z)
print(f"\n생성 모델 입력: {z.shape}")          # [1, 64] — 잠재 벡터
print(f"생성 모델 출력: {generated.shape}")    # [1, 1, 28, 28] — 이미지!
```

> 🔥 **실무 팁**: 생성 모델의 출력에 `Tanh()`를 사용하면 값이 [-1, 1] 범위로 제한됩니다. 학습 데이터도 같은 범위로 정규화해야 합니다. `Sigmoid()`를 쓰면 [0, 1] 범위가 되죠. 이 불일치가 학습 실패의 흔한 원인입니다.

## 흔한 오해와 팁

> ⚠️ **흔한 오해**: "생성 모델은 학습 데이터를 복사한다" — 아닙니다! 잘 학습된 생성 모델은 학습 데이터와 **비슷하지만 새로운** 이미지를 만듭니다. 학습 데이터를 단순 복사하는 것은 오히려 과적합(Overfitting)의 신호이죠.

> 💡 **알고 계셨나요?**: 생성 모델의 역사는 생각보다 오래되었습니다. 1980년대 볼츠만 머신(Boltzmann Machine)이 초기 생성 모델이었고, 2006년 Hinton의 Deep Belief Network도 생성 모델의 일종입니다. VAE(2013)와 GAN(2014)이 현대적 생성 모델의 시작을 열었죠.

## 핵심 정리

| 개념 | 설명 |
|------|------|
| 판별 모델 | $P(Y \mid X)$ — 입력을 보고 카테고리를 예측 |
| 생성 모델 | $P(X)$ — 데이터의 확률 분포를 학습하여 새 데이터 생성 |
| VAE | 인코더-디코더로 잠재 공간을 학습, 안정적이나 흐릿 |
| GAN | 생성자와 판별자의 적대적 경쟁, 선명하나 불안정 |
| Diffusion | 노이즈 추가/제거 반복, 최고 품질이나 느림 |
| 잠재 공간 | 데이터의 핵심 특성이 압축된 저차원 공간 |

## 다음 섹션 미리보기

다음 섹션 [Variational Autoencoder](./02-vae.md)에서는 생성 모델의 첫 번째 주자 VAE를 자세히 알아봅니다. "이미지를 핵심 코드로 압축했다가 다시 펼치는" 이 우아한 방법이 어떻게 작동하는지, PyTorch로 직접 구현해볼 거예요.

## 참고 자료

- [Google ML - What is a Generative Model?](https://developers.google.com/machine-learning/gan/generative) - 구글의 생성 모델 기초 설명
- [Generative vs Discriminative Models (DataCamp)](https://www.datacamp.com/blog/generative-vs-discriminative-models) - 두 패러다임의 차이를 깔끔하게 정리
- [Ian Goodfellow et al., "Generative Adversarial Nets" (2014)](https://arxiv.org/abs/1406.2661) - GAN 원논문, 생성 모델의 역사적 전환점
- [Kingma & Welling, "Auto-Encoding Variational Bayes" (2013)](https://arxiv.org/abs/1312.6114) - VAE 원논문
